{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-21 16:22:40--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  1.72MB/s    in 0.6s    \n",
      "\n",
      "2025-02-21 16:22:41 (1.72 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n",
      "\n",
      "First 1000 characters:\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in characters: {len(text)}\")\n",
    "print(\"\\nFirst 1000 characters:\\n\")\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(' '.join(chars))\n",
    "print(f\"\\nVocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. tokenize the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and val\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement the bigram language model\n",
    "start from the simpliest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(loss) \n",
    "print(logits.shape)\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: 4.692410945892334\n",
      "step 1 loss: 4.664144515991211\n",
      "step 2 loss: 4.765714645385742\n",
      "step 3 loss: 4.70655632019043\n",
      "step 4 loss: 4.5956573486328125\n",
      "step 5 loss: 4.7101240158081055\n",
      "step 6 loss: 4.713661193847656\n",
      "step 7 loss: 4.686909198760986\n",
      "step 8 loss: 4.700076103210449\n",
      "step 9 loss: 4.718283653259277\n",
      "step 10 loss: 4.715603351593018\n",
      "step 11 loss: 4.684308052062988\n",
      "step 12 loss: 4.745601177215576\n",
      "step 13 loss: 4.735717296600342\n",
      "step 14 loss: 4.666238784790039\n",
      "step 15 loss: 4.58615255355835\n",
      "step 16 loss: 4.714625835418701\n",
      "step 17 loss: 4.671982765197754\n",
      "step 18 loss: 4.715047359466553\n",
      "step 19 loss: 4.74489164352417\n",
      "step 20 loss: 4.630162715911865\n",
      "step 21 loss: 4.707578182220459\n",
      "step 22 loss: 4.670665740966797\n",
      "step 23 loss: 4.582583427429199\n",
      "step 24 loss: 4.739546298980713\n",
      "step 25 loss: 4.674807071685791\n",
      "step 26 loss: 4.805595874786377\n",
      "step 27 loss: 4.749917507171631\n",
      "step 28 loss: 4.691989421844482\n",
      "step 29 loss: 4.604404926300049\n",
      "step 30 loss: 4.721841335296631\n",
      "step 31 loss: 4.741591930389404\n",
      "step 32 loss: 4.609963417053223\n",
      "step 33 loss: 4.662769794464111\n",
      "step 34 loss: 4.730099678039551\n",
      "step 35 loss: 4.738433361053467\n",
      "step 36 loss: 4.688235282897949\n",
      "step 37 loss: 4.639987945556641\n",
      "step 38 loss: 4.736632823944092\n",
      "step 39 loss: 4.709773540496826\n",
      "step 40 loss: 4.736939430236816\n",
      "step 41 loss: 4.69184684753418\n",
      "step 42 loss: 4.719646453857422\n",
      "step 43 loss: 4.752516746520996\n",
      "step 44 loss: 4.570086479187012\n",
      "step 45 loss: 4.643786907196045\n",
      "step 46 loss: 4.699163913726807\n",
      "step 47 loss: 4.806960105895996\n",
      "step 48 loss: 4.572142601013184\n",
      "step 49 loss: 4.717066287994385\n",
      "step 50 loss: 4.509502410888672\n",
      "step 51 loss: 4.603540897369385\n",
      "step 52 loss: 4.6649675369262695\n",
      "step 53 loss: 4.712099075317383\n",
      "step 54 loss: 4.736577033996582\n",
      "step 55 loss: 4.812878131866455\n",
      "step 56 loss: 4.596436977386475\n",
      "step 57 loss: 4.702690601348877\n",
      "step 58 loss: 4.711158752441406\n",
      "step 59 loss: 4.636075019836426\n",
      "step 60 loss: 4.706498146057129\n",
      "step 61 loss: 4.706602573394775\n",
      "step 62 loss: 4.776336193084717\n",
      "step 63 loss: 4.538954257965088\n",
      "step 64 loss: 4.595245838165283\n",
      "step 65 loss: 4.648927688598633\n",
      "step 66 loss: 4.668923854827881\n",
      "step 67 loss: 4.5940351486206055\n",
      "step 68 loss: 4.757757186889648\n",
      "step 69 loss: 4.683036804199219\n",
      "step 70 loss: 4.627960205078125\n",
      "step 71 loss: 4.809708118438721\n",
      "step 72 loss: 4.720103740692139\n",
      "step 73 loss: 4.69432258605957\n",
      "step 74 loss: 4.593808650970459\n",
      "step 75 loss: 4.627923488616943\n",
      "step 76 loss: 4.628483772277832\n",
      "step 77 loss: 4.594723224639893\n",
      "step 78 loss: 4.667132377624512\n",
      "step 79 loss: 4.563992500305176\n",
      "step 80 loss: 4.598272323608398\n",
      "step 81 loss: 4.718154430389404\n",
      "step 82 loss: 4.686450958251953\n",
      "step 83 loss: 4.564647197723389\n",
      "step 84 loss: 4.66386604309082\n",
      "step 85 loss: 4.6551690101623535\n",
      "step 86 loss: 4.503818511962891\n",
      "step 87 loss: 4.662378311157227\n",
      "step 88 loss: 4.597469329833984\n",
      "step 89 loss: 4.553940773010254\n",
      "step 90 loss: 4.6458845138549805\n",
      "step 91 loss: 4.658196926116943\n",
      "step 92 loss: 4.652643203735352\n",
      "step 93 loss: 4.572150230407715\n",
      "step 94 loss: 4.654421806335449\n",
      "step 95 loss: 4.505650997161865\n",
      "step 96 loss: 4.6306376457214355\n",
      "step 97 loss: 4.7071919441223145\n",
      "step 98 loss: 4.6614508628845215\n",
      "step 99 loss: 4.65630578994751\n",
      "step 100 loss: 4.621085166931152\n",
      "step 101 loss: 4.699936389923096\n",
      "step 102 loss: 4.604835510253906\n",
      "step 103 loss: 4.62617826461792\n",
      "step 104 loss: 4.598467826843262\n",
      "step 105 loss: 4.562093257904053\n",
      "step 106 loss: 4.61775541305542\n",
      "step 107 loss: 4.545349597930908\n",
      "step 108 loss: 4.5832133293151855\n",
      "step 109 loss: 4.523378849029541\n",
      "step 110 loss: 4.568169593811035\n",
      "step 111 loss: 4.605664253234863\n",
      "step 112 loss: 4.625260829925537\n",
      "step 113 loss: 4.56585168838501\n",
      "step 114 loss: 4.675260066986084\n",
      "step 115 loss: 4.607123851776123\n",
      "step 116 loss: 4.707728385925293\n",
      "step 117 loss: 4.575418949127197\n",
      "step 118 loss: 4.564972877502441\n",
      "step 119 loss: 4.588582515716553\n",
      "step 120 loss: 4.592921733856201\n",
      "step 121 loss: 4.5604448318481445\n",
      "step 122 loss: 4.7858452796936035\n",
      "step 123 loss: 4.569169521331787\n",
      "step 124 loss: 4.5731329917907715\n",
      "step 125 loss: 4.601418495178223\n",
      "step 126 loss: 4.576107978820801\n",
      "step 127 loss: 4.60406494140625\n",
      "step 128 loss: 4.595185279846191\n",
      "step 129 loss: 4.515230655670166\n",
      "step 130 loss: 4.476064205169678\n",
      "step 131 loss: 4.521181583404541\n",
      "step 132 loss: 4.655724048614502\n",
      "step 133 loss: 4.520238399505615\n",
      "step 134 loss: 4.546555042266846\n",
      "step 135 loss: 4.582464694976807\n",
      "step 136 loss: 4.651482105255127\n",
      "step 137 loss: 4.5329389572143555\n",
      "step 138 loss: 4.552127361297607\n",
      "step 139 loss: 4.610964775085449\n",
      "step 140 loss: 4.720569610595703\n",
      "step 141 loss: 4.532519817352295\n",
      "step 142 loss: 4.590734004974365\n",
      "step 143 loss: 4.659023284912109\n",
      "step 144 loss: 4.569576740264893\n",
      "step 145 loss: 4.501828193664551\n",
      "step 146 loss: 4.534751892089844\n",
      "step 147 loss: 4.5555877685546875\n",
      "step 148 loss: 4.437855243682861\n",
      "step 149 loss: 4.584807872772217\n",
      "step 150 loss: 4.493810176849365\n",
      "step 151 loss: 4.496149063110352\n",
      "step 152 loss: 4.61268424987793\n",
      "step 153 loss: 4.541433811187744\n",
      "step 154 loss: 4.462303161621094\n",
      "step 155 loss: 4.494714736938477\n",
      "step 156 loss: 4.5445098876953125\n",
      "step 157 loss: 4.591465473175049\n",
      "step 158 loss: 4.559953212738037\n",
      "step 159 loss: 4.522037029266357\n",
      "step 160 loss: 4.604924201965332\n",
      "step 161 loss: 4.505509853363037\n",
      "step 162 loss: 4.600823402404785\n",
      "step 163 loss: 4.534110069274902\n",
      "step 164 loss: 4.466233730316162\n",
      "step 165 loss: 4.4869561195373535\n",
      "step 166 loss: 4.499792098999023\n",
      "step 167 loss: 4.558073043823242\n",
      "step 168 loss: 4.6118950843811035\n",
      "step 169 loss: 4.532322406768799\n",
      "step 170 loss: 4.494655132293701\n",
      "step 171 loss: 4.501111030578613\n",
      "step 172 loss: 4.487987518310547\n",
      "step 173 loss: 4.500681400299072\n",
      "step 174 loss: 4.447858810424805\n",
      "step 175 loss: 4.551181793212891\n",
      "step 176 loss: 4.5280585289001465\n",
      "step 177 loss: 4.5666728019714355\n",
      "step 178 loss: 4.546204566955566\n",
      "step 179 loss: 4.588383197784424\n",
      "step 180 loss: 4.5528364181518555\n",
      "step 181 loss: 4.483713626861572\n",
      "step 182 loss: 4.496376037597656\n",
      "step 183 loss: 4.405621528625488\n",
      "step 184 loss: 4.482258319854736\n",
      "step 185 loss: 4.484110355377197\n",
      "step 186 loss: 4.4490227699279785\n",
      "step 187 loss: 4.338376045227051\n",
      "step 188 loss: 4.460762977600098\n",
      "step 189 loss: 4.561094284057617\n",
      "step 190 loss: 4.521778583526611\n",
      "step 191 loss: 4.64401912689209\n",
      "step 192 loss: 4.514046669006348\n",
      "step 193 loss: 4.4592413902282715\n",
      "step 194 loss: 4.4573655128479\n",
      "step 195 loss: 4.652929782867432\n",
      "step 196 loss: 4.569197177886963\n",
      "step 197 loss: 4.549731254577637\n",
      "step 198 loss: 4.605109691619873\n",
      "step 199 loss: 4.531443119049072\n",
      "step 200 loss: 4.549462795257568\n",
      "step 201 loss: 4.4532389640808105\n",
      "step 202 loss: 4.42667293548584\n",
      "step 203 loss: 4.428462505340576\n",
      "step 204 loss: 4.5073981285095215\n",
      "step 205 loss: 4.599745273590088\n",
      "step 206 loss: 4.428247451782227\n",
      "step 207 loss: 4.516406059265137\n",
      "step 208 loss: 4.531017780303955\n",
      "step 209 loss: 4.524899005889893\n",
      "step 210 loss: 4.43112325668335\n",
      "step 211 loss: 4.413572311401367\n",
      "step 212 loss: 4.460278511047363\n",
      "step 213 loss: 4.394894123077393\n",
      "step 214 loss: 4.440585136413574\n",
      "step 215 loss: 4.515986442565918\n",
      "step 216 loss: 4.501888751983643\n",
      "step 217 loss: 4.540366172790527\n",
      "step 218 loss: 4.476080417633057\n",
      "step 219 loss: 4.480386257171631\n",
      "step 220 loss: 4.420384407043457\n",
      "step 221 loss: 4.483820915222168\n",
      "step 222 loss: 4.512678146362305\n",
      "step 223 loss: 4.560887336730957\n",
      "step 224 loss: 4.491804599761963\n",
      "step 225 loss: 4.480713844299316\n",
      "step 226 loss: 4.423666954040527\n",
      "step 227 loss: 4.433342456817627\n",
      "step 228 loss: 4.417287826538086\n",
      "step 229 loss: 4.414680480957031\n",
      "step 230 loss: 4.418458461761475\n",
      "step 231 loss: 4.457849025726318\n",
      "step 232 loss: 4.501733779907227\n",
      "step 233 loss: 4.55715799331665\n",
      "step 234 loss: 4.420411109924316\n",
      "step 235 loss: 4.500718116760254\n",
      "step 236 loss: 4.4469828605651855\n",
      "step 237 loss: 4.404678821563721\n",
      "step 238 loss: 4.448772430419922\n",
      "step 239 loss: 4.48320198059082\n",
      "step 240 loss: 4.401884078979492\n",
      "step 241 loss: 4.445903301239014\n",
      "step 242 loss: 4.52253532409668\n",
      "step 243 loss: 4.410618305206299\n",
      "step 244 loss: 4.45168924331665\n",
      "step 245 loss: 4.405672073364258\n",
      "step 246 loss: 4.359124660491943\n",
      "step 247 loss: 4.418424606323242\n",
      "step 248 loss: 4.447394371032715\n",
      "step 249 loss: 4.4999918937683105\n",
      "step 250 loss: 4.495026111602783\n",
      "step 251 loss: 4.441216945648193\n",
      "step 252 loss: 4.3983259201049805\n",
      "step 253 loss: 4.399979591369629\n",
      "step 254 loss: 4.396550178527832\n",
      "step 255 loss: 4.515158176422119\n",
      "step 256 loss: 4.436650276184082\n",
      "step 257 loss: 4.254152774810791\n",
      "step 258 loss: 4.4689860343933105\n",
      "step 259 loss: 4.4714837074279785\n",
      "step 260 loss: 4.331572532653809\n",
      "step 261 loss: 4.43186616897583\n",
      "step 262 loss: 4.361902713775635\n",
      "step 263 loss: 4.399670124053955\n",
      "step 264 loss: 4.441038608551025\n",
      "step 265 loss: 4.391478061676025\n",
      "step 266 loss: 4.487184524536133\n",
      "step 267 loss: 4.294976234436035\n",
      "step 268 loss: 4.41543436050415\n",
      "step 269 loss: 4.420255661010742\n",
      "step 270 loss: 4.351208209991455\n",
      "step 271 loss: 4.358051300048828\n",
      "step 272 loss: 4.529325008392334\n",
      "step 273 loss: 4.34090518951416\n",
      "step 274 loss: 4.488083839416504\n",
      "step 275 loss: 4.442676067352295\n",
      "step 276 loss: 4.422273635864258\n",
      "step 277 loss: 4.372884273529053\n",
      "step 278 loss: 4.420283317565918\n",
      "step 279 loss: 4.343837261199951\n",
      "step 280 loss: 4.380279064178467\n",
      "step 281 loss: 4.460439205169678\n",
      "step 282 loss: 4.4696574211120605\n",
      "step 283 loss: 4.4250054359436035\n",
      "step 284 loss: 4.369536876678467\n",
      "step 285 loss: 4.380033016204834\n",
      "step 286 loss: 4.365662097930908\n",
      "step 287 loss: 4.339999675750732\n",
      "step 288 loss: 4.48419713973999\n",
      "step 289 loss: 4.555813789367676\n",
      "step 290 loss: 4.4380035400390625\n",
      "step 291 loss: 4.477532386779785\n",
      "step 292 loss: 4.378237724304199\n",
      "step 293 loss: 4.336991310119629\n",
      "step 294 loss: 4.448875427246094\n",
      "step 295 loss: 4.361961841583252\n",
      "step 296 loss: 4.365668296813965\n",
      "step 297 loss: 4.469748020172119\n",
      "step 298 loss: 4.392701625823975\n",
      "step 299 loss: 4.334584712982178\n",
      "step 300 loss: 4.345612049102783\n",
      "step 301 loss: 4.306689739227295\n",
      "step 302 loss: 4.438986301422119\n",
      "step 303 loss: 4.454972267150879\n",
      "step 304 loss: 4.497693061828613\n",
      "step 305 loss: 4.392993927001953\n",
      "step 306 loss: 4.369708061218262\n",
      "step 307 loss: 4.354208469390869\n",
      "step 308 loss: 4.376923084259033\n",
      "step 309 loss: 4.388126850128174\n",
      "step 310 loss: 4.346914291381836\n",
      "step 311 loss: 4.454780101776123\n",
      "step 312 loss: 4.369599342346191\n",
      "step 313 loss: 4.400058746337891\n",
      "step 314 loss: 4.321812629699707\n",
      "step 315 loss: 4.430188179016113\n",
      "step 316 loss: 4.363119125366211\n",
      "step 317 loss: 4.258288860321045\n",
      "step 318 loss: 4.324169635772705\n",
      "step 319 loss: 4.358462333679199\n",
      "step 320 loss: 4.358053207397461\n",
      "step 321 loss: 4.346726417541504\n",
      "step 322 loss: 4.4252777099609375\n",
      "step 323 loss: 4.3356428146362305\n",
      "step 324 loss: 4.356608867645264\n",
      "step 325 loss: 4.268974304199219\n",
      "step 326 loss: 4.34548282623291\n",
      "step 327 loss: 4.391635894775391\n",
      "step 328 loss: 4.350566387176514\n",
      "step 329 loss: 4.366872310638428\n",
      "step 330 loss: 4.406307220458984\n",
      "step 331 loss: 4.266676425933838\n",
      "step 332 loss: 4.258195877075195\n",
      "step 333 loss: 4.2933526039123535\n",
      "step 334 loss: 4.3209686279296875\n",
      "step 335 loss: 4.332687854766846\n",
      "step 336 loss: 4.352051258087158\n",
      "step 337 loss: 4.382238864898682\n",
      "step 338 loss: 4.383143901824951\n",
      "step 339 loss: 4.244317531585693\n",
      "step 340 loss: 4.3100972175598145\n",
      "step 341 loss: 4.301767826080322\n",
      "step 342 loss: 4.462956428527832\n",
      "step 343 loss: 4.325232028961182\n",
      "step 344 loss: 4.341349124908447\n",
      "step 345 loss: 4.296145439147949\n",
      "step 346 loss: 4.235404014587402\n",
      "step 347 loss: 4.250915050506592\n",
      "step 348 loss: 4.308598518371582\n",
      "step 349 loss: 4.376663684844971\n",
      "step 350 loss: 4.237582683563232\n",
      "step 351 loss: 4.350257396697998\n",
      "step 352 loss: 4.228846549987793\n",
      "step 353 loss: 4.305774211883545\n",
      "step 354 loss: 4.354750633239746\n",
      "step 355 loss: 4.467280864715576\n",
      "step 356 loss: 4.318342685699463\n",
      "step 357 loss: 4.338566780090332\n",
      "step 358 loss: 4.289865970611572\n",
      "step 359 loss: 4.2690815925598145\n",
      "step 360 loss: 4.180973529815674\n",
      "step 361 loss: 4.387163162231445\n",
      "step 362 loss: 4.275718688964844\n",
      "step 363 loss: 4.193756580352783\n",
      "step 364 loss: 4.348846435546875\n",
      "step 365 loss: 4.311291694641113\n",
      "step 366 loss: 4.2306294441223145\n",
      "step 367 loss: 4.389198303222656\n",
      "step 368 loss: 4.380277633666992\n",
      "step 369 loss: 4.376614570617676\n",
      "step 370 loss: 4.3441596031188965\n",
      "step 371 loss: 4.27800989151001\n",
      "step 372 loss: 4.265659332275391\n",
      "step 373 loss: 4.27650260925293\n",
      "step 374 loss: 4.285951614379883\n",
      "step 375 loss: 4.260744094848633\n",
      "step 376 loss: 4.254425525665283\n",
      "step 377 loss: 4.387601852416992\n",
      "step 378 loss: 4.293213367462158\n",
      "step 379 loss: 4.275619029998779\n",
      "step 380 loss: 4.264163017272949\n",
      "step 381 loss: 4.214735507965088\n",
      "step 382 loss: 4.323124885559082\n",
      "step 383 loss: 4.272207260131836\n",
      "step 384 loss: 4.2730560302734375\n",
      "step 385 loss: 4.332861423492432\n",
      "step 386 loss: 4.228231906890869\n",
      "step 387 loss: 4.272346496582031\n",
      "step 388 loss: 4.331362724304199\n",
      "step 389 loss: 4.377825736999512\n",
      "step 390 loss: 4.194937705993652\n",
      "step 391 loss: 4.29219913482666\n",
      "step 392 loss: 4.230798244476318\n",
      "step 393 loss: 4.251380920410156\n",
      "step 394 loss: 4.296419620513916\n",
      "step 395 loss: 4.252346992492676\n",
      "step 396 loss: 4.2831315994262695\n",
      "step 397 loss: 4.3855204582214355\n",
      "step 398 loss: 4.1769537925720215\n",
      "step 399 loss: 4.227319717407227\n",
      "step 400 loss: 4.25573205947876\n",
      "step 401 loss: 4.238279819488525\n",
      "step 402 loss: 4.283568382263184\n",
      "step 403 loss: 4.299123764038086\n",
      "step 404 loss: 4.25726318359375\n",
      "step 405 loss: 4.330580711364746\n",
      "step 406 loss: 4.313518524169922\n",
      "step 407 loss: 4.335666179656982\n",
      "step 408 loss: 4.248019695281982\n",
      "step 409 loss: 4.212539196014404\n",
      "step 410 loss: 4.390359878540039\n",
      "step 411 loss: 4.26472806930542\n",
      "step 412 loss: 4.235801696777344\n",
      "step 413 loss: 4.21953821182251\n",
      "step 414 loss: 4.3849310874938965\n",
      "step 415 loss: 4.191295146942139\n",
      "step 416 loss: 4.29976224899292\n",
      "step 417 loss: 4.263235092163086\n",
      "step 418 loss: 4.209293365478516\n",
      "step 419 loss: 4.223510265350342\n",
      "step 420 loss: 4.197415351867676\n",
      "step 421 loss: 4.352291584014893\n",
      "step 422 loss: 4.358164310455322\n",
      "step 423 loss: 4.247671604156494\n",
      "step 424 loss: 4.329915523529053\n",
      "step 425 loss: 4.220633506774902\n",
      "step 426 loss: 4.240585803985596\n",
      "step 427 loss: 4.314285755157471\n",
      "step 428 loss: 4.2603759765625\n",
      "step 429 loss: 4.259975433349609\n",
      "step 430 loss: 4.2319464683532715\n",
      "step 431 loss: 4.184201240539551\n",
      "step 432 loss: 4.277885913848877\n",
      "step 433 loss: 4.279393196105957\n",
      "step 434 loss: 4.180922508239746\n",
      "step 435 loss: 4.270190238952637\n",
      "step 436 loss: 4.253904819488525\n",
      "step 437 loss: 4.179213047027588\n",
      "step 438 loss: 4.2253098487854\n",
      "step 439 loss: 4.279184341430664\n",
      "step 440 loss: 4.301351547241211\n",
      "step 441 loss: 4.229222297668457\n",
      "step 442 loss: 4.343070030212402\n",
      "step 443 loss: 4.276855945587158\n",
      "step 444 loss: 4.191099166870117\n",
      "step 445 loss: 4.293545722961426\n",
      "step 446 loss: 4.201022148132324\n",
      "step 447 loss: 4.206254959106445\n",
      "step 448 loss: 4.195919990539551\n",
      "step 449 loss: 4.2168192863464355\n",
      "step 450 loss: 4.221164703369141\n",
      "step 451 loss: 4.163954257965088\n",
      "step 452 loss: 4.355230331420898\n",
      "step 453 loss: 4.183740139007568\n",
      "step 454 loss: 4.180234432220459\n",
      "step 455 loss: 4.350985050201416\n",
      "step 456 loss: 4.179173946380615\n",
      "step 457 loss: 4.266575336456299\n",
      "step 458 loss: 4.145872592926025\n",
      "step 459 loss: 4.3596954345703125\n",
      "step 460 loss: 4.116206169128418\n",
      "step 461 loss: 4.278004169464111\n",
      "step 462 loss: 4.263311386108398\n",
      "step 463 loss: 4.26434850692749\n",
      "step 464 loss: 4.247138977050781\n",
      "step 465 loss: 4.2241597175598145\n",
      "step 466 loss: 4.163585662841797\n",
      "step 467 loss: 4.185435771942139\n",
      "step 468 loss: 4.255035877227783\n",
      "step 469 loss: 4.19374418258667\n",
      "step 470 loss: 4.256316661834717\n",
      "step 471 loss: 4.281538963317871\n",
      "step 472 loss: 4.171205043792725\n",
      "step 473 loss: 4.182270526885986\n",
      "step 474 loss: 4.194500923156738\n",
      "step 475 loss: 4.283398628234863\n",
      "step 476 loss: 4.299307823181152\n",
      "step 477 loss: 4.241708755493164\n",
      "step 478 loss: 4.283669948577881\n",
      "step 479 loss: 4.143308639526367\n",
      "step 480 loss: 4.15635347366333\n",
      "step 481 loss: 4.232965469360352\n",
      "step 482 loss: 4.241590976715088\n",
      "step 483 loss: 4.03627872467041\n",
      "step 484 loss: 4.157522201538086\n",
      "step 485 loss: 4.231443405151367\n",
      "step 486 loss: 4.147864818572998\n",
      "step 487 loss: 4.156970024108887\n",
      "step 488 loss: 4.223931789398193\n",
      "step 489 loss: 4.105999946594238\n",
      "step 490 loss: 4.140515327453613\n",
      "step 491 loss: 4.220064163208008\n",
      "step 492 loss: 4.173741817474365\n",
      "step 493 loss: 4.072698593139648\n",
      "step 494 loss: 4.158931255340576\n",
      "step 495 loss: 4.1450676918029785\n",
      "step 496 loss: 4.184323787689209\n",
      "step 497 loss: 4.103063583374023\n",
      "step 498 loss: 4.092406272888184\n",
      "step 499 loss: 4.1035356521606445\n",
      "step 500 loss: 4.214480876922607\n",
      "step 501 loss: 4.084356307983398\n",
      "step 502 loss: 4.133087158203125\n",
      "step 503 loss: 4.223405838012695\n",
      "step 504 loss: 4.220446586608887\n",
      "step 505 loss: 4.17624568939209\n",
      "step 506 loss: 4.128087043762207\n",
      "step 507 loss: 4.1905412673950195\n",
      "step 508 loss: 4.2345123291015625\n",
      "step 509 loss: 4.164053440093994\n",
      "step 510 loss: 4.095093250274658\n",
      "step 511 loss: 4.139039039611816\n",
      "step 512 loss: 4.137739181518555\n",
      "step 513 loss: 4.16702127456665\n",
      "step 514 loss: 4.160588264465332\n",
      "step 515 loss: 4.111112594604492\n",
      "step 516 loss: 4.019512176513672\n",
      "step 517 loss: 4.21013879776001\n",
      "step 518 loss: 4.071273326873779\n",
      "step 519 loss: 4.1922783851623535\n",
      "step 520 loss: 4.240005016326904\n",
      "step 521 loss: 4.245697021484375\n",
      "step 522 loss: 4.147686958312988\n",
      "step 523 loss: 4.2053656578063965\n",
      "step 524 loss: 4.214274883270264\n",
      "step 525 loss: 4.0432515144348145\n",
      "step 526 loss: 4.149839401245117\n",
      "step 527 loss: 4.167660713195801\n",
      "step 528 loss: 4.0837626457214355\n",
      "step 529 loss: 4.21851110458374\n",
      "step 530 loss: 4.211829662322998\n",
      "step 531 loss: 4.033554553985596\n",
      "step 532 loss: 4.161428928375244\n",
      "step 533 loss: 4.165452003479004\n",
      "step 534 loss: 4.050341606140137\n",
      "step 535 loss: 4.1566925048828125\n",
      "step 536 loss: 4.160954475402832\n",
      "step 537 loss: 4.093878269195557\n",
      "step 538 loss: 4.132650375366211\n",
      "step 539 loss: 4.1404008865356445\n",
      "step 540 loss: 4.130082130432129\n",
      "step 541 loss: 4.175171375274658\n",
      "step 542 loss: 4.112301826477051\n",
      "step 543 loss: 4.073479175567627\n",
      "step 544 loss: 4.167318820953369\n",
      "step 545 loss: 4.1959919929504395\n",
      "step 546 loss: 4.078147888183594\n",
      "step 547 loss: 4.069385528564453\n",
      "step 548 loss: 4.063492774963379\n",
      "step 549 loss: 4.158783435821533\n",
      "step 550 loss: 4.046174049377441\n",
      "step 551 loss: 4.117081642150879\n",
      "step 552 loss: 4.2450761795043945\n",
      "step 553 loss: 4.109870433807373\n",
      "step 554 loss: 4.118143081665039\n",
      "step 555 loss: 4.16467809677124\n",
      "step 556 loss: 4.108386039733887\n",
      "step 557 loss: 4.165163040161133\n",
      "step 558 loss: 4.048225402832031\n",
      "step 559 loss: 4.135279655456543\n",
      "step 560 loss: 4.158371925354004\n",
      "step 561 loss: 4.126312732696533\n",
      "step 562 loss: 3.964298725128174\n",
      "step 563 loss: 4.15919303894043\n",
      "step 564 loss: 4.119665145874023\n",
      "step 565 loss: 4.238160133361816\n",
      "step 566 loss: 4.114436626434326\n",
      "step 567 loss: 4.085212230682373\n",
      "step 568 loss: 4.048520088195801\n",
      "step 569 loss: 4.12908411026001\n",
      "step 570 loss: 4.118659019470215\n",
      "step 571 loss: 4.099023818969727\n",
      "step 572 loss: 4.183162689208984\n",
      "step 573 loss: 4.058305740356445\n",
      "step 574 loss: 4.143056392669678\n",
      "step 575 loss: 4.114629745483398\n",
      "step 576 loss: 4.0972514152526855\n",
      "step 577 loss: 4.099020481109619\n",
      "step 578 loss: 4.109713554382324\n",
      "step 579 loss: 4.0857157707214355\n",
      "step 580 loss: 4.125316143035889\n",
      "step 581 loss: 4.175577640533447\n",
      "step 582 loss: 4.063840389251709\n",
      "step 583 loss: 4.108927249908447\n",
      "step 584 loss: 4.104191780090332\n",
      "step 585 loss: 4.119007110595703\n",
      "step 586 loss: 4.09041166305542\n",
      "step 587 loss: 4.067192077636719\n",
      "step 588 loss: 4.102412223815918\n",
      "step 589 loss: 4.113567352294922\n",
      "step 590 loss: 4.192253589630127\n",
      "step 591 loss: 4.100231170654297\n",
      "step 592 loss: 4.120125770568848\n",
      "step 593 loss: 4.143221378326416\n",
      "step 594 loss: 4.08186674118042\n",
      "step 595 loss: 4.087782382965088\n",
      "step 596 loss: 4.074817657470703\n",
      "step 597 loss: 4.0610480308532715\n",
      "step 598 loss: 4.183136463165283\n",
      "step 599 loss: 4.0359954833984375\n",
      "step 600 loss: 4.124096870422363\n",
      "step 601 loss: 4.125418186187744\n",
      "step 602 loss: 4.087498188018799\n",
      "step 603 loss: 4.018767356872559\n",
      "step 604 loss: 4.111219882965088\n",
      "step 605 loss: 4.014806270599365\n",
      "step 606 loss: 4.001387596130371\n",
      "step 607 loss: 3.984426736831665\n",
      "step 608 loss: 4.148003578186035\n",
      "step 609 loss: 4.025826454162598\n",
      "step 610 loss: 4.057998180389404\n",
      "step 611 loss: 3.9568910598754883\n",
      "step 612 loss: 4.0929460525512695\n",
      "step 613 loss: 4.072330474853516\n",
      "step 614 loss: 4.03060245513916\n",
      "step 615 loss: 4.048306941986084\n",
      "step 616 loss: 4.133228778839111\n",
      "step 617 loss: 4.038397789001465\n",
      "step 618 loss: 4.125432014465332\n",
      "step 619 loss: 4.072927474975586\n",
      "step 620 loss: 4.045656204223633\n",
      "step 621 loss: 4.1224045753479\n",
      "step 622 loss: 4.121188163757324\n",
      "step 623 loss: 4.098836898803711\n",
      "step 624 loss: 4.161966323852539\n",
      "step 625 loss: 4.04520845413208\n",
      "step 626 loss: 4.056055068969727\n",
      "step 627 loss: 3.9817774295806885\n",
      "step 628 loss: 4.114030361175537\n",
      "step 629 loss: 4.055196762084961\n",
      "step 630 loss: 4.181158065795898\n",
      "step 631 loss: 4.015369415283203\n",
      "step 632 loss: 4.070910453796387\n",
      "step 633 loss: 4.0655999183654785\n",
      "step 634 loss: 4.0205183029174805\n",
      "step 635 loss: 4.009420871734619\n",
      "step 636 loss: 4.009321689605713\n",
      "step 637 loss: 4.121938705444336\n",
      "step 638 loss: 4.040282726287842\n",
      "step 639 loss: 4.006514072418213\n",
      "step 640 loss: 3.9769175052642822\n",
      "step 641 loss: 3.9687490463256836\n",
      "step 642 loss: 4.053492546081543\n",
      "step 643 loss: 4.024524211883545\n",
      "step 644 loss: 4.1228790283203125\n",
      "step 645 loss: 3.9823174476623535\n",
      "step 646 loss: 3.9732022285461426\n",
      "step 647 loss: 4.015324592590332\n",
      "step 648 loss: 4.0347113609313965\n",
      "step 649 loss: 4.076888561248779\n",
      "step 650 loss: 4.0141777992248535\n",
      "step 651 loss: 4.0707106590271\n",
      "step 652 loss: 4.029590606689453\n",
      "step 653 loss: 4.0675764083862305\n",
      "step 654 loss: 3.958583116531372\n",
      "step 655 loss: 4.049327850341797\n",
      "step 656 loss: 3.9634346961975098\n",
      "step 657 loss: 4.109421730041504\n",
      "step 658 loss: 4.038191795349121\n",
      "step 659 loss: 4.125021934509277\n",
      "step 660 loss: 4.031307220458984\n",
      "step 661 loss: 4.070112705230713\n",
      "step 662 loss: 3.9946374893188477\n",
      "step 663 loss: 4.0985188484191895\n",
      "step 664 loss: 3.9959030151367188\n",
      "step 665 loss: 4.035952568054199\n",
      "step 666 loss: 4.017228603363037\n",
      "step 667 loss: 4.024997234344482\n",
      "step 668 loss: 3.9557952880859375\n",
      "step 669 loss: 4.03995418548584\n",
      "step 670 loss: 4.067239761352539\n",
      "step 671 loss: 4.015563011169434\n",
      "step 672 loss: 4.054080486297607\n",
      "step 673 loss: 4.0563883781433105\n",
      "step 674 loss: 3.9339613914489746\n",
      "step 675 loss: 3.991907835006714\n",
      "step 676 loss: 3.9665045738220215\n",
      "step 677 loss: 4.003228664398193\n",
      "step 678 loss: 4.0467000007629395\n",
      "step 679 loss: 4.060911178588867\n",
      "step 680 loss: 3.9072952270507812\n",
      "step 681 loss: 4.017505645751953\n",
      "step 682 loss: 4.071604251861572\n",
      "step 683 loss: 4.083905220031738\n",
      "step 684 loss: 4.046928405761719\n",
      "step 685 loss: 4.020439624786377\n",
      "step 686 loss: 3.9562582969665527\n",
      "step 687 loss: 3.857187509536743\n",
      "step 688 loss: 3.9379940032958984\n",
      "step 689 loss: 4.030976295471191\n",
      "step 690 loss: 4.080989837646484\n",
      "step 691 loss: 3.94681715965271\n",
      "step 692 loss: 4.019579887390137\n",
      "step 693 loss: 3.9735379219055176\n",
      "step 694 loss: 4.019238471984863\n",
      "step 695 loss: 4.019179344177246\n",
      "step 696 loss: 3.9712741374969482\n",
      "step 697 loss: 4.026883602142334\n",
      "step 698 loss: 4.021551609039307\n",
      "step 699 loss: 4.090793132781982\n",
      "step 700 loss: 3.9863951206207275\n",
      "step 701 loss: 4.023050308227539\n",
      "step 702 loss: 3.958601474761963\n",
      "step 703 loss: 3.948957681655884\n",
      "step 704 loss: 3.9391560554504395\n",
      "step 705 loss: 3.979315996170044\n",
      "step 706 loss: 4.014042854309082\n",
      "step 707 loss: 3.987943410873413\n",
      "step 708 loss: 4.037786483764648\n",
      "step 709 loss: 3.9132330417633057\n",
      "step 710 loss: 3.93660569190979\n",
      "step 711 loss: 3.9861817359924316\n",
      "step 712 loss: 3.9920737743377686\n",
      "step 713 loss: 3.935559034347534\n",
      "step 714 loss: 4.01882791519165\n",
      "step 715 loss: 3.9929792881011963\n",
      "step 716 loss: 3.959758758544922\n",
      "step 717 loss: 3.9396843910217285\n",
      "step 718 loss: 4.044256687164307\n",
      "step 719 loss: 4.004631996154785\n",
      "step 720 loss: 3.996853828430176\n",
      "step 721 loss: 4.005602836608887\n",
      "step 722 loss: 4.0114874839782715\n",
      "step 723 loss: 3.909653663635254\n",
      "step 724 loss: 3.9997363090515137\n",
      "step 725 loss: 4.052602291107178\n",
      "step 726 loss: 3.982553720474243\n",
      "step 727 loss: 3.9164369106292725\n",
      "step 728 loss: 3.942601203918457\n",
      "step 729 loss: 3.9261810779571533\n",
      "step 730 loss: 3.895392894744873\n",
      "step 731 loss: 3.9324474334716797\n",
      "step 732 loss: 3.9205896854400635\n",
      "step 733 loss: 3.862194061279297\n",
      "step 734 loss: 3.914445400238037\n",
      "step 735 loss: 3.8829867839813232\n",
      "step 736 loss: 3.958883047103882\n",
      "step 737 loss: 3.985666036605835\n",
      "step 738 loss: 3.899744987487793\n",
      "step 739 loss: 3.9055404663085938\n",
      "step 740 loss: 3.9618642330169678\n",
      "step 741 loss: 3.889052391052246\n",
      "step 742 loss: 3.9750149250030518\n",
      "step 743 loss: 3.955091953277588\n",
      "step 744 loss: 3.9291818141937256\n",
      "step 745 loss: 4.087095737457275\n",
      "step 746 loss: 3.9071226119995117\n",
      "step 747 loss: 3.909717082977295\n",
      "step 748 loss: 3.9263739585876465\n",
      "step 749 loss: 3.936264991760254\n",
      "step 750 loss: 3.9365665912628174\n",
      "step 751 loss: 3.938066005706787\n",
      "step 752 loss: 3.970029830932617\n",
      "step 753 loss: 3.9482243061065674\n",
      "step 754 loss: 3.8840718269348145\n",
      "step 755 loss: 3.952329635620117\n",
      "step 756 loss: 3.8996760845184326\n",
      "step 757 loss: 3.868910789489746\n",
      "step 758 loss: 3.926823377609253\n",
      "step 759 loss: 3.8954102993011475\n",
      "step 760 loss: 4.0050368309021\n",
      "step 761 loss: 3.949986696243286\n",
      "step 762 loss: 3.946974277496338\n",
      "step 763 loss: 3.8552606105804443\n",
      "step 764 loss: 4.010585784912109\n",
      "step 765 loss: 4.076684951782227\n",
      "step 766 loss: 3.9837636947631836\n",
      "step 767 loss: 3.980055332183838\n",
      "step 768 loss: 4.009552478790283\n",
      "step 769 loss: 3.981788158416748\n",
      "step 770 loss: 3.9590961933135986\n",
      "step 771 loss: 3.8400533199310303\n",
      "step 772 loss: 3.88386607170105\n",
      "step 773 loss: 3.8691532611846924\n",
      "step 774 loss: 3.783965826034546\n",
      "step 775 loss: 3.9049429893493652\n",
      "step 776 loss: 3.9803521633148193\n",
      "step 777 loss: 3.9815800189971924\n",
      "step 778 loss: 3.8576600551605225\n",
      "step 779 loss: 3.8969061374664307\n",
      "step 780 loss: 3.9168310165405273\n",
      "step 781 loss: 3.8810322284698486\n",
      "step 782 loss: 3.8791961669921875\n",
      "step 783 loss: 3.9240477085113525\n",
      "step 784 loss: 3.853982448577881\n",
      "step 785 loss: 3.9570164680480957\n",
      "step 786 loss: 3.8988611698150635\n",
      "step 787 loss: 3.906435251235962\n",
      "step 788 loss: 3.8281397819519043\n",
      "step 789 loss: 3.8748936653137207\n",
      "step 790 loss: 3.7957098484039307\n",
      "step 791 loss: 3.88936185836792\n",
      "step 792 loss: 3.8909873962402344\n",
      "step 793 loss: 3.9083378314971924\n",
      "step 794 loss: 3.9673893451690674\n",
      "step 795 loss: 3.7797911167144775\n",
      "step 796 loss: 3.875593900680542\n",
      "step 797 loss: 3.9253997802734375\n",
      "step 798 loss: 3.9070682525634766\n",
      "step 799 loss: 3.947261333465576\n",
      "step 800 loss: 3.9517807960510254\n",
      "step 801 loss: 3.9063897132873535\n",
      "step 802 loss: 3.9117953777313232\n",
      "step 803 loss: 3.870689630508423\n",
      "step 804 loss: 3.8529112339019775\n",
      "step 805 loss: 3.899077892303467\n",
      "step 806 loss: 3.8730580806732178\n",
      "step 807 loss: 3.8374412059783936\n",
      "step 808 loss: 3.9627768993377686\n",
      "step 809 loss: 3.8845608234405518\n",
      "step 810 loss: 3.8707284927368164\n",
      "step 811 loss: 3.812702178955078\n",
      "step 812 loss: 3.9962551593780518\n",
      "step 813 loss: 3.9128105640411377\n",
      "step 814 loss: 3.903803825378418\n",
      "step 815 loss: 3.901273012161255\n",
      "step 816 loss: 3.8220882415771484\n",
      "step 817 loss: 3.8226215839385986\n",
      "step 818 loss: 3.8852691650390625\n",
      "step 819 loss: 3.82008957862854\n",
      "step 820 loss: 3.8163669109344482\n",
      "step 821 loss: 3.7907116413116455\n",
      "step 822 loss: 3.8253843784332275\n",
      "step 823 loss: 3.8396363258361816\n",
      "step 824 loss: 3.8457422256469727\n",
      "step 825 loss: 3.8769731521606445\n",
      "step 826 loss: 3.8439457416534424\n",
      "step 827 loss: 3.876339912414551\n",
      "step 828 loss: 3.908766746520996\n",
      "step 829 loss: 3.8244807720184326\n",
      "step 830 loss: 3.840985059738159\n",
      "step 831 loss: 3.755624294281006\n",
      "step 832 loss: 3.857574939727783\n",
      "step 833 loss: 3.9703409671783447\n",
      "step 834 loss: 3.698105812072754\n",
      "step 835 loss: 3.7928614616394043\n",
      "step 836 loss: 3.8883657455444336\n",
      "step 837 loss: 3.9447708129882812\n",
      "step 838 loss: 3.814781665802002\n",
      "step 839 loss: 3.8951287269592285\n",
      "step 840 loss: 3.802658796310425\n",
      "step 841 loss: 3.8546926975250244\n",
      "step 842 loss: 3.90655255317688\n",
      "step 843 loss: 3.7954976558685303\n",
      "step 844 loss: 3.80258846282959\n",
      "step 845 loss: 3.848461389541626\n",
      "step 846 loss: 3.8054921627044678\n",
      "step 847 loss: 3.831833839416504\n",
      "step 848 loss: 3.858421564102173\n",
      "step 849 loss: 3.7204763889312744\n",
      "step 850 loss: 3.9306366443634033\n",
      "step 851 loss: 3.8283700942993164\n",
      "step 852 loss: 3.828134775161743\n",
      "step 853 loss: 3.854487895965576\n",
      "step 854 loss: 3.8038671016693115\n",
      "step 855 loss: 3.739840030670166\n",
      "step 856 loss: 3.8414194583892822\n",
      "step 857 loss: 3.840139627456665\n",
      "step 858 loss: 3.858626365661621\n",
      "step 859 loss: 3.8682193756103516\n",
      "step 860 loss: 3.7725791931152344\n",
      "step 861 loss: 3.7404379844665527\n",
      "step 862 loss: 3.853250503540039\n",
      "step 863 loss: 3.909426212310791\n",
      "step 864 loss: 3.827120304107666\n",
      "step 865 loss: 3.9142749309539795\n",
      "step 866 loss: 3.8566269874572754\n",
      "step 867 loss: 3.836930990219116\n",
      "step 868 loss: 3.8507373332977295\n",
      "step 869 loss: 3.7641050815582275\n",
      "step 870 loss: 3.8466036319732666\n",
      "step 871 loss: 3.8550658226013184\n",
      "step 872 loss: 3.814547538757324\n",
      "step 873 loss: 3.858736515045166\n",
      "step 874 loss: 3.835547924041748\n",
      "step 875 loss: 3.820634126663208\n",
      "step 876 loss: 3.8446009159088135\n",
      "step 877 loss: 3.8565011024475098\n",
      "step 878 loss: 3.793048858642578\n",
      "step 879 loss: 3.803140640258789\n",
      "step 880 loss: 3.877279281616211\n",
      "step 881 loss: 3.915501356124878\n",
      "step 882 loss: 3.737924098968506\n",
      "step 883 loss: 3.7830746173858643\n",
      "step 884 loss: 3.773716926574707\n",
      "step 885 loss: 3.769728660583496\n",
      "step 886 loss: 3.76853084564209\n",
      "step 887 loss: 3.774484395980835\n",
      "step 888 loss: 3.908461093902588\n",
      "step 889 loss: 3.785860300064087\n",
      "step 890 loss: 3.840094804763794\n",
      "step 891 loss: 3.9154365062713623\n",
      "step 892 loss: 3.7070677280426025\n",
      "step 893 loss: 3.876446485519409\n",
      "step 894 loss: 3.8132681846618652\n",
      "step 895 loss: 3.7787420749664307\n",
      "step 896 loss: 3.808058023452759\n",
      "step 897 loss: 3.7111711502075195\n",
      "step 898 loss: 3.7674307823181152\n",
      "step 899 loss: 3.886543035507202\n",
      "step 900 loss: 3.837888717651367\n",
      "step 901 loss: 3.7464873790740967\n",
      "step 902 loss: 3.8434603214263916\n",
      "step 903 loss: 3.8652989864349365\n",
      "step 904 loss: 3.9370501041412354\n",
      "step 905 loss: 3.7554266452789307\n",
      "step 906 loss: 3.80645751953125\n",
      "step 907 loss: 3.7218282222747803\n",
      "step 908 loss: 3.841688394546509\n",
      "step 909 loss: 3.838592529296875\n",
      "step 910 loss: 3.7659902572631836\n",
      "step 911 loss: 3.8026273250579834\n",
      "step 912 loss: 3.730456829071045\n",
      "step 913 loss: 3.7594165802001953\n",
      "step 914 loss: 3.868187189102173\n",
      "step 915 loss: 3.8208365440368652\n",
      "step 916 loss: 3.8494954109191895\n",
      "step 917 loss: 3.6943864822387695\n",
      "step 918 loss: 3.825169324874878\n",
      "step 919 loss: 3.802319049835205\n",
      "step 920 loss: 3.7781949043273926\n",
      "step 921 loss: 3.8065850734710693\n",
      "step 922 loss: 3.795403003692627\n",
      "step 923 loss: 3.826564311981201\n",
      "step 924 loss: 3.7271037101745605\n",
      "step 925 loss: 3.769099473953247\n",
      "step 926 loss: 3.814549684524536\n",
      "step 927 loss: 3.7501776218414307\n",
      "step 928 loss: 3.8552398681640625\n",
      "step 929 loss: 3.7635231018066406\n",
      "step 930 loss: 3.757671594619751\n",
      "step 931 loss: 3.7829813957214355\n",
      "step 932 loss: 3.856001377105713\n",
      "step 933 loss: 3.786350965499878\n",
      "step 934 loss: 3.7992660999298096\n",
      "step 935 loss: 3.853888511657715\n",
      "step 936 loss: 3.7279489040374756\n",
      "step 937 loss: 3.77148699760437\n",
      "step 938 loss: 3.6774191856384277\n",
      "step 939 loss: 3.7062580585479736\n",
      "step 940 loss: 3.8540217876434326\n",
      "step 941 loss: 3.714585065841675\n",
      "step 942 loss: 3.75681209564209\n",
      "step 943 loss: 3.8052268028259277\n",
      "step 944 loss: 3.750840425491333\n",
      "step 945 loss: 3.7266221046447754\n",
      "step 946 loss: 3.8073298931121826\n",
      "step 947 loss: 3.621952772140503\n",
      "step 948 loss: 3.7910683155059814\n",
      "step 949 loss: 3.783512830734253\n",
      "step 950 loss: 3.916217088699341\n",
      "step 951 loss: 3.8640291690826416\n",
      "step 952 loss: 3.75536847114563\n",
      "step 953 loss: 3.7300283908843994\n",
      "step 954 loss: 3.797163963317871\n",
      "step 955 loss: 3.7928595542907715\n",
      "step 956 loss: 3.7810394763946533\n",
      "step 957 loss: 3.8614728450775146\n",
      "step 958 loss: 3.801198720932007\n",
      "step 959 loss: 3.831483840942383\n",
      "step 960 loss: 3.7052838802337646\n",
      "step 961 loss: 3.786813735961914\n",
      "step 962 loss: 3.731647253036499\n",
      "step 963 loss: 3.7757680416107178\n",
      "step 964 loss: 3.7474052906036377\n",
      "step 965 loss: 3.761415481567383\n",
      "step 966 loss: 3.806239128112793\n",
      "step 967 loss: 3.7878215312957764\n",
      "step 968 loss: 3.8432199954986572\n",
      "step 969 loss: 3.7990047931671143\n",
      "step 970 loss: 3.7246756553649902\n",
      "step 971 loss: 3.711846113204956\n",
      "step 972 loss: 3.7594316005706787\n",
      "step 973 loss: 3.83084774017334\n",
      "step 974 loss: 3.6742987632751465\n",
      "step 975 loss: 3.684211015701294\n",
      "step 976 loss: 3.766167163848877\n",
      "step 977 loss: 3.725360155105591\n",
      "step 978 loss: 3.788503408432007\n",
      "step 979 loss: 3.7539544105529785\n",
      "step 980 loss: 3.74045467376709\n",
      "step 981 loss: 3.6661877632141113\n",
      "step 982 loss: 3.8223159313201904\n",
      "step 983 loss: 3.707447052001953\n",
      "step 984 loss: 3.723768949508667\n",
      "step 985 loss: 3.678398609161377\n",
      "step 986 loss: 3.639932155609131\n",
      "step 987 loss: 3.683429718017578\n",
      "step 988 loss: 3.703873634338379\n",
      "step 989 loss: 3.670771360397339\n",
      "step 990 loss: 3.747714042663574\n",
      "step 991 loss: 3.8158376216888428\n",
      "step 992 loss: 3.765026569366455\n",
      "step 993 loss: 3.719977617263794\n",
      "step 994 loss: 3.709388017654419\n",
      "step 995 loss: 3.685523271560669\n",
      "step 996 loss: 3.697873115539551\n",
      "step 997 loss: 3.727125883102417\n",
      "step 998 loss: 3.804948091506958\n",
      "step 999 loss: 3.704137086868286\n",
      "step 1000 loss: 3.7637593746185303\n",
      "step 1001 loss: 3.760815382003784\n",
      "step 1002 loss: 3.7295024394989014\n",
      "step 1003 loss: 3.76991868019104\n",
      "step 1004 loss: 3.651653528213501\n",
      "step 1005 loss: 3.7412242889404297\n",
      "step 1006 loss: 3.5917458534240723\n",
      "step 1007 loss: 3.801196336746216\n",
      "step 1008 loss: 3.72733736038208\n",
      "step 1009 loss: 3.71734881401062\n",
      "step 1010 loss: 3.74552845954895\n",
      "step 1011 loss: 3.632333517074585\n",
      "step 1012 loss: 3.7212233543395996\n",
      "step 1013 loss: 3.812814950942993\n",
      "step 1014 loss: 3.794088125228882\n",
      "step 1015 loss: 3.6777656078338623\n",
      "step 1016 loss: 3.683845281600952\n",
      "step 1017 loss: 3.707848310470581\n",
      "step 1018 loss: 3.7250285148620605\n",
      "step 1019 loss: 3.6925432682037354\n",
      "step 1020 loss: 3.6531364917755127\n",
      "step 1021 loss: 3.659817934036255\n",
      "step 1022 loss: 3.6261472702026367\n",
      "step 1023 loss: 3.660931348800659\n",
      "step 1024 loss: 3.7745466232299805\n",
      "step 1025 loss: 3.81119704246521\n",
      "step 1026 loss: 3.800462484359741\n",
      "step 1027 loss: 3.7950568199157715\n",
      "step 1028 loss: 3.7313954830169678\n",
      "step 1029 loss: 3.705239772796631\n",
      "step 1030 loss: 3.7860496044158936\n",
      "step 1031 loss: 3.6133577823638916\n",
      "step 1032 loss: 3.7759718894958496\n",
      "step 1033 loss: 3.7864086627960205\n",
      "step 1034 loss: 3.733623504638672\n",
      "step 1035 loss: 3.644171953201294\n",
      "step 1036 loss: 3.681720495223999\n",
      "step 1037 loss: 3.7327325344085693\n",
      "step 1038 loss: 3.6357054710388184\n",
      "step 1039 loss: 3.726031541824341\n",
      "step 1040 loss: 3.732304096221924\n",
      "step 1041 loss: 3.7361340522766113\n",
      "step 1042 loss: 3.7386937141418457\n",
      "step 1043 loss: 3.7054808139801025\n",
      "step 1044 loss: 3.716219663619995\n",
      "step 1045 loss: 3.7219111919403076\n",
      "step 1046 loss: 3.6327240467071533\n",
      "step 1047 loss: 3.833463668823242\n",
      "step 1048 loss: 3.7487125396728516\n",
      "step 1049 loss: 3.59930419921875\n",
      "step 1050 loss: 3.737290143966675\n",
      "step 1051 loss: 3.830488920211792\n",
      "step 1052 loss: 3.5564372539520264\n",
      "step 1053 loss: 3.765251636505127\n",
      "step 1054 loss: 3.7173023223876953\n",
      "step 1055 loss: 3.6367578506469727\n",
      "step 1056 loss: 3.695281744003296\n",
      "step 1057 loss: 3.7630207538604736\n",
      "step 1058 loss: 3.508382558822632\n",
      "step 1059 loss: 3.663442850112915\n",
      "step 1060 loss: 3.613909959793091\n",
      "step 1061 loss: 3.7125837802886963\n",
      "step 1062 loss: 3.6183583736419678\n",
      "step 1063 loss: 3.709897041320801\n",
      "step 1064 loss: 3.6047327518463135\n",
      "step 1065 loss: 3.5741469860076904\n",
      "step 1066 loss: 3.752105712890625\n",
      "step 1067 loss: 3.689621925354004\n",
      "step 1068 loss: 3.6168007850646973\n",
      "step 1069 loss: 3.669369697570801\n",
      "step 1070 loss: 3.6949751377105713\n",
      "step 1071 loss: 3.7388062477111816\n",
      "step 1072 loss: 3.7041192054748535\n",
      "step 1073 loss: 3.765953302383423\n",
      "step 1074 loss: 3.6947946548461914\n",
      "step 1075 loss: 3.6722917556762695\n",
      "step 1076 loss: 3.688305377960205\n",
      "step 1077 loss: 3.6928765773773193\n",
      "step 1078 loss: 3.6818642616271973\n",
      "step 1079 loss: 3.673569440841675\n",
      "step 1080 loss: 3.6317903995513916\n",
      "step 1081 loss: 3.75984263420105\n",
      "step 1082 loss: 3.6545345783233643\n",
      "step 1083 loss: 3.65710711479187\n",
      "step 1084 loss: 3.656646251678467\n",
      "step 1085 loss: 3.7105162143707275\n",
      "step 1086 loss: 3.667161226272583\n",
      "step 1087 loss: 3.7559518814086914\n",
      "step 1088 loss: 3.673699140548706\n",
      "step 1089 loss: 3.6156229972839355\n",
      "step 1090 loss: 3.6844842433929443\n",
      "step 1091 loss: 3.582674980163574\n",
      "step 1092 loss: 3.65226411819458\n",
      "step 1093 loss: 3.625478506088257\n",
      "step 1094 loss: 3.6835029125213623\n",
      "step 1095 loss: 3.674412727355957\n",
      "step 1096 loss: 3.6355621814727783\n",
      "step 1097 loss: 3.659939765930176\n",
      "step 1098 loss: 3.645096778869629\n",
      "step 1099 loss: 3.6883111000061035\n",
      "step 1100 loss: 3.6824679374694824\n",
      "step 1101 loss: 3.6916990280151367\n",
      "step 1102 loss: 3.775355100631714\n",
      "step 1103 loss: 3.5679879188537598\n",
      "step 1104 loss: 3.6252493858337402\n",
      "step 1105 loss: 3.6441354751586914\n",
      "step 1106 loss: 3.6750290393829346\n",
      "step 1107 loss: 3.7602500915527344\n",
      "step 1108 loss: 3.655397653579712\n",
      "step 1109 loss: 3.6514639854431152\n",
      "step 1110 loss: 3.688493251800537\n",
      "step 1111 loss: 3.702279806137085\n",
      "step 1112 loss: 3.613967180252075\n",
      "step 1113 loss: 3.7137131690979004\n",
      "step 1114 loss: 3.7722153663635254\n",
      "step 1115 loss: 3.596604347229004\n",
      "step 1116 loss: 3.5304887294769287\n",
      "step 1117 loss: 3.6604740619659424\n",
      "step 1118 loss: 3.5630903244018555\n",
      "step 1119 loss: 3.59187388420105\n",
      "step 1120 loss: 3.5366814136505127\n",
      "step 1121 loss: 3.5414323806762695\n",
      "step 1122 loss: 3.608551025390625\n",
      "step 1123 loss: 3.7356154918670654\n",
      "step 1124 loss: 3.6624581813812256\n",
      "step 1125 loss: 3.6300947666168213\n",
      "step 1126 loss: 3.6306936740875244\n",
      "step 1127 loss: 3.60614275932312\n",
      "step 1128 loss: 3.5736145973205566\n",
      "step 1129 loss: 3.67541766166687\n",
      "step 1130 loss: 3.5313472747802734\n",
      "step 1131 loss: 3.6371285915374756\n",
      "step 1132 loss: 3.661085844039917\n",
      "step 1133 loss: 3.6264193058013916\n",
      "step 1134 loss: 3.650089979171753\n",
      "step 1135 loss: 3.53578782081604\n",
      "step 1136 loss: 3.6060991287231445\n",
      "step 1137 loss: 3.6430013179779053\n",
      "step 1138 loss: 3.591032028198242\n",
      "step 1139 loss: 3.6671526432037354\n",
      "step 1140 loss: 3.5971174240112305\n",
      "step 1141 loss: 3.654073476791382\n",
      "step 1142 loss: 3.67277455329895\n",
      "step 1143 loss: 3.5566089153289795\n",
      "step 1144 loss: 3.725360155105591\n",
      "step 1145 loss: 3.5462851524353027\n",
      "step 1146 loss: 3.59515380859375\n",
      "step 1147 loss: 3.6235764026641846\n",
      "step 1148 loss: 3.64681077003479\n",
      "step 1149 loss: 3.632132053375244\n",
      "step 1150 loss: 3.59301495552063\n",
      "step 1151 loss: 3.6381375789642334\n",
      "step 1152 loss: 3.5832936763763428\n",
      "step 1153 loss: 3.7322165966033936\n",
      "step 1154 loss: 3.7393901348114014\n",
      "step 1155 loss: 3.6387319564819336\n",
      "step 1156 loss: 3.5988409519195557\n",
      "step 1157 loss: 3.558255434036255\n",
      "step 1158 loss: 3.6498284339904785\n",
      "step 1159 loss: 3.6019906997680664\n",
      "step 1160 loss: 3.597728967666626\n",
      "step 1161 loss: 3.634012460708618\n",
      "step 1162 loss: 3.6564154624938965\n",
      "step 1163 loss: 3.568612575531006\n",
      "step 1164 loss: 3.6068599224090576\n",
      "step 1165 loss: 3.6166608333587646\n",
      "step 1166 loss: 3.686493158340454\n",
      "step 1167 loss: 3.619938373565674\n",
      "step 1168 loss: 3.6338977813720703\n",
      "step 1169 loss: 3.5989279747009277\n",
      "step 1170 loss: 3.6961400508880615\n",
      "step 1171 loss: 3.5462446212768555\n",
      "step 1172 loss: 3.6098079681396484\n",
      "step 1173 loss: 3.578995943069458\n",
      "step 1174 loss: 3.4836041927337646\n",
      "step 1175 loss: 3.605802536010742\n",
      "step 1176 loss: 3.5357978343963623\n",
      "step 1177 loss: 3.612697124481201\n",
      "step 1178 loss: 3.599095344543457\n",
      "step 1179 loss: 3.5906851291656494\n",
      "step 1180 loss: 3.536933660507202\n",
      "step 1181 loss: 3.6215319633483887\n",
      "step 1182 loss: 3.5636608600616455\n",
      "step 1183 loss: 3.6064906120300293\n",
      "step 1184 loss: 3.606163740158081\n",
      "step 1185 loss: 3.5634865760803223\n",
      "step 1186 loss: 3.5161335468292236\n",
      "step 1187 loss: 3.605494499206543\n",
      "step 1188 loss: 3.6346232891082764\n",
      "step 1189 loss: 3.4788002967834473\n",
      "step 1190 loss: 3.673884868621826\n",
      "step 1191 loss: 3.596086025238037\n",
      "step 1192 loss: 3.53511381149292\n",
      "step 1193 loss: 3.579685688018799\n",
      "step 1194 loss: 3.669189214706421\n",
      "step 1195 loss: 3.5924501419067383\n",
      "step 1196 loss: 3.5153987407684326\n",
      "step 1197 loss: 3.540961503982544\n",
      "step 1198 loss: 3.521974802017212\n",
      "step 1199 loss: 3.5528454780578613\n",
      "step 1200 loss: 3.533822536468506\n",
      "step 1201 loss: 3.676227331161499\n",
      "step 1202 loss: 3.608130693435669\n",
      "step 1203 loss: 3.5625159740448\n",
      "step 1204 loss: 3.5699970722198486\n",
      "step 1205 loss: 3.65990948677063\n",
      "step 1206 loss: 3.5319058895111084\n",
      "step 1207 loss: 3.531675100326538\n",
      "step 1208 loss: 3.499455451965332\n",
      "step 1209 loss: 3.592872142791748\n",
      "step 1210 loss: 3.480402708053589\n",
      "step 1211 loss: 3.6119701862335205\n",
      "step 1212 loss: 3.5069470405578613\n",
      "step 1213 loss: 3.5654523372650146\n",
      "step 1214 loss: 3.6222214698791504\n",
      "step 1215 loss: 3.6037495136260986\n",
      "step 1216 loss: 3.6392030715942383\n",
      "step 1217 loss: 3.6475131511688232\n",
      "step 1218 loss: 3.5623769760131836\n",
      "step 1219 loss: 3.6562585830688477\n",
      "step 1220 loss: 3.5285916328430176\n",
      "step 1221 loss: 3.588643789291382\n",
      "step 1222 loss: 3.7325491905212402\n",
      "step 1223 loss: 3.5281307697296143\n",
      "step 1224 loss: 3.594722032546997\n",
      "step 1225 loss: 3.549257278442383\n",
      "step 1226 loss: 3.482416868209839\n",
      "step 1227 loss: 3.5967857837677\n",
      "step 1228 loss: 3.5934231281280518\n",
      "step 1229 loss: 3.5675957202911377\n",
      "step 1230 loss: 3.558568000793457\n",
      "step 1231 loss: 3.5310235023498535\n",
      "step 1232 loss: 3.5351061820983887\n",
      "step 1233 loss: 3.5376667976379395\n",
      "step 1234 loss: 3.6422414779663086\n",
      "step 1235 loss: 3.5549561977386475\n",
      "step 1236 loss: 3.568449020385742\n",
      "step 1237 loss: 3.574716329574585\n",
      "step 1238 loss: 3.513176202774048\n",
      "step 1239 loss: 3.6505048274993896\n",
      "step 1240 loss: 3.69789719581604\n",
      "step 1241 loss: 3.4686460494995117\n",
      "step 1242 loss: 3.5311734676361084\n",
      "step 1243 loss: 3.5277929306030273\n",
      "step 1244 loss: 3.642239570617676\n",
      "step 1245 loss: 3.5299875736236572\n",
      "step 1246 loss: 3.5473744869232178\n",
      "step 1247 loss: 3.537161111831665\n",
      "step 1248 loss: 3.494866371154785\n",
      "step 1249 loss: 3.5672976970672607\n",
      "step 1250 loss: 3.4565792083740234\n",
      "step 1251 loss: 3.4242982864379883\n",
      "step 1252 loss: 3.5626370906829834\n",
      "step 1253 loss: 3.549572229385376\n",
      "step 1254 loss: 3.620471715927124\n",
      "step 1255 loss: 3.583460569381714\n",
      "step 1256 loss: 3.3880443572998047\n",
      "step 1257 loss: 3.4392428398132324\n",
      "step 1258 loss: 3.444765567779541\n",
      "step 1259 loss: 3.554847002029419\n",
      "step 1260 loss: 3.585571050643921\n",
      "step 1261 loss: 3.673006057739258\n",
      "step 1262 loss: 3.540569543838501\n",
      "step 1263 loss: 3.6246097087860107\n",
      "step 1264 loss: 3.518397331237793\n",
      "step 1265 loss: 3.60261607170105\n",
      "step 1266 loss: 3.559687614440918\n",
      "step 1267 loss: 3.4958128929138184\n",
      "step 1268 loss: 3.6792619228363037\n",
      "step 1269 loss: 3.4790027141571045\n",
      "step 1270 loss: 3.5444893836975098\n",
      "step 1271 loss: 3.5725343227386475\n",
      "step 1272 loss: 3.478005886077881\n",
      "step 1273 loss: 3.3706791400909424\n",
      "step 1274 loss: 3.5792136192321777\n",
      "step 1275 loss: 3.549891471862793\n",
      "step 1276 loss: 3.5755667686462402\n",
      "step 1277 loss: 3.467986583709717\n",
      "step 1278 loss: 3.484262704849243\n",
      "step 1279 loss: 3.535595655441284\n",
      "step 1280 loss: 3.4771032333374023\n",
      "step 1281 loss: 3.454075813293457\n",
      "step 1282 loss: 3.550774574279785\n",
      "step 1283 loss: 3.53173565864563\n",
      "step 1284 loss: 3.5272130966186523\n",
      "step 1285 loss: 3.513310432434082\n",
      "step 1286 loss: 3.489943027496338\n",
      "step 1287 loss: 3.547034502029419\n",
      "step 1288 loss: 3.5532150268554688\n",
      "step 1289 loss: 3.4090635776519775\n",
      "step 1290 loss: 3.5374679565429688\n",
      "step 1291 loss: 3.4746036529541016\n",
      "step 1292 loss: 3.500748634338379\n",
      "step 1293 loss: 3.572253704071045\n",
      "step 1294 loss: 3.5745222568511963\n",
      "step 1295 loss: 3.519819736480713\n",
      "step 1296 loss: 3.478593349456787\n",
      "step 1297 loss: 3.554482936859131\n",
      "step 1298 loss: 3.581031322479248\n",
      "step 1299 loss: 3.4376158714294434\n",
      "step 1300 loss: 3.513596773147583\n",
      "step 1301 loss: 3.4920475482940674\n",
      "step 1302 loss: 3.6158807277679443\n",
      "step 1303 loss: 3.4778759479522705\n",
      "step 1304 loss: 3.4746806621551514\n",
      "step 1305 loss: 3.476297378540039\n",
      "step 1306 loss: 3.5685088634490967\n",
      "step 1307 loss: 3.582345962524414\n",
      "step 1308 loss: 3.624885082244873\n",
      "step 1309 loss: 3.5399792194366455\n",
      "step 1310 loss: 3.455087661743164\n",
      "step 1311 loss: 3.505512237548828\n",
      "step 1312 loss: 3.452834129333496\n",
      "step 1313 loss: 3.4823296070098877\n",
      "step 1314 loss: 3.44356107711792\n",
      "step 1315 loss: 3.4874770641326904\n",
      "step 1316 loss: 3.500288724899292\n",
      "step 1317 loss: 3.4307103157043457\n",
      "step 1318 loss: 3.5101258754730225\n",
      "step 1319 loss: 3.5510165691375732\n",
      "step 1320 loss: 3.4752910137176514\n",
      "step 1321 loss: 3.451035976409912\n",
      "step 1322 loss: 3.476080894470215\n",
      "step 1323 loss: 3.503736734390259\n",
      "step 1324 loss: 3.5053741931915283\n",
      "step 1325 loss: 3.44270658493042\n",
      "step 1326 loss: 3.4427266120910645\n",
      "step 1327 loss: 3.5845072269439697\n",
      "step 1328 loss: 3.4452524185180664\n",
      "step 1329 loss: 3.5391461849212646\n",
      "step 1330 loss: 3.4656264781951904\n",
      "step 1331 loss: 3.45918869972229\n",
      "step 1332 loss: 3.4729905128479004\n",
      "step 1333 loss: 3.5081980228424072\n",
      "step 1334 loss: 3.4671852588653564\n",
      "step 1335 loss: 3.43593168258667\n",
      "step 1336 loss: 3.5770747661590576\n",
      "step 1337 loss: 3.4848225116729736\n",
      "step 1338 loss: 3.5730502605438232\n",
      "step 1339 loss: 3.528606653213501\n",
      "step 1340 loss: 3.5636637210845947\n",
      "step 1341 loss: 3.4763176441192627\n",
      "step 1342 loss: 3.4208364486694336\n",
      "step 1343 loss: 3.5142157077789307\n",
      "step 1344 loss: 3.4705793857574463\n",
      "step 1345 loss: 3.4796066284179688\n",
      "step 1346 loss: 3.5235705375671387\n",
      "step 1347 loss: 3.498183250427246\n",
      "step 1348 loss: 3.4660837650299072\n",
      "step 1349 loss: 3.501591205596924\n",
      "step 1350 loss: 3.4976627826690674\n",
      "step 1351 loss: 3.4792208671569824\n",
      "step 1352 loss: 3.450889825820923\n",
      "step 1353 loss: 3.381554365158081\n",
      "step 1354 loss: 3.5182647705078125\n",
      "step 1355 loss: 3.451977252960205\n",
      "step 1356 loss: 3.5372869968414307\n",
      "step 1357 loss: 3.5014307498931885\n",
      "step 1358 loss: 3.4820356369018555\n",
      "step 1359 loss: 3.487736225128174\n",
      "step 1360 loss: 3.6065621376037598\n",
      "step 1361 loss: 3.4635696411132812\n",
      "step 1362 loss: 3.5209856033325195\n",
      "step 1363 loss: 3.5945627689361572\n",
      "step 1364 loss: 3.442713975906372\n",
      "step 1365 loss: 3.4943442344665527\n",
      "step 1366 loss: 3.4089651107788086\n",
      "step 1367 loss: 3.5215280055999756\n",
      "step 1368 loss: 3.499279022216797\n",
      "step 1369 loss: 3.557582139968872\n",
      "step 1370 loss: 3.446476697921753\n",
      "step 1371 loss: 3.3893063068389893\n",
      "step 1372 loss: 3.4325053691864014\n",
      "step 1373 loss: 3.4126508235931396\n",
      "step 1374 loss: 3.5027875900268555\n",
      "step 1375 loss: 3.4731569290161133\n",
      "step 1376 loss: 3.4370691776275635\n",
      "step 1377 loss: 3.4045300483703613\n",
      "step 1378 loss: 3.4892375469207764\n",
      "step 1379 loss: 3.539316177368164\n",
      "step 1380 loss: 3.4670279026031494\n",
      "step 1381 loss: 3.4488909244537354\n",
      "step 1382 loss: 3.3743369579315186\n",
      "step 1383 loss: 3.47956919670105\n",
      "step 1384 loss: 3.487053871154785\n",
      "step 1385 loss: 3.4808387756347656\n",
      "step 1386 loss: 3.5195953845977783\n",
      "step 1387 loss: 3.606743574142456\n",
      "step 1388 loss: 3.557830333709717\n",
      "step 1389 loss: 3.369206666946411\n",
      "step 1390 loss: 3.4444103240966797\n",
      "step 1391 loss: 3.4894752502441406\n",
      "step 1392 loss: 3.4343056678771973\n",
      "step 1393 loss: 3.466531991958618\n",
      "step 1394 loss: 3.492295265197754\n",
      "step 1395 loss: 3.452609062194824\n",
      "step 1396 loss: 3.4838783740997314\n",
      "step 1397 loss: 3.408015012741089\n",
      "step 1398 loss: 3.410783052444458\n",
      "step 1399 loss: 3.440384864807129\n",
      "step 1400 loss: 3.4971799850463867\n",
      "step 1401 loss: 3.4848155975341797\n",
      "step 1402 loss: 3.498465061187744\n",
      "step 1403 loss: 3.4244847297668457\n",
      "step 1404 loss: 3.392991542816162\n",
      "step 1405 loss: 3.5131728649139404\n",
      "step 1406 loss: 3.4683103561401367\n",
      "step 1407 loss: 3.4236955642700195\n",
      "step 1408 loss: 3.5569069385528564\n",
      "step 1409 loss: 3.4428374767303467\n",
      "step 1410 loss: 3.4160428047180176\n",
      "step 1411 loss: 3.492375135421753\n",
      "step 1412 loss: 3.3810582160949707\n",
      "step 1413 loss: 3.3566431999206543\n",
      "step 1414 loss: 3.467700719833374\n",
      "step 1415 loss: 3.427262306213379\n",
      "step 1416 loss: 3.433197498321533\n",
      "step 1417 loss: 3.4431450366973877\n",
      "step 1418 loss: 3.4958982467651367\n",
      "step 1419 loss: 3.412886619567871\n",
      "step 1420 loss: 3.397977352142334\n",
      "step 1421 loss: 3.3626813888549805\n",
      "step 1422 loss: 3.5162065029144287\n",
      "step 1423 loss: 3.477705478668213\n",
      "step 1424 loss: 3.4201319217681885\n",
      "step 1425 loss: 3.397433280944824\n",
      "step 1426 loss: 3.361482858657837\n",
      "step 1427 loss: 3.369211435317993\n",
      "step 1428 loss: 3.4093215465545654\n",
      "step 1429 loss: 3.451183557510376\n",
      "step 1430 loss: 3.411647319793701\n",
      "step 1431 loss: 3.396657943725586\n",
      "step 1432 loss: 3.410034418106079\n",
      "step 1433 loss: 3.3729896545410156\n",
      "step 1434 loss: 3.442838191986084\n",
      "step 1435 loss: 3.3322513103485107\n",
      "step 1436 loss: 3.348400115966797\n",
      "step 1437 loss: 3.456611394882202\n",
      "step 1438 loss: 3.4619789123535156\n",
      "step 1439 loss: 3.4653542041778564\n",
      "step 1440 loss: 3.508078098297119\n",
      "step 1441 loss: 3.358595848083496\n",
      "step 1442 loss: 3.3429627418518066\n",
      "step 1443 loss: 3.475673198699951\n",
      "step 1444 loss: 3.409705400466919\n",
      "step 1445 loss: 3.4374051094055176\n",
      "step 1446 loss: 3.418816328048706\n",
      "step 1447 loss: 3.4159698486328125\n",
      "step 1448 loss: 3.404510021209717\n",
      "step 1449 loss: 3.4849002361297607\n",
      "step 1450 loss: 3.3295693397521973\n",
      "step 1451 loss: 3.4949190616607666\n",
      "step 1452 loss: 3.410362720489502\n",
      "step 1453 loss: 3.4048240184783936\n",
      "step 1454 loss: 3.4675955772399902\n",
      "step 1455 loss: 3.407223701477051\n",
      "step 1456 loss: 3.415804147720337\n",
      "step 1457 loss: 3.456226110458374\n",
      "step 1458 loss: 3.384314775466919\n",
      "step 1459 loss: 3.3859024047851562\n",
      "step 1460 loss: 3.4400761127471924\n",
      "step 1461 loss: 3.346078634262085\n",
      "step 1462 loss: 3.4144415855407715\n",
      "step 1463 loss: 3.3979368209838867\n",
      "step 1464 loss: 3.4159605503082275\n",
      "step 1465 loss: 3.4459404945373535\n",
      "step 1466 loss: 3.438652515411377\n",
      "step 1467 loss: 3.427842378616333\n",
      "step 1468 loss: 3.4048073291778564\n",
      "step 1469 loss: 3.4059362411499023\n",
      "step 1470 loss: 3.453827381134033\n",
      "step 1471 loss: 3.340776205062866\n",
      "step 1472 loss: 3.331699848175049\n",
      "step 1473 loss: 3.305809736251831\n",
      "step 1474 loss: 3.4490389823913574\n",
      "step 1475 loss: 3.456044912338257\n",
      "step 1476 loss: 3.4365649223327637\n",
      "step 1477 loss: 3.441457509994507\n",
      "step 1478 loss: 3.438816785812378\n",
      "step 1479 loss: 3.4209628105163574\n",
      "step 1480 loss: 3.3413846492767334\n",
      "step 1481 loss: 3.344409465789795\n",
      "step 1482 loss: 3.397526741027832\n",
      "step 1483 loss: 3.3796653747558594\n",
      "step 1484 loss: 3.413365602493286\n",
      "step 1485 loss: 3.405921697616577\n",
      "step 1486 loss: 3.4498698711395264\n",
      "step 1487 loss: 3.327824354171753\n",
      "step 1488 loss: 3.4651477336883545\n",
      "step 1489 loss: 3.3988351821899414\n",
      "step 1490 loss: 3.3846395015716553\n",
      "step 1491 loss: 3.3542604446411133\n",
      "step 1492 loss: 3.4854605197906494\n",
      "step 1493 loss: 3.336419105529785\n",
      "step 1494 loss: 3.408950090408325\n",
      "step 1495 loss: 3.4014241695404053\n",
      "step 1496 loss: 3.4073519706726074\n",
      "step 1497 loss: 3.404332399368286\n",
      "step 1498 loss: 3.3855996131896973\n",
      "step 1499 loss: 3.405637741088867\n",
      "step 1500 loss: 3.3378093242645264\n",
      "step 1501 loss: 3.3733749389648438\n",
      "step 1502 loss: 3.291133165359497\n",
      "step 1503 loss: 3.394235134124756\n",
      "step 1504 loss: 3.3222408294677734\n",
      "step 1505 loss: 3.381092071533203\n",
      "step 1506 loss: 3.366412878036499\n",
      "step 1507 loss: 3.485015869140625\n",
      "step 1508 loss: 3.3801515102386475\n",
      "step 1509 loss: 3.2766427993774414\n",
      "step 1510 loss: 3.4018139839172363\n",
      "step 1511 loss: 3.3726556301116943\n",
      "step 1512 loss: 3.4333419799804688\n",
      "step 1513 loss: 3.344499111175537\n",
      "step 1514 loss: 3.3264951705932617\n",
      "step 1515 loss: 3.286846876144409\n",
      "step 1516 loss: 3.348593235015869\n",
      "step 1517 loss: 3.3997581005096436\n",
      "step 1518 loss: 3.3655943870544434\n",
      "step 1519 loss: 3.3861515522003174\n",
      "step 1520 loss: 3.392791271209717\n",
      "step 1521 loss: 3.304910898208618\n",
      "step 1522 loss: 3.404515027999878\n",
      "step 1523 loss: 3.318579912185669\n",
      "step 1524 loss: 3.390424966812134\n",
      "step 1525 loss: 3.3497262001037598\n",
      "step 1526 loss: 3.4074699878692627\n",
      "step 1527 loss: 3.361961841583252\n",
      "step 1528 loss: 3.3432838916778564\n",
      "step 1529 loss: 3.390036106109619\n",
      "step 1530 loss: 3.3922626972198486\n",
      "step 1531 loss: 3.4557275772094727\n",
      "step 1532 loss: 3.3674776554107666\n",
      "step 1533 loss: 3.373843193054199\n",
      "step 1534 loss: 3.2946550846099854\n",
      "step 1535 loss: 3.3256590366363525\n",
      "step 1536 loss: 3.4375739097595215\n",
      "step 1537 loss: 3.3700549602508545\n",
      "step 1538 loss: 3.415087938308716\n",
      "step 1539 loss: 3.3288538455963135\n",
      "step 1540 loss: 3.313502788543701\n",
      "step 1541 loss: 3.3391382694244385\n",
      "step 1542 loss: 3.422729015350342\n",
      "step 1543 loss: 3.422409772872925\n",
      "step 1544 loss: 3.315507173538208\n",
      "step 1545 loss: 3.311030864715576\n",
      "step 1546 loss: 3.399080991744995\n",
      "step 1547 loss: 3.323190689086914\n",
      "step 1548 loss: 3.3115034103393555\n",
      "step 1549 loss: 3.41113543510437\n",
      "step 1550 loss: 3.316822052001953\n",
      "step 1551 loss: 3.3209197521209717\n",
      "step 1552 loss: 3.3485262393951416\n",
      "step 1553 loss: 3.358712673187256\n",
      "step 1554 loss: 3.4131977558135986\n",
      "step 1555 loss: 3.4150307178497314\n",
      "step 1556 loss: 3.3486032485961914\n",
      "step 1557 loss: 3.410818338394165\n",
      "step 1558 loss: 3.3043787479400635\n",
      "step 1559 loss: 3.3730320930480957\n",
      "step 1560 loss: 3.25028133392334\n",
      "step 1561 loss: 3.356779098510742\n",
      "step 1562 loss: 3.311847448348999\n",
      "step 1563 loss: 3.3913984298706055\n",
      "step 1564 loss: 3.323639392852783\n",
      "step 1565 loss: 3.348475694656372\n",
      "step 1566 loss: 3.384538173675537\n",
      "step 1567 loss: 3.291412353515625\n",
      "step 1568 loss: 3.2276663780212402\n",
      "step 1569 loss: 3.3490421772003174\n",
      "step 1570 loss: 3.3565621376037598\n",
      "step 1571 loss: 3.2832915782928467\n",
      "step 1572 loss: 3.3640520572662354\n",
      "step 1573 loss: 3.2977097034454346\n",
      "step 1574 loss: 3.342942476272583\n",
      "step 1575 loss: 3.3275911808013916\n",
      "step 1576 loss: 3.399881601333618\n",
      "step 1577 loss: 3.300980567932129\n",
      "step 1578 loss: 3.3213508129119873\n",
      "step 1579 loss: 3.3106155395507812\n",
      "step 1580 loss: 3.4147450923919678\n",
      "step 1581 loss: 3.3365442752838135\n",
      "step 1582 loss: 3.3031628131866455\n",
      "step 1583 loss: 3.309652090072632\n",
      "step 1584 loss: 3.3428876399993896\n",
      "step 1585 loss: 3.4316647052764893\n",
      "step 1586 loss: 3.354748010635376\n",
      "step 1587 loss: 3.434666156768799\n",
      "step 1588 loss: 3.23095703125\n",
      "step 1589 loss: 3.346879005432129\n",
      "step 1590 loss: 3.3354344367980957\n",
      "step 1591 loss: 3.1938226222991943\n",
      "step 1592 loss: 3.2639997005462646\n",
      "step 1593 loss: 3.3555922508239746\n",
      "step 1594 loss: 3.2773241996765137\n",
      "step 1595 loss: 3.225203514099121\n",
      "step 1596 loss: 3.311750650405884\n",
      "step 1597 loss: 3.354273557662964\n",
      "step 1598 loss: 3.3436810970306396\n",
      "step 1599 loss: 3.3504130840301514\n",
      "step 1600 loss: 3.3668527603149414\n",
      "step 1601 loss: 3.337010145187378\n",
      "step 1602 loss: 3.2369697093963623\n",
      "step 1603 loss: 3.2865219116210938\n",
      "step 1604 loss: 3.375467538833618\n",
      "step 1605 loss: 3.3234386444091797\n",
      "step 1606 loss: 3.3350417613983154\n",
      "step 1607 loss: 3.311195135116577\n",
      "step 1608 loss: 3.3273842334747314\n",
      "step 1609 loss: 3.2894725799560547\n",
      "step 1610 loss: 3.266592264175415\n",
      "step 1611 loss: 3.340492010116577\n",
      "step 1612 loss: 3.2490692138671875\n",
      "step 1613 loss: 3.4111292362213135\n",
      "step 1614 loss: 3.320774555206299\n",
      "step 1615 loss: 3.289039134979248\n",
      "step 1616 loss: 3.3254213333129883\n",
      "step 1617 loss: 3.3526666164398193\n",
      "step 1618 loss: 3.3712122440338135\n",
      "step 1619 loss: 3.3419723510742188\n",
      "step 1620 loss: 3.377053737640381\n",
      "step 1621 loss: 3.2222702503204346\n",
      "step 1622 loss: 3.3687376976013184\n",
      "step 1623 loss: 3.34375262260437\n",
      "step 1624 loss: 3.2827274799346924\n",
      "step 1625 loss: 3.3494460582733154\n",
      "step 1626 loss: 3.2800538539886475\n",
      "step 1627 loss: 3.364511013031006\n",
      "step 1628 loss: 3.3604555130004883\n",
      "step 1629 loss: 3.2629427909851074\n",
      "step 1630 loss: 3.3019356727600098\n",
      "step 1631 loss: 3.330825090408325\n",
      "step 1632 loss: 3.248178720474243\n",
      "step 1633 loss: 3.279142379760742\n",
      "step 1634 loss: 3.305049419403076\n",
      "step 1635 loss: 3.30605411529541\n",
      "step 1636 loss: 3.2742669582366943\n",
      "step 1637 loss: 3.2547197341918945\n",
      "step 1638 loss: 3.3662543296813965\n",
      "step 1639 loss: 3.2779734134674072\n",
      "step 1640 loss: 3.271064281463623\n",
      "step 1641 loss: 3.330867052078247\n",
      "step 1642 loss: 3.376645088195801\n",
      "step 1643 loss: 3.2705588340759277\n",
      "step 1644 loss: 3.288431167602539\n",
      "step 1645 loss: 3.3676278591156006\n",
      "step 1646 loss: 3.285165309906006\n",
      "step 1647 loss: 3.2972571849823\n",
      "step 1648 loss: 3.3084611892700195\n",
      "step 1649 loss: 3.3007194995880127\n",
      "step 1650 loss: 3.3552908897399902\n",
      "step 1651 loss: 3.361701250076294\n",
      "step 1652 loss: 3.2581543922424316\n",
      "step 1653 loss: 3.2975471019744873\n",
      "step 1654 loss: 3.3554422855377197\n",
      "step 1655 loss: 3.2319412231445312\n",
      "step 1656 loss: 3.259697675704956\n",
      "step 1657 loss: 3.2078163623809814\n",
      "step 1658 loss: 3.168574810028076\n",
      "step 1659 loss: 3.3635427951812744\n",
      "step 1660 loss: 3.292365074157715\n",
      "step 1661 loss: 3.173616886138916\n",
      "step 1662 loss: 3.341648578643799\n",
      "step 1663 loss: 3.3102986812591553\n",
      "step 1664 loss: 3.313289165496826\n",
      "step 1665 loss: 3.3792293071746826\n",
      "step 1666 loss: 3.262455940246582\n",
      "step 1667 loss: 3.230750560760498\n",
      "step 1668 loss: 3.328913450241089\n",
      "step 1669 loss: 3.118701457977295\n",
      "step 1670 loss: 3.286607265472412\n",
      "step 1671 loss: 3.176698684692383\n",
      "step 1672 loss: 3.2971556186676025\n",
      "step 1673 loss: 3.2876248359680176\n",
      "step 1674 loss: 3.349937915802002\n",
      "step 1675 loss: 3.263390064239502\n",
      "step 1676 loss: 3.2462873458862305\n",
      "step 1677 loss: 3.382678985595703\n",
      "step 1678 loss: 3.3313584327697754\n",
      "step 1679 loss: 3.3109073638916016\n",
      "step 1680 loss: 3.2078819274902344\n",
      "step 1681 loss: 3.2761425971984863\n",
      "step 1682 loss: 3.3135581016540527\n",
      "step 1683 loss: 3.27169132232666\n",
      "step 1684 loss: 3.267695903778076\n",
      "step 1685 loss: 3.217452049255371\n",
      "step 1686 loss: 3.356968879699707\n",
      "step 1687 loss: 3.2600700855255127\n",
      "step 1688 loss: 3.278160810470581\n",
      "step 1689 loss: 3.2158443927764893\n",
      "step 1690 loss: 3.295006036758423\n",
      "step 1691 loss: 3.215156316757202\n",
      "step 1692 loss: 3.2866291999816895\n",
      "step 1693 loss: 3.3293707370758057\n",
      "step 1694 loss: 3.2844321727752686\n",
      "step 1695 loss: 3.2246928215026855\n",
      "step 1696 loss: 3.3091495037078857\n",
      "step 1697 loss: 3.2070295810699463\n",
      "step 1698 loss: 3.3323819637298584\n",
      "step 1699 loss: 3.327280044555664\n",
      "step 1700 loss: 3.2826080322265625\n",
      "step 1701 loss: 3.224522829055786\n",
      "step 1702 loss: 3.2409608364105225\n",
      "step 1703 loss: 3.2651093006134033\n",
      "step 1704 loss: 3.214683771133423\n",
      "step 1705 loss: 3.291255474090576\n",
      "step 1706 loss: 3.2126660346984863\n",
      "step 1707 loss: 3.23405385017395\n",
      "step 1708 loss: 3.273935079574585\n",
      "step 1709 loss: 3.282559633255005\n",
      "step 1710 loss: 3.141706705093384\n",
      "step 1711 loss: 3.285360097885132\n",
      "step 1712 loss: 3.2963712215423584\n",
      "step 1713 loss: 3.2170867919921875\n",
      "step 1714 loss: 3.2659213542938232\n",
      "step 1715 loss: 3.211547374725342\n",
      "step 1716 loss: 3.2908804416656494\n",
      "step 1717 loss: 3.234668493270874\n",
      "step 1718 loss: 3.279135227203369\n",
      "step 1719 loss: 3.18255615234375\n",
      "step 1720 loss: 3.318256378173828\n",
      "step 1721 loss: 3.223573684692383\n",
      "step 1722 loss: 3.2774465084075928\n",
      "step 1723 loss: 3.209981679916382\n",
      "step 1724 loss: 3.255476236343384\n",
      "step 1725 loss: 3.1764402389526367\n",
      "step 1726 loss: 3.2720534801483154\n",
      "step 1727 loss: 3.17098331451416\n",
      "step 1728 loss: 3.2219371795654297\n",
      "step 1729 loss: 3.1781816482543945\n",
      "step 1730 loss: 3.2039265632629395\n",
      "step 1731 loss: 3.3307602405548096\n",
      "step 1732 loss: 3.3250796794891357\n",
      "step 1733 loss: 3.2241435050964355\n",
      "step 1734 loss: 3.276714563369751\n",
      "step 1735 loss: 3.265209436416626\n",
      "step 1736 loss: 3.289771556854248\n",
      "step 1737 loss: 3.3330209255218506\n",
      "step 1738 loss: 3.2607760429382324\n",
      "step 1739 loss: 3.249166250228882\n",
      "step 1740 loss: 3.217733860015869\n",
      "step 1741 loss: 3.286982297897339\n",
      "step 1742 loss: 3.2728865146636963\n",
      "step 1743 loss: 3.063701868057251\n",
      "step 1744 loss: 3.1571342945098877\n",
      "step 1745 loss: 3.230231761932373\n",
      "step 1746 loss: 3.129680871963501\n",
      "step 1747 loss: 3.2726259231567383\n",
      "step 1748 loss: 3.1371636390686035\n",
      "step 1749 loss: 3.2263519763946533\n",
      "step 1750 loss: 3.2038166522979736\n",
      "step 1751 loss: 3.235271453857422\n",
      "step 1752 loss: 3.21915602684021\n",
      "step 1753 loss: 3.2071752548217773\n",
      "step 1754 loss: 3.2918059825897217\n",
      "step 1755 loss: 3.224608898162842\n",
      "step 1756 loss: 3.21454119682312\n",
      "step 1757 loss: 3.2643697261810303\n",
      "step 1758 loss: 3.3764524459838867\n",
      "step 1759 loss: 3.176510810852051\n",
      "step 1760 loss: 3.156837224960327\n",
      "step 1761 loss: 3.2190630435943604\n",
      "step 1762 loss: 3.1816844940185547\n",
      "step 1763 loss: 3.1843903064727783\n",
      "step 1764 loss: 3.2644832134246826\n",
      "step 1765 loss: 3.2227492332458496\n",
      "step 1766 loss: 3.301292657852173\n",
      "step 1767 loss: 3.226567029953003\n",
      "step 1768 loss: 3.3772735595703125\n",
      "step 1769 loss: 3.244814872741699\n",
      "step 1770 loss: 3.212254047393799\n",
      "step 1771 loss: 3.178436517715454\n",
      "step 1772 loss: 3.163334608078003\n",
      "step 1773 loss: 3.1904470920562744\n",
      "step 1774 loss: 3.280255079269409\n",
      "step 1775 loss: 3.3053736686706543\n",
      "step 1776 loss: 3.1256823539733887\n",
      "step 1777 loss: 3.1902706623077393\n",
      "step 1778 loss: 3.2186930179595947\n",
      "step 1779 loss: 3.2689125537872314\n",
      "step 1780 loss: 3.199646472930908\n",
      "step 1781 loss: 3.174687623977661\n",
      "step 1782 loss: 3.1387863159179688\n",
      "step 1783 loss: 3.308959484100342\n",
      "step 1784 loss: 3.237354040145874\n",
      "step 1785 loss: 3.244534969329834\n",
      "step 1786 loss: 3.259204387664795\n",
      "step 1787 loss: 3.1780154705047607\n",
      "step 1788 loss: 3.3217532634735107\n",
      "step 1789 loss: 3.2772741317749023\n",
      "step 1790 loss: 3.238548517227173\n",
      "step 1791 loss: 3.250688076019287\n",
      "step 1792 loss: 3.2160236835479736\n",
      "step 1793 loss: 3.2117388248443604\n",
      "step 1794 loss: 3.2160110473632812\n",
      "step 1795 loss: 3.3423478603363037\n",
      "step 1796 loss: 3.2529423236846924\n",
      "step 1797 loss: 3.2250874042510986\n",
      "step 1798 loss: 3.320399045944214\n",
      "step 1799 loss: 3.210890293121338\n",
      "step 1800 loss: 3.1327052116394043\n",
      "step 1801 loss: 3.249723196029663\n",
      "step 1802 loss: 3.1129026412963867\n",
      "step 1803 loss: 3.1808221340179443\n",
      "step 1804 loss: 3.2004764080047607\n",
      "step 1805 loss: 3.2399744987487793\n",
      "step 1806 loss: 3.1919538974761963\n",
      "step 1807 loss: 3.271249294281006\n",
      "step 1808 loss: 3.201392889022827\n",
      "step 1809 loss: 3.1895792484283447\n",
      "step 1810 loss: 3.322889804840088\n",
      "step 1811 loss: 3.2075893878936768\n",
      "step 1812 loss: 3.2429747581481934\n",
      "step 1813 loss: 3.188906192779541\n",
      "step 1814 loss: 3.244816303253174\n",
      "step 1815 loss: 3.1107430458068848\n",
      "step 1816 loss: 3.0891263484954834\n",
      "step 1817 loss: 3.163414239883423\n",
      "step 1818 loss: 3.2470407485961914\n",
      "step 1819 loss: 3.204618453979492\n",
      "step 1820 loss: 3.200762987136841\n",
      "step 1821 loss: 3.13240909576416\n",
      "step 1822 loss: 3.185868978500366\n",
      "step 1823 loss: 3.1417665481567383\n",
      "step 1824 loss: 3.2103981971740723\n",
      "step 1825 loss: 3.201212167739868\n",
      "step 1826 loss: 3.234140157699585\n",
      "step 1827 loss: 3.18730092048645\n",
      "step 1828 loss: 3.1864540576934814\n",
      "step 1829 loss: 3.3220059871673584\n",
      "step 1830 loss: 3.251852512359619\n",
      "step 1831 loss: 3.1155738830566406\n",
      "step 1832 loss: 3.360062837600708\n",
      "step 1833 loss: 3.255850076675415\n",
      "step 1834 loss: 3.228135108947754\n",
      "step 1835 loss: 3.221987009048462\n",
      "step 1836 loss: 3.207165241241455\n",
      "step 1837 loss: 3.1727402210235596\n",
      "step 1838 loss: 3.2283694744110107\n",
      "step 1839 loss: 3.283872127532959\n",
      "step 1840 loss: 3.2128117084503174\n",
      "step 1841 loss: 3.0964980125427246\n",
      "step 1842 loss: 3.1870110034942627\n",
      "step 1843 loss: 3.144282341003418\n",
      "step 1844 loss: 3.1265153884887695\n",
      "step 1845 loss: 3.218003034591675\n",
      "step 1846 loss: 3.22285532951355\n",
      "step 1847 loss: 3.246044397354126\n",
      "step 1848 loss: 3.0454623699188232\n",
      "step 1849 loss: 3.2382287979125977\n",
      "step 1850 loss: 3.094738245010376\n",
      "step 1851 loss: 3.220698595046997\n",
      "step 1852 loss: 3.1401326656341553\n",
      "step 1853 loss: 3.197850227355957\n",
      "step 1854 loss: 3.1631710529327393\n",
      "step 1855 loss: 3.153327226638794\n",
      "step 1856 loss: 3.2077174186706543\n",
      "step 1857 loss: 3.291868209838867\n",
      "step 1858 loss: 3.166853427886963\n",
      "step 1859 loss: 3.1814773082733154\n",
      "step 1860 loss: 3.2674601078033447\n",
      "step 1861 loss: 3.1994736194610596\n",
      "step 1862 loss: 3.2288818359375\n",
      "step 1863 loss: 3.1274449825286865\n",
      "step 1864 loss: 3.234846830368042\n",
      "step 1865 loss: 3.194779396057129\n",
      "step 1866 loss: 3.2093429565429688\n",
      "step 1867 loss: 3.2593448162078857\n",
      "step 1868 loss: 3.2067928314208984\n",
      "step 1869 loss: 3.052320718765259\n",
      "step 1870 loss: 3.1047773361206055\n",
      "step 1871 loss: 3.125150203704834\n",
      "step 1872 loss: 3.2055468559265137\n",
      "step 1873 loss: 3.137179136276245\n",
      "step 1874 loss: 3.11620831489563\n",
      "step 1875 loss: 3.036351203918457\n",
      "step 1876 loss: 3.1316850185394287\n",
      "step 1877 loss: 3.246335506439209\n",
      "step 1878 loss: 3.168938398361206\n",
      "step 1879 loss: 3.1985912322998047\n",
      "step 1880 loss: 3.150218963623047\n",
      "step 1881 loss: 3.1943061351776123\n",
      "step 1882 loss: 3.126917839050293\n",
      "step 1883 loss: 3.2779834270477295\n",
      "step 1884 loss: 3.111590623855591\n",
      "step 1885 loss: 3.1134002208709717\n",
      "step 1886 loss: 3.2154603004455566\n",
      "step 1887 loss: 3.220374345779419\n",
      "step 1888 loss: 3.1807429790496826\n",
      "step 1889 loss: 3.1501805782318115\n",
      "step 1890 loss: 3.2500786781311035\n",
      "step 1891 loss: 3.157017946243286\n",
      "step 1892 loss: 3.2325985431671143\n",
      "step 1893 loss: 3.123624563217163\n",
      "step 1894 loss: 3.262256622314453\n",
      "step 1895 loss: 3.2147715091705322\n",
      "step 1896 loss: 3.099865674972534\n",
      "step 1897 loss: 3.142479181289673\n",
      "step 1898 loss: 3.175543785095215\n",
      "step 1899 loss: 3.2939796447753906\n",
      "step 1900 loss: 3.160909652709961\n",
      "step 1901 loss: 3.135530471801758\n",
      "step 1902 loss: 3.1320390701293945\n",
      "step 1903 loss: 3.228038787841797\n",
      "step 1904 loss: 3.151289939880371\n",
      "step 1905 loss: 3.18462872505188\n",
      "step 1906 loss: 3.07747220993042\n",
      "step 1907 loss: 3.196981906890869\n",
      "step 1908 loss: 3.1517722606658936\n",
      "step 1909 loss: 3.225595474243164\n",
      "step 1910 loss: 3.244776487350464\n",
      "step 1911 loss: 3.102879285812378\n",
      "step 1912 loss: 3.240483522415161\n",
      "step 1913 loss: 3.0807878971099854\n",
      "step 1914 loss: 3.1213858127593994\n",
      "step 1915 loss: 3.13806414604187\n",
      "step 1916 loss: 3.2108466625213623\n",
      "step 1917 loss: 3.1105730533599854\n",
      "step 1918 loss: 3.2080416679382324\n",
      "step 1919 loss: 3.1623358726501465\n",
      "step 1920 loss: 3.1880764961242676\n",
      "step 1921 loss: 3.217818021774292\n",
      "step 1922 loss: 3.2317981719970703\n",
      "step 1923 loss: 3.1472525596618652\n",
      "step 1924 loss: 3.132002592086792\n",
      "step 1925 loss: 3.1392624378204346\n",
      "step 1926 loss: 3.2661826610565186\n",
      "step 1927 loss: 3.1531004905700684\n",
      "step 1928 loss: 3.082653522491455\n",
      "step 1929 loss: 3.1041059494018555\n",
      "step 1930 loss: 3.2715048789978027\n",
      "step 1931 loss: 3.056511878967285\n",
      "step 1932 loss: 3.186967611312866\n",
      "step 1933 loss: 3.1010212898254395\n",
      "step 1934 loss: 3.128485918045044\n",
      "step 1935 loss: 3.1375393867492676\n",
      "step 1936 loss: 3.1445817947387695\n",
      "step 1937 loss: 3.1923725605010986\n",
      "step 1938 loss: 3.1659610271453857\n",
      "step 1939 loss: 3.081956624984741\n",
      "step 1940 loss: 3.169597864151001\n",
      "step 1941 loss: 3.1387245655059814\n",
      "step 1942 loss: 3.185239315032959\n",
      "step 1943 loss: 3.1716246604919434\n",
      "step 1944 loss: 3.174926996231079\n",
      "step 1945 loss: 3.1537413597106934\n",
      "step 1946 loss: 3.150068998336792\n",
      "step 1947 loss: 3.1514041423797607\n",
      "step 1948 loss: 3.075385332107544\n",
      "step 1949 loss: 3.2018001079559326\n",
      "step 1950 loss: 3.131441354751587\n",
      "step 1951 loss: 3.179110050201416\n",
      "step 1952 loss: 3.1819400787353516\n",
      "step 1953 loss: 3.1091976165771484\n",
      "step 1954 loss: 3.2071468830108643\n",
      "step 1955 loss: 3.2594144344329834\n",
      "step 1956 loss: 3.169245481491089\n",
      "step 1957 loss: 3.32069730758667\n",
      "step 1958 loss: 3.195711851119995\n",
      "step 1959 loss: 3.0469696521759033\n",
      "step 1960 loss: 3.185786485671997\n",
      "step 1961 loss: 3.098173141479492\n",
      "step 1962 loss: 3.1470048427581787\n",
      "step 1963 loss: 3.06870174407959\n",
      "step 1964 loss: 3.189624071121216\n",
      "step 1965 loss: 3.156919479370117\n",
      "step 1966 loss: 3.1558985710144043\n",
      "step 1967 loss: 3.175633430480957\n",
      "step 1968 loss: 3.0341174602508545\n",
      "step 1969 loss: 3.161806583404541\n",
      "step 1970 loss: 3.072429895401001\n",
      "step 1971 loss: 3.113632917404175\n",
      "step 1972 loss: 3.1447010040283203\n",
      "step 1973 loss: 3.1411798000335693\n",
      "step 1974 loss: 3.04371976852417\n",
      "step 1975 loss: 3.2081587314605713\n",
      "step 1976 loss: 3.133478879928589\n",
      "step 1977 loss: 3.1254935264587402\n",
      "step 1978 loss: 3.1489741802215576\n",
      "step 1979 loss: 3.1555161476135254\n",
      "step 1980 loss: 3.192533493041992\n",
      "step 1981 loss: 3.099370241165161\n",
      "step 1982 loss: 3.286879539489746\n",
      "step 1983 loss: 3.0001885890960693\n",
      "step 1984 loss: 3.212430715560913\n",
      "step 1985 loss: 3.1058502197265625\n",
      "step 1986 loss: 3.1873512268066406\n",
      "step 1987 loss: 3.182231903076172\n",
      "step 1988 loss: 3.1797308921813965\n",
      "step 1989 loss: 3.0570228099823\n",
      "step 1990 loss: 3.131434917449951\n",
      "step 1991 loss: 3.164717435836792\n",
      "step 1992 loss: 3.0997354984283447\n",
      "step 1993 loss: 3.1176295280456543\n",
      "step 1994 loss: 3.155418872833252\n",
      "step 1995 loss: 3.1908223628997803\n",
      "step 1996 loss: 3.1161835193634033\n",
      "step 1997 loss: 3.1683945655822754\n",
      "step 1998 loss: 3.116499900817871\n",
      "step 1999 loss: 3.106309175491333\n",
      "step 2000 loss: 3.2342257499694824\n",
      "step 2001 loss: 3.192662000656128\n",
      "step 2002 loss: 3.1981866359710693\n",
      "step 2003 loss: 3.0375094413757324\n",
      "step 2004 loss: 3.1391961574554443\n",
      "step 2005 loss: 3.1250216960906982\n",
      "step 2006 loss: 3.208712339401245\n",
      "step 2007 loss: 3.163727045059204\n",
      "step 2008 loss: 3.096379280090332\n",
      "step 2009 loss: 3.1669631004333496\n",
      "step 2010 loss: 3.243758201599121\n",
      "step 2011 loss: 3.164602041244507\n",
      "step 2012 loss: 3.136106252670288\n",
      "step 2013 loss: 3.1171836853027344\n",
      "step 2014 loss: 3.1838247776031494\n",
      "step 2015 loss: 3.0679514408111572\n",
      "step 2016 loss: 3.1610188484191895\n",
      "step 2017 loss: 3.177401304244995\n",
      "step 2018 loss: 3.151343822479248\n",
      "step 2019 loss: 3.053217887878418\n",
      "step 2020 loss: 3.0940818786621094\n",
      "step 2021 loss: 3.1165249347686768\n",
      "step 2022 loss: 3.1759018898010254\n",
      "step 2023 loss: 3.072417974472046\n",
      "step 2024 loss: 3.0636918544769287\n",
      "step 2025 loss: 3.1357948780059814\n",
      "step 2026 loss: 3.116898536682129\n",
      "step 2027 loss: 3.0812458992004395\n",
      "step 2028 loss: 3.1482725143432617\n",
      "step 2029 loss: 3.0730016231536865\n",
      "step 2030 loss: 3.1360464096069336\n",
      "step 2031 loss: 3.1525650024414062\n",
      "step 2032 loss: 3.11092209815979\n",
      "step 2033 loss: 3.1118884086608887\n",
      "step 2034 loss: 3.088418483734131\n",
      "step 2035 loss: 3.1658072471618652\n",
      "step 2036 loss: 3.1689722537994385\n",
      "step 2037 loss: 3.222921371459961\n",
      "step 2038 loss: 3.1229310035705566\n",
      "step 2039 loss: 3.1140708923339844\n",
      "step 2040 loss: 3.024508237838745\n",
      "step 2041 loss: 3.1445958614349365\n",
      "step 2042 loss: 3.158327579498291\n",
      "step 2043 loss: 3.088998556137085\n",
      "step 2044 loss: 3.1170620918273926\n",
      "step 2045 loss: 3.158742666244507\n",
      "step 2046 loss: 3.1927483081817627\n",
      "step 2047 loss: 3.0994889736175537\n",
      "step 2048 loss: 3.045720338821411\n",
      "step 2049 loss: 3.053609609603882\n",
      "step 2050 loss: 3.1358845233917236\n",
      "step 2051 loss: 3.107856512069702\n",
      "step 2052 loss: 3.1966917514801025\n",
      "step 2053 loss: 2.995530605316162\n",
      "step 2054 loss: 3.064422845840454\n",
      "step 2055 loss: 3.124194383621216\n",
      "step 2056 loss: 3.026811122894287\n",
      "step 2057 loss: 3.1407864093780518\n",
      "step 2058 loss: 3.098668336868286\n",
      "step 2059 loss: 3.105461835861206\n",
      "step 2060 loss: 3.1461143493652344\n",
      "step 2061 loss: 3.082618236541748\n",
      "step 2062 loss: 3.0890064239501953\n",
      "step 2063 loss: 3.1567068099975586\n",
      "step 2064 loss: 3.064009189605713\n",
      "step 2065 loss: 3.0947444438934326\n",
      "step 2066 loss: 3.087893009185791\n",
      "step 2067 loss: 3.082144260406494\n",
      "step 2068 loss: 3.1125309467315674\n",
      "step 2069 loss: 3.058459520339966\n",
      "step 2070 loss: 3.015188694000244\n",
      "step 2071 loss: 3.0212113857269287\n",
      "step 2072 loss: 3.061570644378662\n",
      "step 2073 loss: 3.061093807220459\n",
      "step 2074 loss: 3.110684871673584\n",
      "step 2075 loss: 3.0000524520874023\n",
      "step 2076 loss: 3.1331374645233154\n",
      "step 2077 loss: 3.0624492168426514\n",
      "step 2078 loss: 3.126819610595703\n",
      "step 2079 loss: 3.1514151096343994\n",
      "step 2080 loss: 2.9802768230438232\n",
      "step 2081 loss: 3.1085002422332764\n",
      "step 2082 loss: 3.1144049167633057\n",
      "step 2083 loss: 2.992621421813965\n",
      "step 2084 loss: 3.078420400619507\n",
      "step 2085 loss: 3.09555983543396\n",
      "step 2086 loss: 3.056344747543335\n",
      "step 2087 loss: 3.0702292919158936\n",
      "step 2088 loss: 3.101361036300659\n",
      "step 2089 loss: 3.0713396072387695\n",
      "step 2090 loss: 3.1256263256073\n",
      "step 2091 loss: 2.967482089996338\n",
      "step 2092 loss: 3.0711770057678223\n",
      "step 2093 loss: 3.080427646636963\n",
      "step 2094 loss: 3.192387819290161\n",
      "step 2095 loss: 3.0058205127716064\n",
      "step 2096 loss: 3.173752546310425\n",
      "step 2097 loss: 3.093513250350952\n",
      "step 2098 loss: 3.2017767429351807\n",
      "step 2099 loss: 3.07047963142395\n",
      "step 2100 loss: 2.997836112976074\n",
      "step 2101 loss: 3.0756053924560547\n",
      "step 2102 loss: 3.091844320297241\n",
      "step 2103 loss: 3.0713982582092285\n",
      "step 2104 loss: 3.1523263454437256\n",
      "step 2105 loss: 3.125617265701294\n",
      "step 2106 loss: 2.948585033416748\n",
      "step 2107 loss: 3.121544361114502\n",
      "step 2108 loss: 3.062269687652588\n",
      "step 2109 loss: 3.0960683822631836\n",
      "step 2110 loss: 2.9981930255889893\n",
      "step 2111 loss: 3.041565179824829\n",
      "step 2112 loss: 3.1301543712615967\n",
      "step 2113 loss: 3.1183888912200928\n",
      "step 2114 loss: 3.0431807041168213\n",
      "step 2115 loss: 3.1297574043273926\n",
      "step 2116 loss: 3.142927408218384\n",
      "step 2117 loss: 3.0842511653900146\n",
      "step 2118 loss: 3.1691794395446777\n",
      "step 2119 loss: 3.0298917293548584\n",
      "step 2120 loss: 3.063725471496582\n",
      "step 2121 loss: 3.105806589126587\n",
      "step 2122 loss: 3.1019790172576904\n",
      "step 2123 loss: 3.063411235809326\n",
      "step 2124 loss: 3.1391828060150146\n",
      "step 2125 loss: 3.168444871902466\n",
      "step 2126 loss: 3.054912805557251\n",
      "step 2127 loss: 2.9990649223327637\n",
      "step 2128 loss: 3.1093862056732178\n",
      "step 2129 loss: 3.059506416320801\n",
      "step 2130 loss: 3.0280919075012207\n",
      "step 2131 loss: 3.167435884475708\n",
      "step 2132 loss: 2.974466562271118\n",
      "step 2133 loss: 3.032482862472534\n",
      "step 2134 loss: 3.1270461082458496\n",
      "step 2135 loss: 3.026775598526001\n",
      "step 2136 loss: 3.0162699222564697\n",
      "step 2137 loss: 3.1095902919769287\n",
      "step 2138 loss: 3.065070867538452\n",
      "step 2139 loss: 3.0250349044799805\n",
      "step 2140 loss: 3.120211362838745\n",
      "step 2141 loss: 3.083777666091919\n",
      "step 2142 loss: 3.0140380859375\n",
      "step 2143 loss: 3.05186128616333\n",
      "step 2144 loss: 3.0248045921325684\n",
      "step 2145 loss: 2.9956586360931396\n",
      "step 2146 loss: 3.119506359100342\n",
      "step 2147 loss: 3.007261276245117\n",
      "step 2148 loss: 2.9384472370147705\n",
      "step 2149 loss: 2.980154514312744\n",
      "step 2150 loss: 3.059758186340332\n",
      "step 2151 loss: 3.0765528678894043\n",
      "step 2152 loss: 2.983393907546997\n",
      "step 2153 loss: 3.1258716583251953\n",
      "step 2154 loss: 3.142104148864746\n",
      "step 2155 loss: 3.00063157081604\n",
      "step 2156 loss: 3.062718152999878\n",
      "step 2157 loss: 3.1256113052368164\n",
      "step 2158 loss: 3.112558364868164\n",
      "step 2159 loss: 3.008800983428955\n",
      "step 2160 loss: 3.020928382873535\n",
      "step 2161 loss: 3.160475254058838\n",
      "step 2162 loss: 2.912400722503662\n",
      "step 2163 loss: 3.0491905212402344\n",
      "step 2164 loss: 3.0470492839813232\n",
      "step 2165 loss: 3.062143564224243\n",
      "step 2166 loss: 3.05753755569458\n",
      "step 2167 loss: 3.1165757179260254\n",
      "step 2168 loss: 3.021270990371704\n",
      "step 2169 loss: 3.048602819442749\n",
      "step 2170 loss: 3.14634108543396\n",
      "step 2171 loss: 3.039090394973755\n",
      "step 2172 loss: 2.9869306087493896\n",
      "step 2173 loss: 3.082068681716919\n",
      "step 2174 loss: 3.0846445560455322\n",
      "step 2175 loss: 3.058844804763794\n",
      "step 2176 loss: 3.019425392150879\n",
      "step 2177 loss: 2.9310855865478516\n",
      "step 2178 loss: 3.0032076835632324\n",
      "step 2179 loss: 3.0908403396606445\n",
      "step 2180 loss: 3.1210060119628906\n",
      "step 2181 loss: 3.006625175476074\n",
      "step 2182 loss: 3.0436384677886963\n",
      "step 2183 loss: 3.023030996322632\n",
      "step 2184 loss: 2.9817137718200684\n",
      "step 2185 loss: 2.992163896560669\n",
      "step 2186 loss: 3.052245855331421\n",
      "step 2187 loss: 2.9561357498168945\n",
      "step 2188 loss: 3.095294237136841\n",
      "step 2189 loss: 3.045602560043335\n",
      "step 2190 loss: 3.1471750736236572\n",
      "step 2191 loss: 3.088322162628174\n",
      "step 2192 loss: 2.986521005630493\n",
      "step 2193 loss: 2.98210072517395\n",
      "step 2194 loss: 3.073169469833374\n",
      "step 2195 loss: 3.0062756538391113\n",
      "step 2196 loss: 3.0434317588806152\n",
      "step 2197 loss: 3.016982078552246\n",
      "step 2198 loss: 3.093862771987915\n",
      "step 2199 loss: 2.97415828704834\n",
      "step 2200 loss: 3.094273090362549\n",
      "step 2201 loss: 3.091675043106079\n",
      "step 2202 loss: 2.968334674835205\n",
      "step 2203 loss: 3.0471882820129395\n",
      "step 2204 loss: 3.035881757736206\n",
      "step 2205 loss: 3.09565806388855\n",
      "step 2206 loss: 3.0410759449005127\n",
      "step 2207 loss: 3.085965633392334\n",
      "step 2208 loss: 2.9549319744110107\n",
      "step 2209 loss: 2.994889736175537\n",
      "step 2210 loss: 3.0826518535614014\n",
      "step 2211 loss: 3.0826821327209473\n",
      "step 2212 loss: 3.0111234188079834\n",
      "step 2213 loss: 2.9896559715270996\n",
      "step 2214 loss: 3.0228309631347656\n",
      "step 2215 loss: 3.072681188583374\n",
      "step 2216 loss: 3.0437822341918945\n",
      "step 2217 loss: 3.0602867603302\n",
      "step 2218 loss: 2.950650453567505\n",
      "step 2219 loss: 3.010526180267334\n",
      "step 2220 loss: 3.0309083461761475\n",
      "step 2221 loss: 3.0729873180389404\n",
      "step 2222 loss: 3.093932867050171\n",
      "step 2223 loss: 2.983121156692505\n",
      "step 2224 loss: 2.979642868041992\n",
      "step 2225 loss: 3.113652229309082\n",
      "step 2226 loss: 2.972076416015625\n",
      "step 2227 loss: 2.9673867225646973\n",
      "step 2228 loss: 3.096421480178833\n",
      "step 2229 loss: 3.067521095275879\n",
      "step 2230 loss: 3.018721342086792\n",
      "step 2231 loss: 2.8611276149749756\n",
      "step 2232 loss: 3.0452983379364014\n",
      "step 2233 loss: 3.037595510482788\n",
      "step 2234 loss: 2.8922457695007324\n",
      "step 2235 loss: 3.0168943405151367\n",
      "step 2236 loss: 3.0649993419647217\n",
      "step 2237 loss: 3.070328712463379\n",
      "step 2238 loss: 3.0350804328918457\n",
      "step 2239 loss: 3.071204423904419\n",
      "step 2240 loss: 2.9586360454559326\n",
      "step 2241 loss: 3.0262954235076904\n",
      "step 2242 loss: 2.9674243927001953\n",
      "step 2243 loss: 3.0329582691192627\n",
      "step 2244 loss: 3.03653621673584\n",
      "step 2245 loss: 3.046546459197998\n",
      "step 2246 loss: 3.007124900817871\n",
      "step 2247 loss: 3.0861029624938965\n",
      "step 2248 loss: 3.0746755599975586\n",
      "step 2249 loss: 3.019538164138794\n",
      "step 2250 loss: 2.981079578399658\n",
      "step 2251 loss: 3.0212743282318115\n",
      "step 2252 loss: 2.9077975749969482\n",
      "step 2253 loss: 2.9750356674194336\n",
      "step 2254 loss: 3.004267930984497\n",
      "step 2255 loss: 3.048283815383911\n",
      "step 2256 loss: 3.030189037322998\n",
      "step 2257 loss: 2.983283281326294\n",
      "step 2258 loss: 3.0582826137542725\n",
      "step 2259 loss: 3.0551917552948\n",
      "step 2260 loss: 3.0029377937316895\n",
      "step 2261 loss: 3.035351514816284\n",
      "step 2262 loss: 2.9667646884918213\n",
      "step 2263 loss: 2.988797426223755\n",
      "step 2264 loss: 2.958547592163086\n",
      "step 2265 loss: 3.069365978240967\n",
      "step 2266 loss: 3.0911989212036133\n",
      "step 2267 loss: 3.071723699569702\n",
      "step 2268 loss: 3.0338754653930664\n",
      "step 2269 loss: 3.0854926109313965\n",
      "step 2270 loss: 3.115708112716675\n",
      "step 2271 loss: 3.0668437480926514\n",
      "step 2272 loss: 3.034860372543335\n",
      "step 2273 loss: 3.071761131286621\n",
      "step 2274 loss: 3.110304355621338\n",
      "step 2275 loss: 3.05482816696167\n",
      "step 2276 loss: 3.008540153503418\n",
      "step 2277 loss: 3.004573345184326\n",
      "step 2278 loss: 2.901270866394043\n",
      "step 2279 loss: 3.093810558319092\n",
      "step 2280 loss: 3.0905463695526123\n",
      "step 2281 loss: 2.931100368499756\n",
      "step 2282 loss: 2.9067671298980713\n",
      "step 2283 loss: 2.9288737773895264\n",
      "step 2284 loss: 3.023658514022827\n",
      "step 2285 loss: 2.948885440826416\n",
      "step 2286 loss: 3.0051820278167725\n",
      "step 2287 loss: 2.9549875259399414\n",
      "step 2288 loss: 2.9915709495544434\n",
      "step 2289 loss: 2.973637104034424\n",
      "step 2290 loss: 2.8764092922210693\n",
      "step 2291 loss: 2.9713363647460938\n",
      "step 2292 loss: 3.073154926300049\n",
      "step 2293 loss: 3.019841194152832\n",
      "step 2294 loss: 2.9362521171569824\n",
      "step 2295 loss: 2.971374273300171\n",
      "step 2296 loss: 2.984898805618286\n",
      "step 2297 loss: 3.0371603965759277\n",
      "step 2298 loss: 3.0483012199401855\n",
      "step 2299 loss: 3.007462978363037\n",
      "step 2300 loss: 2.9780402183532715\n",
      "step 2301 loss: 2.996206283569336\n",
      "step 2302 loss: 2.9307899475097656\n",
      "step 2303 loss: 3.070986747741699\n",
      "step 2304 loss: 2.9916257858276367\n",
      "step 2305 loss: 2.929560422897339\n",
      "step 2306 loss: 3.040900707244873\n",
      "step 2307 loss: 2.9153127670288086\n",
      "step 2308 loss: 2.938925266265869\n",
      "step 2309 loss: 2.955275774002075\n",
      "step 2310 loss: 3.035935401916504\n",
      "step 2311 loss: 2.9836513996124268\n",
      "step 2312 loss: 2.949766159057617\n",
      "step 2313 loss: 3.0265145301818848\n",
      "step 2314 loss: 3.0128095149993896\n",
      "step 2315 loss: 3.088517665863037\n",
      "step 2316 loss: 3.013533353805542\n",
      "step 2317 loss: 3.0040371417999268\n",
      "step 2318 loss: 3.031724452972412\n",
      "step 2319 loss: 3.004011392593384\n",
      "step 2320 loss: 3.0415663719177246\n",
      "step 2321 loss: 2.9581360816955566\n",
      "step 2322 loss: 3.0060842037200928\n",
      "step 2323 loss: 3.05680251121521\n",
      "step 2324 loss: 3.0739846229553223\n",
      "step 2325 loss: 3.0162320137023926\n",
      "step 2326 loss: 3.0091912746429443\n",
      "step 2327 loss: 3.046360969543457\n",
      "step 2328 loss: 3.0126588344573975\n",
      "step 2329 loss: 2.981369733810425\n",
      "step 2330 loss: 3.055697202682495\n",
      "step 2331 loss: 2.9622390270233154\n",
      "step 2332 loss: 2.965867519378662\n",
      "step 2333 loss: 2.969882011413574\n",
      "step 2334 loss: 2.9465951919555664\n",
      "step 2335 loss: 2.867687463760376\n",
      "step 2336 loss: 2.9184844493865967\n",
      "step 2337 loss: 3.0131428241729736\n",
      "step 2338 loss: 2.988752603530884\n",
      "step 2339 loss: 3.0312702655792236\n",
      "step 2340 loss: 2.935131549835205\n",
      "step 2341 loss: 3.1447410583496094\n",
      "step 2342 loss: 2.8944814205169678\n",
      "step 2343 loss: 2.9747731685638428\n",
      "step 2344 loss: 3.0070667266845703\n",
      "step 2345 loss: 3.0328800678253174\n",
      "step 2346 loss: 2.9630279541015625\n",
      "step 2347 loss: 2.988734722137451\n",
      "step 2348 loss: 2.860597848892212\n",
      "step 2349 loss: 2.9690256118774414\n",
      "step 2350 loss: 2.9739346504211426\n",
      "step 2351 loss: 3.000663995742798\n",
      "step 2352 loss: 3.09907865524292\n",
      "step 2353 loss: 2.9064018726348877\n",
      "step 2354 loss: 2.999448776245117\n",
      "step 2355 loss: 3.0052475929260254\n",
      "step 2356 loss: 2.874826669692993\n",
      "step 2357 loss: 2.847533702850342\n",
      "step 2358 loss: 2.9915802478790283\n",
      "step 2359 loss: 2.9807519912719727\n",
      "step 2360 loss: 3.0360448360443115\n",
      "step 2361 loss: 3.018771171569824\n",
      "step 2362 loss: 2.915461540222168\n",
      "step 2363 loss: 3.0029995441436768\n",
      "step 2364 loss: 2.9483251571655273\n",
      "step 2365 loss: 2.850005626678467\n",
      "step 2366 loss: 2.9657516479492188\n",
      "step 2367 loss: 3.053544044494629\n",
      "step 2368 loss: 3.0547704696655273\n",
      "step 2369 loss: 2.940958261489868\n",
      "step 2370 loss: 3.0255887508392334\n",
      "step 2371 loss: 3.0180187225341797\n",
      "step 2372 loss: 3.0242302417755127\n",
      "step 2373 loss: 3.0047106742858887\n",
      "step 2374 loss: 2.9896061420440674\n",
      "step 2375 loss: 2.973383903503418\n",
      "step 2376 loss: 3.0549004077911377\n",
      "step 2377 loss: 2.849839925765991\n",
      "step 2378 loss: 2.997772216796875\n",
      "step 2379 loss: 2.997324228286743\n",
      "step 2380 loss: 3.050563097000122\n",
      "step 2381 loss: 2.939620018005371\n",
      "step 2382 loss: 2.96584153175354\n",
      "step 2383 loss: 3.060051441192627\n",
      "step 2384 loss: 2.975240707397461\n",
      "step 2385 loss: 3.001016855239868\n",
      "step 2386 loss: 3.0342273712158203\n",
      "step 2387 loss: 3.0045032501220703\n",
      "step 2388 loss: 3.0330746173858643\n",
      "step 2389 loss: 3.0269744396209717\n",
      "step 2390 loss: 2.9479875564575195\n",
      "step 2391 loss: 2.990034818649292\n",
      "step 2392 loss: 2.8992233276367188\n",
      "step 2393 loss: 2.8968122005462646\n",
      "step 2394 loss: 2.9739975929260254\n",
      "step 2395 loss: 2.911991834640503\n",
      "step 2396 loss: 2.98856520652771\n",
      "step 2397 loss: 3.1184792518615723\n",
      "step 2398 loss: 2.8506078720092773\n",
      "step 2399 loss: 2.9123175144195557\n",
      "step 2400 loss: 2.890953302383423\n",
      "step 2401 loss: 3.0099358558654785\n",
      "step 2402 loss: 2.985617160797119\n",
      "step 2403 loss: 2.9691879749298096\n",
      "step 2404 loss: 3.0679500102996826\n",
      "step 2405 loss: 2.899034023284912\n",
      "step 2406 loss: 3.035090208053589\n",
      "step 2407 loss: 2.9615588188171387\n",
      "step 2408 loss: 2.920084238052368\n",
      "step 2409 loss: 2.9397635459899902\n",
      "step 2410 loss: 2.952460289001465\n",
      "step 2411 loss: 2.97115421295166\n",
      "step 2412 loss: 2.8342368602752686\n",
      "step 2413 loss: 2.93896484375\n",
      "step 2414 loss: 2.9093573093414307\n",
      "step 2415 loss: 2.9869256019592285\n",
      "step 2416 loss: 3.060795545578003\n",
      "step 2417 loss: 2.8667805194854736\n",
      "step 2418 loss: 3.007077217102051\n",
      "step 2419 loss: 2.9285852909088135\n",
      "step 2420 loss: 2.975198984146118\n",
      "step 2421 loss: 2.9594411849975586\n",
      "step 2422 loss: 3.0469837188720703\n",
      "step 2423 loss: 3.0046582221984863\n",
      "step 2424 loss: 2.988501787185669\n",
      "step 2425 loss: 2.836934804916382\n",
      "step 2426 loss: 3.0194737911224365\n",
      "step 2427 loss: 2.8743984699249268\n",
      "step 2428 loss: 3.0155301094055176\n",
      "step 2429 loss: 2.898211717605591\n",
      "step 2430 loss: 3.0712432861328125\n",
      "step 2431 loss: 2.914293050765991\n",
      "step 2432 loss: 2.996744394302368\n",
      "step 2433 loss: 2.9687795639038086\n",
      "step 2434 loss: 2.961577892303467\n",
      "step 2435 loss: 2.976954936981201\n",
      "step 2436 loss: 2.9921536445617676\n",
      "step 2437 loss: 2.916638135910034\n",
      "step 2438 loss: 2.8982675075531006\n",
      "step 2439 loss: 2.944255828857422\n",
      "step 2440 loss: 3.0338244438171387\n",
      "step 2441 loss: 2.9656691551208496\n",
      "step 2442 loss: 2.9124226570129395\n",
      "step 2443 loss: 2.96372127532959\n",
      "step 2444 loss: 2.933896064758301\n",
      "step 2445 loss: 2.873572826385498\n",
      "step 2446 loss: 2.9677388668060303\n",
      "step 2447 loss: 2.9150278568267822\n",
      "step 2448 loss: 2.9800474643707275\n",
      "step 2449 loss: 3.035402774810791\n",
      "step 2450 loss: 2.9740378856658936\n",
      "step 2451 loss: 3.0292539596557617\n",
      "step 2452 loss: 2.9490675926208496\n",
      "step 2453 loss: 2.9457690715789795\n",
      "step 2454 loss: 2.9588639736175537\n",
      "step 2455 loss: 3.000900983810425\n",
      "step 2456 loss: 2.9841558933258057\n",
      "step 2457 loss: 2.9722084999084473\n",
      "step 2458 loss: 2.887650966644287\n",
      "step 2459 loss: 3.0032200813293457\n",
      "step 2460 loss: 2.991499423980713\n",
      "step 2461 loss: 2.932875871658325\n",
      "step 2462 loss: 3.0872371196746826\n",
      "step 2463 loss: 3.0358314514160156\n",
      "step 2464 loss: 2.9020795822143555\n",
      "step 2465 loss: 2.9282407760620117\n",
      "step 2466 loss: 3.053882122039795\n",
      "step 2467 loss: 2.8522889614105225\n",
      "step 2468 loss: 2.8920297622680664\n",
      "step 2469 loss: 2.8491199016571045\n",
      "step 2470 loss: 2.9171464443206787\n",
      "step 2471 loss: 3.0009357929229736\n",
      "step 2472 loss: 2.968528985977173\n",
      "step 2473 loss: 2.9786646366119385\n",
      "step 2474 loss: 2.9037892818450928\n",
      "step 2475 loss: 2.8558743000030518\n",
      "step 2476 loss: 2.899996757507324\n",
      "step 2477 loss: 2.9606406688690186\n",
      "step 2478 loss: 2.8624801635742188\n",
      "step 2479 loss: 2.9499499797821045\n",
      "step 2480 loss: 3.0597054958343506\n",
      "step 2481 loss: 2.9087579250335693\n",
      "step 2482 loss: 2.868417978286743\n",
      "step 2483 loss: 2.868865489959717\n",
      "step 2484 loss: 2.9379870891571045\n",
      "step 2485 loss: 2.8843159675598145\n",
      "step 2486 loss: 2.9331588745117188\n",
      "step 2487 loss: 2.9070165157318115\n",
      "step 2488 loss: 2.899366617202759\n",
      "step 2489 loss: 2.937859535217285\n",
      "step 2490 loss: 2.9579527378082275\n",
      "step 2491 loss: 3.058429002761841\n",
      "step 2492 loss: 2.9821853637695312\n",
      "step 2493 loss: 2.930290460586548\n",
      "step 2494 loss: 3.0149312019348145\n",
      "step 2495 loss: 2.9589438438415527\n",
      "step 2496 loss: 2.9986507892608643\n",
      "step 2497 loss: 2.939690113067627\n",
      "step 2498 loss: 2.961465835571289\n",
      "step 2499 loss: 3.0473380088806152\n",
      "step 2500 loss: 2.939120292663574\n",
      "step 2501 loss: 2.9663517475128174\n",
      "step 2502 loss: 2.9551467895507812\n",
      "step 2503 loss: 2.999281167984009\n",
      "step 2504 loss: 2.968554973602295\n",
      "step 2505 loss: 2.9374735355377197\n",
      "step 2506 loss: 2.8750669956207275\n",
      "step 2507 loss: 3.0079338550567627\n",
      "step 2508 loss: 2.9116339683532715\n",
      "step 2509 loss: 3.0213637351989746\n",
      "step 2510 loss: 2.981294631958008\n",
      "step 2511 loss: 2.8537535667419434\n",
      "step 2512 loss: 2.860482692718506\n",
      "step 2513 loss: 2.8689749240875244\n",
      "step 2514 loss: 2.9326705932617188\n",
      "step 2515 loss: 2.903268814086914\n",
      "step 2516 loss: 2.888460159301758\n",
      "step 2517 loss: 2.8927841186523438\n",
      "step 2518 loss: 3.073896646499634\n",
      "step 2519 loss: 3.0002894401550293\n",
      "step 2520 loss: 3.0251951217651367\n",
      "step 2521 loss: 2.9397220611572266\n",
      "step 2522 loss: 3.0297651290893555\n",
      "step 2523 loss: 2.900331735610962\n",
      "step 2524 loss: 3.0084147453308105\n",
      "step 2525 loss: 2.9598212242126465\n",
      "step 2526 loss: 2.870678186416626\n",
      "step 2527 loss: 2.9026806354522705\n",
      "step 2528 loss: 2.871727228164673\n",
      "step 2529 loss: 2.936837911605835\n",
      "step 2530 loss: 2.910170316696167\n",
      "step 2531 loss: 3.014220952987671\n",
      "step 2532 loss: 2.880380153656006\n",
      "step 2533 loss: 2.9194703102111816\n",
      "step 2534 loss: 2.898409366607666\n",
      "step 2535 loss: 2.955778121948242\n",
      "step 2536 loss: 2.8324990272521973\n",
      "step 2537 loss: 3.0272529125213623\n",
      "step 2538 loss: 2.862553119659424\n",
      "step 2539 loss: 2.948810338973999\n",
      "step 2540 loss: 2.9480693340301514\n",
      "step 2541 loss: 2.8094656467437744\n",
      "step 2542 loss: 2.8953065872192383\n",
      "step 2543 loss: 2.911391258239746\n",
      "step 2544 loss: 3.019348382949829\n",
      "step 2545 loss: 3.000539541244507\n",
      "step 2546 loss: 2.9025046825408936\n",
      "step 2547 loss: 2.866441249847412\n",
      "step 2548 loss: 2.9241251945495605\n",
      "step 2549 loss: 2.897491931915283\n",
      "step 2550 loss: 2.976036548614502\n",
      "step 2551 loss: 2.9817144870758057\n",
      "step 2552 loss: 2.93776798248291\n",
      "step 2553 loss: 2.9804887771606445\n",
      "step 2554 loss: 2.860593795776367\n",
      "step 2555 loss: 2.899840831756592\n",
      "step 2556 loss: 2.789914131164551\n",
      "step 2557 loss: 2.936663866043091\n",
      "step 2558 loss: 3.0006699562072754\n",
      "step 2559 loss: 2.9450719356536865\n",
      "step 2560 loss: 2.953413248062134\n",
      "step 2561 loss: 2.949431896209717\n",
      "step 2562 loss: 2.909963369369507\n",
      "step 2563 loss: 2.9678549766540527\n",
      "step 2564 loss: 2.9237589836120605\n",
      "step 2565 loss: 2.9008209705352783\n",
      "step 2566 loss: 3.1115224361419678\n",
      "step 2567 loss: 2.8988351821899414\n",
      "step 2568 loss: 2.9235188961029053\n",
      "step 2569 loss: 3.0194149017333984\n",
      "step 2570 loss: 2.9500882625579834\n",
      "step 2571 loss: 2.8603427410125732\n",
      "step 2572 loss: 2.9851677417755127\n",
      "step 2573 loss: 2.8407034873962402\n",
      "step 2574 loss: 2.947836399078369\n",
      "step 2575 loss: 2.815678358078003\n",
      "step 2576 loss: 2.922020673751831\n",
      "step 2577 loss: 2.904895544052124\n",
      "step 2578 loss: 2.8790054321289062\n",
      "step 2579 loss: 3.021804094314575\n",
      "step 2580 loss: 2.9643027782440186\n",
      "step 2581 loss: 2.9296154975891113\n",
      "step 2582 loss: 2.998852252960205\n",
      "step 2583 loss: 2.8743374347686768\n",
      "step 2584 loss: 2.896848678588867\n",
      "step 2585 loss: 2.935192108154297\n",
      "step 2586 loss: 2.9012608528137207\n",
      "step 2587 loss: 2.8676772117614746\n",
      "step 2588 loss: 2.9225449562072754\n",
      "step 2589 loss: 2.9047794342041016\n",
      "step 2590 loss: 2.748443126678467\n",
      "step 2591 loss: 2.857957601547241\n",
      "step 2592 loss: 2.9205989837646484\n",
      "step 2593 loss: 2.8920819759368896\n",
      "step 2594 loss: 2.90090274810791\n",
      "step 2595 loss: 2.9787609577178955\n",
      "step 2596 loss: 2.979546308517456\n",
      "step 2597 loss: 2.844904661178589\n",
      "step 2598 loss: 2.944186210632324\n",
      "step 2599 loss: 2.984738349914551\n",
      "step 2600 loss: 2.8254292011260986\n",
      "step 2601 loss: 2.9743077754974365\n",
      "step 2602 loss: 2.8878393173217773\n",
      "step 2603 loss: 2.8745617866516113\n",
      "step 2604 loss: 2.821476936340332\n",
      "step 2605 loss: 2.9011518955230713\n",
      "step 2606 loss: 2.9766077995300293\n",
      "step 2607 loss: 2.8848912715911865\n",
      "step 2608 loss: 2.8246445655822754\n",
      "step 2609 loss: 2.859372615814209\n",
      "step 2610 loss: 2.902937650680542\n",
      "step 2611 loss: 2.9010777473449707\n",
      "step 2612 loss: 2.9829883575439453\n",
      "step 2613 loss: 2.8114609718322754\n",
      "step 2614 loss: 2.9216253757476807\n",
      "step 2615 loss: 2.9335474967956543\n",
      "step 2616 loss: 2.860034465789795\n",
      "step 2617 loss: 3.0494794845581055\n",
      "step 2618 loss: 2.956784725189209\n",
      "step 2619 loss: 2.8508052825927734\n",
      "step 2620 loss: 2.8631722927093506\n",
      "step 2621 loss: 2.8649544715881348\n",
      "step 2622 loss: 2.885529041290283\n",
      "step 2623 loss: 2.9516122341156006\n",
      "step 2624 loss: 2.876950263977051\n",
      "step 2625 loss: 2.8488199710845947\n",
      "step 2626 loss: 2.8026092052459717\n",
      "step 2627 loss: 2.8609304428100586\n",
      "step 2628 loss: 2.854257345199585\n",
      "step 2629 loss: 2.9167988300323486\n",
      "step 2630 loss: 2.838812828063965\n",
      "step 2631 loss: 2.904604911804199\n",
      "step 2632 loss: 2.880666971206665\n",
      "step 2633 loss: 2.9021618366241455\n",
      "step 2634 loss: 2.8200085163116455\n",
      "step 2635 loss: 2.7620720863342285\n",
      "step 2636 loss: 2.8649823665618896\n",
      "step 2637 loss: 2.9472107887268066\n",
      "step 2638 loss: 3.0033464431762695\n",
      "step 2639 loss: 2.88669753074646\n",
      "step 2640 loss: 2.918212413787842\n",
      "step 2641 loss: 2.9005167484283447\n",
      "step 2642 loss: 2.8015010356903076\n",
      "step 2643 loss: 2.9258081912994385\n",
      "step 2644 loss: 2.952296257019043\n",
      "step 2645 loss: 2.8650834560394287\n",
      "step 2646 loss: 2.876555919647217\n",
      "step 2647 loss: 2.981654644012451\n",
      "step 2648 loss: 2.912996292114258\n",
      "step 2649 loss: 2.9101455211639404\n",
      "step 2650 loss: 2.8646628856658936\n",
      "step 2651 loss: 2.865129232406616\n",
      "step 2652 loss: 2.903671979904175\n",
      "step 2653 loss: 2.762280225753784\n",
      "step 2654 loss: 2.9457948207855225\n",
      "step 2655 loss: 2.9241795539855957\n",
      "step 2656 loss: 3.096498489379883\n",
      "step 2657 loss: 2.8957438468933105\n",
      "step 2658 loss: 2.8784446716308594\n",
      "step 2659 loss: 2.931885004043579\n",
      "step 2660 loss: 3.0715150833129883\n",
      "step 2661 loss: 2.876741409301758\n",
      "step 2662 loss: 2.90842604637146\n",
      "step 2663 loss: 2.9265975952148438\n",
      "step 2664 loss: 2.93289852142334\n",
      "step 2665 loss: 2.9342682361602783\n",
      "step 2666 loss: 2.8366405963897705\n",
      "step 2667 loss: 2.918262243270874\n",
      "step 2668 loss: 2.799647569656372\n",
      "step 2669 loss: 2.7895877361297607\n",
      "step 2670 loss: 2.9623398780822754\n",
      "step 2671 loss: 3.0178301334381104\n",
      "step 2672 loss: 2.8732895851135254\n",
      "step 2673 loss: 2.8682827949523926\n",
      "step 2674 loss: 2.804698944091797\n",
      "step 2675 loss: 2.9436516761779785\n",
      "step 2676 loss: 2.9708564281463623\n",
      "step 2677 loss: 2.8273990154266357\n",
      "step 2678 loss: 2.9477698802948\n",
      "step 2679 loss: 2.9165687561035156\n",
      "step 2680 loss: 2.9261856079101562\n",
      "step 2681 loss: 2.956766128540039\n",
      "step 2682 loss: 2.9455223083496094\n",
      "step 2683 loss: 2.811213254928589\n",
      "step 2684 loss: 2.857572555541992\n",
      "step 2685 loss: 2.8841230869293213\n",
      "step 2686 loss: 2.9136464595794678\n",
      "step 2687 loss: 2.8221304416656494\n",
      "step 2688 loss: 2.8318254947662354\n",
      "step 2689 loss: 2.8525772094726562\n",
      "step 2690 loss: 3.0062148571014404\n",
      "step 2691 loss: 2.8347580432891846\n",
      "step 2692 loss: 2.830392837524414\n",
      "step 2693 loss: 2.92844820022583\n",
      "step 2694 loss: 2.996718645095825\n",
      "step 2695 loss: 2.896207094192505\n",
      "step 2696 loss: 2.887281656265259\n",
      "step 2697 loss: 2.87225079536438\n",
      "step 2698 loss: 2.8424911499023438\n",
      "step 2699 loss: 2.8940341472625732\n",
      "step 2700 loss: 2.921311378479004\n",
      "step 2701 loss: 2.812138795852661\n",
      "step 2702 loss: 2.854605197906494\n",
      "step 2703 loss: 2.9350409507751465\n",
      "step 2704 loss: 2.86605167388916\n",
      "step 2705 loss: 2.9482696056365967\n",
      "step 2706 loss: 2.863374710083008\n",
      "step 2707 loss: 2.8647499084472656\n",
      "step 2708 loss: 2.8919754028320312\n",
      "step 2709 loss: 3.0028836727142334\n",
      "step 2710 loss: 2.823453187942505\n",
      "step 2711 loss: 2.9870223999023438\n",
      "step 2712 loss: 2.954379081726074\n",
      "step 2713 loss: 2.8947694301605225\n",
      "step 2714 loss: 2.9131383895874023\n",
      "step 2715 loss: 3.005513906478882\n",
      "step 2716 loss: 2.838592767715454\n",
      "step 2717 loss: 2.93471360206604\n",
      "step 2718 loss: 2.9906420707702637\n",
      "step 2719 loss: 2.97385835647583\n",
      "step 2720 loss: 2.892157793045044\n",
      "step 2721 loss: 2.903507947921753\n",
      "step 2722 loss: 2.815023899078369\n",
      "step 2723 loss: 2.902839183807373\n",
      "step 2724 loss: 3.0426700115203857\n",
      "step 2725 loss: 2.963442087173462\n",
      "step 2726 loss: 2.7408065795898438\n",
      "step 2727 loss: 2.7838051319122314\n",
      "step 2728 loss: 2.9928836822509766\n",
      "step 2729 loss: 2.8519248962402344\n",
      "step 2730 loss: 2.9618523120880127\n",
      "step 2731 loss: 2.8309593200683594\n",
      "step 2732 loss: 2.868056058883667\n",
      "step 2733 loss: 2.9729273319244385\n",
      "step 2734 loss: 2.8819336891174316\n",
      "step 2735 loss: 2.8968629837036133\n",
      "step 2736 loss: 2.842545986175537\n",
      "step 2737 loss: 2.984985589981079\n",
      "step 2738 loss: 2.7795181274414062\n",
      "step 2739 loss: 2.993828773498535\n",
      "step 2740 loss: 2.9344232082366943\n",
      "step 2741 loss: 2.871194362640381\n",
      "step 2742 loss: 2.786391496658325\n",
      "step 2743 loss: 2.8819050788879395\n",
      "step 2744 loss: 2.9221341609954834\n",
      "step 2745 loss: 2.9103000164031982\n",
      "step 2746 loss: 2.928647041320801\n",
      "step 2747 loss: 2.7629940509796143\n",
      "step 2748 loss: 2.8281095027923584\n",
      "step 2749 loss: 2.7695798873901367\n",
      "step 2750 loss: 2.794102907180786\n",
      "step 2751 loss: 2.9322168827056885\n",
      "step 2752 loss: 2.8284153938293457\n",
      "step 2753 loss: 2.94570255279541\n",
      "step 2754 loss: 2.8976383209228516\n",
      "step 2755 loss: 2.925488233566284\n",
      "step 2756 loss: 2.8418257236480713\n",
      "step 2757 loss: 2.887343406677246\n",
      "step 2758 loss: 2.8006534576416016\n",
      "step 2759 loss: 2.9399044513702393\n",
      "step 2760 loss: 2.9805686473846436\n",
      "step 2761 loss: 2.8315517902374268\n",
      "step 2762 loss: 2.711085796356201\n",
      "step 2763 loss: 2.8687455654144287\n",
      "step 2764 loss: 2.8855364322662354\n",
      "step 2765 loss: 2.7950117588043213\n",
      "step 2766 loss: 2.8535900115966797\n",
      "step 2767 loss: 2.822059154510498\n",
      "step 2768 loss: 2.9028055667877197\n",
      "step 2769 loss: 3.0022385120391846\n",
      "step 2770 loss: 2.867955207824707\n",
      "step 2771 loss: 2.8497142791748047\n",
      "step 2772 loss: 2.895268678665161\n",
      "step 2773 loss: 2.806471109390259\n",
      "step 2774 loss: 2.847947359085083\n",
      "step 2775 loss: 2.8625895977020264\n",
      "step 2776 loss: 2.9609270095825195\n",
      "step 2777 loss: 2.893651247024536\n",
      "step 2778 loss: 2.9725534915924072\n",
      "step 2779 loss: 2.809075117111206\n",
      "step 2780 loss: 2.8058342933654785\n",
      "step 2781 loss: 2.9577159881591797\n",
      "step 2782 loss: 2.8797123432159424\n",
      "step 2783 loss: 2.9425318241119385\n",
      "step 2784 loss: 2.8506739139556885\n",
      "step 2785 loss: 2.7124037742614746\n",
      "step 2786 loss: 2.781795024871826\n",
      "step 2787 loss: 2.8977909088134766\n",
      "step 2788 loss: 2.9363865852355957\n",
      "step 2789 loss: 2.76096248626709\n",
      "step 2790 loss: 2.856032609939575\n",
      "step 2791 loss: 2.7328739166259766\n",
      "step 2792 loss: 2.9256460666656494\n",
      "step 2793 loss: 2.881221055984497\n",
      "step 2794 loss: 2.8272430896759033\n",
      "step 2795 loss: 2.8588459491729736\n",
      "step 2796 loss: 2.8555185794830322\n",
      "step 2797 loss: 2.823786973953247\n",
      "step 2798 loss: 2.82114577293396\n",
      "step 2799 loss: 2.784607410430908\n",
      "step 2800 loss: 2.886559247970581\n",
      "step 2801 loss: 2.8887884616851807\n",
      "step 2802 loss: 2.728440761566162\n",
      "step 2803 loss: 2.8192267417907715\n",
      "step 2804 loss: 2.814833164215088\n",
      "step 2805 loss: 2.751030921936035\n",
      "step 2806 loss: 2.888805389404297\n",
      "step 2807 loss: 2.9396722316741943\n",
      "step 2808 loss: 2.8099021911621094\n",
      "step 2809 loss: 2.8146605491638184\n",
      "step 2810 loss: 2.763568162918091\n",
      "step 2811 loss: 2.808237075805664\n",
      "step 2812 loss: 2.942781925201416\n",
      "step 2813 loss: 2.8241963386535645\n",
      "step 2814 loss: 2.7395083904266357\n",
      "step 2815 loss: 2.7949841022491455\n",
      "step 2816 loss: 2.870112419128418\n",
      "step 2817 loss: 2.8154380321502686\n",
      "step 2818 loss: 2.859062433242798\n",
      "step 2819 loss: 2.7983763217926025\n",
      "step 2820 loss: 2.7847707271575928\n",
      "step 2821 loss: 2.83712100982666\n",
      "step 2822 loss: 2.7803072929382324\n",
      "step 2823 loss: 2.860912561416626\n",
      "step 2824 loss: 2.9105451107025146\n",
      "step 2825 loss: 2.776547908782959\n",
      "step 2826 loss: 2.816856861114502\n",
      "step 2827 loss: 2.8256711959838867\n",
      "step 2828 loss: 2.8026864528656006\n",
      "step 2829 loss: 2.8275842666625977\n",
      "step 2830 loss: 2.7411320209503174\n",
      "step 2831 loss: 2.8069396018981934\n",
      "step 2832 loss: 2.869436740875244\n",
      "step 2833 loss: 2.8873791694641113\n",
      "step 2834 loss: 2.8353872299194336\n",
      "step 2835 loss: 2.8616154193878174\n",
      "step 2836 loss: 2.8007113933563232\n",
      "step 2837 loss: 2.9464921951293945\n",
      "step 2838 loss: 2.9318385124206543\n",
      "step 2839 loss: 2.8519697189331055\n",
      "step 2840 loss: 2.8963329792022705\n",
      "step 2841 loss: 2.8843021392822266\n",
      "step 2842 loss: 2.8066227436065674\n",
      "step 2843 loss: 2.8369922637939453\n",
      "step 2844 loss: 2.8903744220733643\n",
      "step 2845 loss: 2.9168319702148438\n",
      "step 2846 loss: 2.7436063289642334\n",
      "step 2847 loss: 2.7637603282928467\n",
      "step 2848 loss: 2.9023776054382324\n",
      "step 2849 loss: 2.746628522872925\n",
      "step 2850 loss: 2.861863136291504\n",
      "step 2851 loss: 2.8845856189727783\n",
      "step 2852 loss: 2.8611385822296143\n",
      "step 2853 loss: 2.839643716812134\n",
      "step 2854 loss: 2.8060998916625977\n",
      "step 2855 loss: 2.728280782699585\n",
      "step 2856 loss: 2.834397315979004\n",
      "step 2857 loss: 2.7700018882751465\n",
      "step 2858 loss: 2.830190420150757\n",
      "step 2859 loss: 2.7264890670776367\n",
      "step 2860 loss: 2.934023857116699\n",
      "step 2861 loss: 2.9157652854919434\n",
      "step 2862 loss: 2.7522261142730713\n",
      "step 2863 loss: 2.798304796218872\n",
      "step 2864 loss: 2.6673805713653564\n",
      "step 2865 loss: 2.8005571365356445\n",
      "step 2866 loss: 2.789407730102539\n",
      "step 2867 loss: 2.918381452560425\n",
      "step 2868 loss: 2.700671672821045\n",
      "step 2869 loss: 2.8961918354034424\n",
      "step 2870 loss: 2.877861499786377\n",
      "step 2871 loss: 2.805449962615967\n",
      "step 2872 loss: 2.7884938716888428\n",
      "step 2873 loss: 2.808636426925659\n",
      "step 2874 loss: 2.815727472305298\n",
      "step 2875 loss: 2.9517886638641357\n",
      "step 2876 loss: 2.743434190750122\n",
      "step 2877 loss: 2.964573621749878\n",
      "step 2878 loss: 2.8295977115631104\n",
      "step 2879 loss: 2.7042770385742188\n",
      "step 2880 loss: 2.794942855834961\n",
      "step 2881 loss: 2.8569869995117188\n",
      "step 2882 loss: 2.8454360961914062\n",
      "step 2883 loss: 2.9098432064056396\n",
      "step 2884 loss: 2.8386194705963135\n",
      "step 2885 loss: 2.72922682762146\n",
      "step 2886 loss: 2.8013901710510254\n",
      "step 2887 loss: 2.7857553958892822\n",
      "step 2888 loss: 2.945580005645752\n",
      "step 2889 loss: 2.9032113552093506\n",
      "step 2890 loss: 2.812943696975708\n",
      "step 2891 loss: 2.7818689346313477\n",
      "step 2892 loss: 2.822779417037964\n",
      "step 2893 loss: 2.8255250453948975\n",
      "step 2894 loss: 2.7748639583587646\n",
      "step 2895 loss: 2.750889778137207\n",
      "step 2896 loss: 2.9075212478637695\n",
      "step 2897 loss: 2.7539010047912598\n",
      "step 2898 loss: 2.7346935272216797\n",
      "step 2899 loss: 2.8761208057403564\n",
      "step 2900 loss: 2.8697657585144043\n",
      "step 2901 loss: 2.9229955673217773\n",
      "step 2902 loss: 2.7631092071533203\n",
      "step 2903 loss: 2.881573438644409\n",
      "step 2904 loss: 2.7425456047058105\n",
      "step 2905 loss: 2.833815574645996\n",
      "step 2906 loss: 2.7507965564727783\n",
      "step 2907 loss: 2.7425742149353027\n",
      "step 2908 loss: 2.8010659217834473\n",
      "step 2909 loss: 2.82297420501709\n",
      "step 2910 loss: 2.818023443222046\n",
      "step 2911 loss: 2.7919483184814453\n",
      "step 2912 loss: 2.8886821269989014\n",
      "step 2913 loss: 2.80881404876709\n",
      "step 2914 loss: 2.8606433868408203\n",
      "step 2915 loss: 2.9385488033294678\n",
      "step 2916 loss: 2.7815909385681152\n",
      "step 2917 loss: 2.7943263053894043\n",
      "step 2918 loss: 2.804506301879883\n",
      "step 2919 loss: 2.689404249191284\n",
      "step 2920 loss: 2.809458017349243\n",
      "step 2921 loss: 2.8312931060791016\n",
      "step 2922 loss: 2.8092727661132812\n",
      "step 2923 loss: 2.8168163299560547\n",
      "step 2924 loss: 2.864271640777588\n",
      "step 2925 loss: 2.8362526893615723\n",
      "step 2926 loss: 2.7479450702667236\n",
      "step 2927 loss: 2.7480978965759277\n",
      "step 2928 loss: 2.7546005249023438\n",
      "step 2929 loss: 2.668863296508789\n",
      "step 2930 loss: 2.7882895469665527\n",
      "step 2931 loss: 2.7450103759765625\n",
      "step 2932 loss: 2.8241586685180664\n",
      "step 2933 loss: 2.8309590816497803\n",
      "step 2934 loss: 2.757234811782837\n",
      "step 2935 loss: 2.853609561920166\n",
      "step 2936 loss: 2.978421926498413\n",
      "step 2937 loss: 2.8582894802093506\n",
      "step 2938 loss: 2.767411708831787\n",
      "step 2939 loss: 2.7968997955322266\n",
      "step 2940 loss: 2.6879351139068604\n",
      "step 2941 loss: 2.8141589164733887\n",
      "step 2942 loss: 2.8126206398010254\n",
      "step 2943 loss: 2.720302104949951\n",
      "step 2944 loss: 2.849757432937622\n",
      "step 2945 loss: 2.785884141921997\n",
      "step 2946 loss: 2.7073659896850586\n",
      "step 2947 loss: 2.726633071899414\n",
      "step 2948 loss: 2.965311288833618\n",
      "step 2949 loss: 2.818009376525879\n",
      "step 2950 loss: 2.8497307300567627\n",
      "step 2951 loss: 2.8804643154144287\n",
      "step 2952 loss: 2.9130935668945312\n",
      "step 2953 loss: 2.7040438652038574\n",
      "step 2954 loss: 2.8379716873168945\n",
      "step 2955 loss: 2.756618022918701\n",
      "step 2956 loss: 2.7474682331085205\n",
      "step 2957 loss: 2.8231589794158936\n",
      "step 2958 loss: 2.801182270050049\n",
      "step 2959 loss: 2.827115058898926\n",
      "step 2960 loss: 2.7782139778137207\n",
      "step 2961 loss: 2.7248668670654297\n",
      "step 2962 loss: 2.8424906730651855\n",
      "step 2963 loss: 2.833055019378662\n",
      "step 2964 loss: 2.89037823677063\n",
      "step 2965 loss: 2.8199756145477295\n",
      "step 2966 loss: 2.8836100101470947\n",
      "step 2967 loss: 2.7895448207855225\n",
      "step 2968 loss: 2.7437732219696045\n",
      "step 2969 loss: 2.8061368465423584\n",
      "step 2970 loss: 2.7854807376861572\n",
      "step 2971 loss: 2.839181900024414\n",
      "step 2972 loss: 2.7010257244110107\n",
      "step 2973 loss: 2.8466720581054688\n",
      "step 2974 loss: 2.8649845123291016\n",
      "step 2975 loss: 2.8168091773986816\n",
      "step 2976 loss: 2.7531895637512207\n",
      "step 2977 loss: 2.9009170532226562\n",
      "step 2978 loss: 2.7758700847625732\n",
      "step 2979 loss: 2.755326271057129\n",
      "step 2980 loss: 2.784935474395752\n",
      "step 2981 loss: 2.8036625385284424\n",
      "step 2982 loss: 2.7935080528259277\n",
      "step 2983 loss: 2.790609121322632\n",
      "step 2984 loss: 2.675935983657837\n",
      "step 2985 loss: 2.7584781646728516\n",
      "step 2986 loss: 2.728339433670044\n",
      "step 2987 loss: 2.825568675994873\n",
      "step 2988 loss: 2.751208543777466\n",
      "step 2989 loss: 2.7781729698181152\n",
      "step 2990 loss: 2.7649118900299072\n",
      "step 2991 loss: 2.7041425704956055\n",
      "step 2992 loss: 2.8604159355163574\n",
      "step 2993 loss: 2.6744277477264404\n",
      "step 2994 loss: 2.8323426246643066\n",
      "step 2995 loss: 2.909048318862915\n",
      "step 2996 loss: 2.778841257095337\n",
      "step 2997 loss: 2.7444655895233154\n",
      "step 2998 loss: 2.7199606895446777\n",
      "step 2999 loss: 2.896475315093994\n",
      "step 3000 loss: 2.892245292663574\n",
      "step 3001 loss: 2.8450088500976562\n",
      "step 3002 loss: 2.848241090774536\n",
      "step 3003 loss: 2.8289618492126465\n",
      "step 3004 loss: 2.7803502082824707\n",
      "step 3005 loss: 2.855630874633789\n",
      "step 3006 loss: 2.834878921508789\n",
      "step 3007 loss: 2.7157485485076904\n",
      "step 3008 loss: 2.822467565536499\n",
      "step 3009 loss: 2.7643165588378906\n",
      "step 3010 loss: 2.9010674953460693\n",
      "step 3011 loss: 2.760094404220581\n",
      "step 3012 loss: 2.863530397415161\n",
      "step 3013 loss: 2.78027081489563\n",
      "step 3014 loss: 2.899807929992676\n",
      "step 3015 loss: 2.744248151779175\n",
      "step 3016 loss: 2.795254707336426\n",
      "step 3017 loss: 2.916581153869629\n",
      "step 3018 loss: 2.8357341289520264\n",
      "step 3019 loss: 2.738945245742798\n",
      "step 3020 loss: 2.8396189212799072\n",
      "step 3021 loss: 2.807103395462036\n",
      "step 3022 loss: 2.7269041538238525\n",
      "step 3023 loss: 2.7797868251800537\n",
      "step 3024 loss: 2.8500776290893555\n",
      "step 3025 loss: 2.6766116619110107\n",
      "step 3026 loss: 2.8300740718841553\n",
      "step 3027 loss: 2.8748087882995605\n",
      "step 3028 loss: 2.8285329341888428\n",
      "step 3029 loss: 2.7994205951690674\n",
      "step 3030 loss: 2.9142544269561768\n",
      "step 3031 loss: 2.9044246673583984\n",
      "step 3032 loss: 2.6807870864868164\n",
      "step 3033 loss: 2.749445915222168\n",
      "step 3034 loss: 2.823033571243286\n",
      "step 3035 loss: 2.771242380142212\n",
      "step 3036 loss: 2.864946126937866\n",
      "step 3037 loss: 2.8976407051086426\n",
      "step 3038 loss: 2.8314502239227295\n",
      "step 3039 loss: 2.717979669570923\n",
      "step 3040 loss: 2.804150104522705\n",
      "step 3041 loss: 2.835784435272217\n",
      "step 3042 loss: 2.9627842903137207\n",
      "step 3043 loss: 2.7499687671661377\n",
      "step 3044 loss: 2.7160489559173584\n",
      "step 3045 loss: 2.7671971321105957\n",
      "step 3046 loss: 2.846418857574463\n",
      "step 3047 loss: 2.663810968399048\n",
      "step 3048 loss: 2.7548115253448486\n",
      "step 3049 loss: 2.8865721225738525\n",
      "step 3050 loss: 2.706777572631836\n",
      "step 3051 loss: 2.7513997554779053\n",
      "step 3052 loss: 2.7990713119506836\n",
      "step 3053 loss: 2.890565872192383\n",
      "step 3054 loss: 2.82588791847229\n",
      "step 3055 loss: 2.79654860496521\n",
      "step 3056 loss: 2.7860312461853027\n",
      "step 3057 loss: 2.747638702392578\n",
      "step 3058 loss: 2.830481767654419\n",
      "step 3059 loss: 2.702641248703003\n",
      "step 3060 loss: 2.7372522354125977\n",
      "step 3061 loss: 2.7894015312194824\n",
      "step 3062 loss: 2.8445417881011963\n",
      "step 3063 loss: 2.8615143299102783\n",
      "step 3064 loss: 2.892759084701538\n",
      "step 3065 loss: 2.8233628273010254\n",
      "step 3066 loss: 2.825594186782837\n",
      "step 3067 loss: 2.7998135089874268\n",
      "step 3068 loss: 2.811713218688965\n",
      "step 3069 loss: 2.7956573963165283\n",
      "step 3070 loss: 2.8281259536743164\n",
      "step 3071 loss: 2.770932674407959\n",
      "step 3072 loss: 2.745429277420044\n",
      "step 3073 loss: 2.669508218765259\n",
      "step 3074 loss: 2.6964969635009766\n",
      "step 3075 loss: 2.7421514987945557\n",
      "step 3076 loss: 2.765758752822876\n",
      "step 3077 loss: 2.8864946365356445\n",
      "step 3078 loss: 2.7697644233703613\n",
      "step 3079 loss: 2.8001325130462646\n",
      "step 3080 loss: 2.753652811050415\n",
      "step 3081 loss: 2.7154436111450195\n",
      "step 3082 loss: 2.8519582748413086\n",
      "step 3083 loss: 2.808082342147827\n",
      "step 3084 loss: 2.713892698287964\n",
      "step 3085 loss: 2.763460159301758\n",
      "step 3086 loss: 2.770338296890259\n",
      "step 3087 loss: 2.8325657844543457\n",
      "step 3088 loss: 2.790212631225586\n",
      "step 3089 loss: 2.8018953800201416\n",
      "step 3090 loss: 2.6800053119659424\n",
      "step 3091 loss: 2.871985673904419\n",
      "step 3092 loss: 2.765413761138916\n",
      "step 3093 loss: 2.77091121673584\n",
      "step 3094 loss: 2.7606379985809326\n",
      "step 3095 loss: 2.741363763809204\n",
      "step 3096 loss: 2.734692335128784\n",
      "step 3097 loss: 2.8462705612182617\n",
      "step 3098 loss: 2.8534038066864014\n",
      "step 3099 loss: 2.7138333320617676\n",
      "step 3100 loss: 2.7563703060150146\n",
      "step 3101 loss: 2.6439032554626465\n",
      "step 3102 loss: 2.843132734298706\n",
      "step 3103 loss: 2.7639353275299072\n",
      "step 3104 loss: 2.8054542541503906\n",
      "step 3105 loss: 2.7736196517944336\n",
      "step 3106 loss: 2.6826975345611572\n",
      "step 3107 loss: 2.841294288635254\n",
      "step 3108 loss: 2.8600258827209473\n",
      "step 3109 loss: 2.795304775238037\n",
      "step 3110 loss: 2.6759915351867676\n",
      "step 3111 loss: 2.7781710624694824\n",
      "step 3112 loss: 2.6532485485076904\n",
      "step 3113 loss: 2.875528573989868\n",
      "step 3114 loss: 2.779254198074341\n",
      "step 3115 loss: 2.735051393508911\n",
      "step 3116 loss: 2.762739658355713\n",
      "step 3117 loss: 2.7352325916290283\n",
      "step 3118 loss: 2.7938601970672607\n",
      "step 3119 loss: 2.685953140258789\n",
      "step 3120 loss: 2.7322540283203125\n",
      "step 3121 loss: 2.726649761199951\n",
      "step 3122 loss: 2.806256055831909\n",
      "step 3123 loss: 2.7475337982177734\n",
      "step 3124 loss: 2.792088508605957\n",
      "step 3125 loss: 2.793004035949707\n",
      "step 3126 loss: 2.7479634284973145\n",
      "step 3127 loss: 2.7243661880493164\n",
      "step 3128 loss: 2.683514356613159\n",
      "step 3129 loss: 2.7898707389831543\n",
      "step 3130 loss: 2.842729091644287\n",
      "step 3131 loss: 2.813939094543457\n",
      "step 3132 loss: 2.8428337574005127\n",
      "step 3133 loss: 2.772172212600708\n",
      "step 3134 loss: 2.7564055919647217\n",
      "step 3135 loss: 2.760134696960449\n",
      "step 3136 loss: 2.7235758304595947\n",
      "step 3137 loss: 2.8289904594421387\n",
      "step 3138 loss: 2.6535239219665527\n",
      "step 3139 loss: 2.7603390216827393\n",
      "step 3140 loss: 2.8520116806030273\n",
      "step 3141 loss: 2.752443790435791\n",
      "step 3142 loss: 2.7582902908325195\n",
      "step 3143 loss: 2.774176836013794\n",
      "step 3144 loss: 2.7383382320404053\n",
      "step 3145 loss: 2.787930727005005\n",
      "step 3146 loss: 2.811955213546753\n",
      "step 3147 loss: 2.669619083404541\n",
      "step 3148 loss: 2.6678884029388428\n",
      "step 3149 loss: 2.7255783081054688\n",
      "step 3150 loss: 2.7977752685546875\n",
      "step 3151 loss: 2.8082807064056396\n",
      "step 3152 loss: 2.82344651222229\n",
      "step 3153 loss: 2.7479729652404785\n",
      "step 3154 loss: 2.6891720294952393\n",
      "step 3155 loss: 2.899043560028076\n",
      "step 3156 loss: 2.7607622146606445\n",
      "step 3157 loss: 2.7488887310028076\n",
      "step 3158 loss: 2.770820379257202\n",
      "step 3159 loss: 2.7929916381835938\n",
      "step 3160 loss: 2.75508189201355\n",
      "step 3161 loss: 2.7865169048309326\n",
      "step 3162 loss: 2.8395473957061768\n",
      "step 3163 loss: 2.8047401905059814\n",
      "step 3164 loss: 2.7380309104919434\n",
      "step 3165 loss: 2.782003164291382\n",
      "step 3166 loss: 2.719928503036499\n",
      "step 3167 loss: 2.7052571773529053\n",
      "step 3168 loss: 2.8308496475219727\n",
      "step 3169 loss: 2.761105537414551\n",
      "step 3170 loss: 2.8593602180480957\n",
      "step 3171 loss: 2.6562843322753906\n",
      "step 3172 loss: 2.6011149883270264\n",
      "step 3173 loss: 2.7597131729125977\n",
      "step 3174 loss: 2.7397401332855225\n",
      "step 3175 loss: 2.7423489093780518\n",
      "step 3176 loss: 2.8080132007598877\n",
      "step 3177 loss: 2.8107564449310303\n",
      "step 3178 loss: 2.6949281692504883\n",
      "step 3179 loss: 2.748746156692505\n",
      "step 3180 loss: 2.7399137020111084\n",
      "step 3181 loss: 2.7033722400665283\n",
      "step 3182 loss: 2.7258493900299072\n",
      "step 3183 loss: 2.758559465408325\n",
      "step 3184 loss: 2.6655335426330566\n",
      "step 3185 loss: 2.684053659439087\n",
      "step 3186 loss: 2.686105489730835\n",
      "step 3187 loss: 2.807983636856079\n",
      "step 3188 loss: 2.769615650177002\n",
      "step 3189 loss: 2.714371919631958\n",
      "step 3190 loss: 2.7292001247406006\n",
      "step 3191 loss: 2.7552008628845215\n",
      "step 3192 loss: 2.843261241912842\n",
      "step 3193 loss: 2.687572717666626\n",
      "step 3194 loss: 2.8991029262542725\n",
      "step 3195 loss: 2.770216464996338\n",
      "step 3196 loss: 2.7409300804138184\n",
      "step 3197 loss: 2.7526814937591553\n",
      "step 3198 loss: 2.8194148540496826\n",
      "step 3199 loss: 2.716991901397705\n",
      "step 3200 loss: 2.6004951000213623\n",
      "step 3201 loss: 2.84255051612854\n",
      "step 3202 loss: 2.735966444015503\n",
      "step 3203 loss: 2.7979423999786377\n",
      "step 3204 loss: 2.6474661827087402\n",
      "step 3205 loss: 2.77791690826416\n",
      "step 3206 loss: 2.7736477851867676\n",
      "step 3207 loss: 2.6768877506256104\n",
      "step 3208 loss: 2.7720463275909424\n",
      "step 3209 loss: 2.648771047592163\n",
      "step 3210 loss: 2.792867660522461\n",
      "step 3211 loss: 2.7152364253997803\n",
      "step 3212 loss: 2.668041944503784\n",
      "step 3213 loss: 2.820101499557495\n",
      "step 3214 loss: 2.6060898303985596\n",
      "step 3215 loss: 2.647639274597168\n",
      "step 3216 loss: 2.7243146896362305\n",
      "step 3217 loss: 2.755920648574829\n",
      "step 3218 loss: 2.827064037322998\n",
      "step 3219 loss: 2.7669544219970703\n",
      "step 3220 loss: 2.945701837539673\n",
      "step 3221 loss: 2.7778666019439697\n",
      "step 3222 loss: 2.726200819015503\n",
      "step 3223 loss: 2.7366418838500977\n",
      "step 3224 loss: 2.732083320617676\n",
      "step 3225 loss: 2.837003469467163\n",
      "step 3226 loss: 2.818559169769287\n",
      "step 3227 loss: 2.772850275039673\n",
      "step 3228 loss: 2.6826391220092773\n",
      "step 3229 loss: 2.75087833404541\n",
      "step 3230 loss: 2.700063943862915\n",
      "step 3231 loss: 2.701819658279419\n",
      "step 3232 loss: 2.769620656967163\n",
      "step 3233 loss: 2.8378164768218994\n",
      "step 3234 loss: 2.6594908237457275\n",
      "step 3235 loss: 2.7943925857543945\n",
      "step 3236 loss: 2.7024171352386475\n",
      "step 3237 loss: 2.734257698059082\n",
      "step 3238 loss: 2.6901915073394775\n",
      "step 3239 loss: 2.729294538497925\n",
      "step 3240 loss: 2.6391780376434326\n",
      "step 3241 loss: 2.6517221927642822\n",
      "step 3242 loss: 2.7892963886260986\n",
      "step 3243 loss: 2.666928291320801\n",
      "step 3244 loss: 2.8619649410247803\n",
      "step 3245 loss: 2.7209315299987793\n",
      "step 3246 loss: 2.720409631729126\n",
      "step 3247 loss: 2.687342643737793\n",
      "step 3248 loss: 2.6644387245178223\n",
      "step 3249 loss: 2.7081503868103027\n",
      "step 3250 loss: 2.7566721439361572\n",
      "step 3251 loss: 2.821707248687744\n",
      "step 3252 loss: 2.822376012802124\n",
      "step 3253 loss: 2.7560365200042725\n",
      "step 3254 loss: 2.7030999660491943\n",
      "step 3255 loss: 2.6905434131622314\n",
      "step 3256 loss: 2.7072925567626953\n",
      "step 3257 loss: 2.7191226482391357\n",
      "step 3258 loss: 2.7876827716827393\n",
      "step 3259 loss: 2.789046287536621\n",
      "step 3260 loss: 2.803224802017212\n",
      "step 3261 loss: 2.753547191619873\n",
      "step 3262 loss: 2.8258161544799805\n",
      "step 3263 loss: 2.8160574436187744\n",
      "step 3264 loss: 2.76377010345459\n",
      "step 3265 loss: 2.681931734085083\n",
      "step 3266 loss: 2.7928900718688965\n",
      "step 3267 loss: 2.6669325828552246\n",
      "step 3268 loss: 2.7313594818115234\n",
      "step 3269 loss: 2.81253981590271\n",
      "step 3270 loss: 2.7533669471740723\n",
      "step 3271 loss: 2.724235773086548\n",
      "step 3272 loss: 2.767726182937622\n",
      "step 3273 loss: 2.735894203186035\n",
      "step 3274 loss: 2.8436481952667236\n",
      "step 3275 loss: 2.6546988487243652\n",
      "step 3276 loss: 2.6685993671417236\n",
      "step 3277 loss: 2.761805772781372\n",
      "step 3278 loss: 2.708432912826538\n",
      "step 3279 loss: 2.841032028198242\n",
      "step 3280 loss: 2.6995291709899902\n",
      "step 3281 loss: 2.8175714015960693\n",
      "step 3282 loss: 2.6988766193389893\n",
      "step 3283 loss: 2.6359407901763916\n",
      "step 3284 loss: 2.7594034671783447\n",
      "step 3285 loss: 2.7162058353424072\n",
      "step 3286 loss: 2.916098117828369\n",
      "step 3287 loss: 2.7131481170654297\n",
      "step 3288 loss: 2.763292074203491\n",
      "step 3289 loss: 2.7136383056640625\n",
      "step 3290 loss: 2.7616848945617676\n",
      "step 3291 loss: 2.8299238681793213\n",
      "step 3292 loss: 2.664409637451172\n",
      "step 3293 loss: 2.6204631328582764\n",
      "step 3294 loss: 2.6796457767486572\n",
      "step 3295 loss: 2.6551976203918457\n",
      "step 3296 loss: 2.6263601779937744\n",
      "step 3297 loss: 2.761472225189209\n",
      "step 3298 loss: 2.8165619373321533\n",
      "step 3299 loss: 2.8221755027770996\n",
      "step 3300 loss: 2.627633571624756\n",
      "step 3301 loss: 2.9850716590881348\n",
      "step 3302 loss: 2.757086992263794\n",
      "step 3303 loss: 2.8392629623413086\n",
      "step 3304 loss: 2.621019124984741\n",
      "step 3305 loss: 2.7697908878326416\n",
      "step 3306 loss: 2.905761957168579\n",
      "step 3307 loss: 2.7743279933929443\n",
      "step 3308 loss: 2.754368782043457\n",
      "step 3309 loss: 2.707491636276245\n",
      "step 3310 loss: 2.756085157394409\n",
      "step 3311 loss: 2.720557689666748\n",
      "step 3312 loss: 2.714231252670288\n",
      "step 3313 loss: 2.8320841789245605\n",
      "step 3314 loss: 2.6754634380340576\n",
      "step 3315 loss: 2.736178398132324\n",
      "step 3316 loss: 2.6476972103118896\n",
      "step 3317 loss: 2.7780938148498535\n",
      "step 3318 loss: 2.7578787803649902\n",
      "step 3319 loss: 2.733882427215576\n",
      "step 3320 loss: 2.744157552719116\n",
      "step 3321 loss: 2.7918143272399902\n",
      "step 3322 loss: 2.8600411415100098\n",
      "step 3323 loss: 2.7751903533935547\n",
      "step 3324 loss: 2.8114333152770996\n",
      "step 3325 loss: 2.6959280967712402\n",
      "step 3326 loss: 2.764482259750366\n",
      "step 3327 loss: 2.6698429584503174\n",
      "step 3328 loss: 2.7960281372070312\n",
      "step 3329 loss: 2.7029330730438232\n",
      "step 3330 loss: 2.8057780265808105\n",
      "step 3331 loss: 2.679403066635132\n",
      "step 3332 loss: 2.812739133834839\n",
      "step 3333 loss: 2.722977876663208\n",
      "step 3334 loss: 2.834357261657715\n",
      "step 3335 loss: 2.818255662918091\n",
      "step 3336 loss: 2.7595930099487305\n",
      "step 3337 loss: 2.7687950134277344\n",
      "step 3338 loss: 2.8039660453796387\n",
      "step 3339 loss: 2.6538641452789307\n",
      "step 3340 loss: 2.638472318649292\n",
      "step 3341 loss: 2.627424716949463\n",
      "step 3342 loss: 2.750288724899292\n",
      "step 3343 loss: 2.6890227794647217\n",
      "step 3344 loss: 2.7161412239074707\n",
      "step 3345 loss: 2.813182830810547\n",
      "step 3346 loss: 2.6361701488494873\n",
      "step 3347 loss: 2.7625105381011963\n",
      "step 3348 loss: 2.6972408294677734\n",
      "step 3349 loss: 2.796025514602661\n",
      "step 3350 loss: 2.8435895442962646\n",
      "step 3351 loss: 2.7637555599212646\n",
      "step 3352 loss: 2.702087640762329\n",
      "step 3353 loss: 2.667473316192627\n",
      "step 3354 loss: 2.6975982189178467\n",
      "step 3355 loss: 2.6441164016723633\n",
      "step 3356 loss: 2.6951394081115723\n",
      "step 3357 loss: 2.776312828063965\n",
      "step 3358 loss: 2.7321081161499023\n",
      "step 3359 loss: 2.700101613998413\n",
      "step 3360 loss: 2.8449652194976807\n",
      "step 3361 loss: 2.702242136001587\n",
      "step 3362 loss: 2.6919586658477783\n",
      "step 3363 loss: 2.743427038192749\n",
      "step 3364 loss: 2.7566757202148438\n",
      "step 3365 loss: 2.7520270347595215\n",
      "step 3366 loss: 2.7589221000671387\n",
      "step 3367 loss: 2.8305559158325195\n",
      "step 3368 loss: 2.7104334831237793\n",
      "step 3369 loss: 2.6767046451568604\n",
      "step 3370 loss: 2.7069849967956543\n",
      "step 3371 loss: 2.626181125640869\n",
      "step 3372 loss: 2.692366361618042\n",
      "step 3373 loss: 2.750857353210449\n",
      "step 3374 loss: 2.8451449871063232\n",
      "step 3375 loss: 2.701908588409424\n",
      "step 3376 loss: 2.562784433364868\n",
      "step 3377 loss: 2.728813648223877\n",
      "step 3378 loss: 2.7039008140563965\n",
      "step 3379 loss: 2.8244762420654297\n",
      "step 3380 loss: 2.710419178009033\n",
      "step 3381 loss: 2.7245376110076904\n",
      "step 3382 loss: 2.7141454219818115\n",
      "step 3383 loss: 2.702721357345581\n",
      "step 3384 loss: 2.6833481788635254\n",
      "step 3385 loss: 2.789006233215332\n",
      "step 3386 loss: 2.774651527404785\n",
      "step 3387 loss: 2.684278964996338\n",
      "step 3388 loss: 2.78371262550354\n",
      "step 3389 loss: 2.6789872646331787\n",
      "step 3390 loss: 2.5892748832702637\n",
      "step 3391 loss: 2.777773141860962\n",
      "step 3392 loss: 2.7311973571777344\n",
      "step 3393 loss: 2.8957955837249756\n",
      "step 3394 loss: 2.755216121673584\n",
      "step 3395 loss: 2.7205138206481934\n",
      "step 3396 loss: 2.687164783477783\n",
      "step 3397 loss: 2.670844793319702\n",
      "step 3398 loss: 2.6985268592834473\n",
      "step 3399 loss: 2.6873791217803955\n",
      "step 3400 loss: 2.7147138118743896\n",
      "step 3401 loss: 2.7371866703033447\n",
      "step 3402 loss: 2.7436845302581787\n",
      "step 3403 loss: 2.619436740875244\n",
      "step 3404 loss: 2.7154414653778076\n",
      "step 3405 loss: 2.780735969543457\n",
      "step 3406 loss: 2.707712411880493\n",
      "step 3407 loss: 2.636086940765381\n",
      "step 3408 loss: 2.6157987117767334\n",
      "step 3409 loss: 2.712080240249634\n",
      "step 3410 loss: 2.7519044876098633\n",
      "step 3411 loss: 2.716782331466675\n",
      "step 3412 loss: 2.8138976097106934\n",
      "step 3413 loss: 2.7590842247009277\n",
      "step 3414 loss: 2.600830316543579\n",
      "step 3415 loss: 2.6955153942108154\n",
      "step 3416 loss: 2.8232874870300293\n",
      "step 3417 loss: 2.797584056854248\n",
      "step 3418 loss: 2.7074053287506104\n",
      "step 3419 loss: 2.7984707355499268\n",
      "step 3420 loss: 2.7816498279571533\n",
      "step 3421 loss: 2.7283713817596436\n",
      "step 3422 loss: 2.6854629516601562\n",
      "step 3423 loss: 2.7439095973968506\n",
      "step 3424 loss: 2.6962289810180664\n",
      "step 3425 loss: 2.7087042331695557\n",
      "step 3426 loss: 2.699056386947632\n",
      "step 3427 loss: 2.8231704235076904\n",
      "step 3428 loss: 2.807605266571045\n",
      "step 3429 loss: 2.661149263381958\n",
      "step 3430 loss: 2.7002835273742676\n",
      "step 3431 loss: 2.700711965560913\n",
      "step 3432 loss: 2.7372829914093018\n",
      "step 3433 loss: 2.7783923149108887\n",
      "step 3434 loss: 2.6819355487823486\n",
      "step 3435 loss: 2.7216217517852783\n",
      "step 3436 loss: 2.627924680709839\n",
      "step 3437 loss: 2.672989845275879\n",
      "step 3438 loss: 2.7173666954040527\n",
      "step 3439 loss: 2.694690465927124\n",
      "step 3440 loss: 2.759241819381714\n",
      "step 3441 loss: 2.7563953399658203\n",
      "step 3442 loss: 2.6763463020324707\n",
      "step 3443 loss: 2.8079280853271484\n",
      "step 3444 loss: 2.705622434616089\n",
      "step 3445 loss: 2.6816117763519287\n",
      "step 3446 loss: 2.7538843154907227\n",
      "step 3447 loss: 2.669311285018921\n",
      "step 3448 loss: 2.713834524154663\n",
      "step 3449 loss: 2.6750667095184326\n",
      "step 3450 loss: 2.6638898849487305\n",
      "step 3451 loss: 2.7457916736602783\n",
      "step 3452 loss: 2.7391257286071777\n",
      "step 3453 loss: 2.7257888317108154\n",
      "step 3454 loss: 2.6667368412017822\n",
      "step 3455 loss: 2.6269147396087646\n",
      "step 3456 loss: 2.7499804496765137\n",
      "step 3457 loss: 2.661813497543335\n",
      "step 3458 loss: 2.659316301345825\n",
      "step 3459 loss: 2.641822576522827\n",
      "step 3460 loss: 2.64517879486084\n",
      "step 3461 loss: 2.6714439392089844\n",
      "step 3462 loss: 2.7866411209106445\n",
      "step 3463 loss: 2.690931558609009\n",
      "step 3464 loss: 2.6760613918304443\n",
      "step 3465 loss: 2.664691925048828\n",
      "step 3466 loss: 2.644390344619751\n",
      "step 3467 loss: 2.7476718425750732\n",
      "step 3468 loss: 2.7552690505981445\n",
      "step 3469 loss: 2.7259232997894287\n",
      "step 3470 loss: 2.657054901123047\n",
      "step 3471 loss: 2.7356762886047363\n",
      "step 3472 loss: 2.627788782119751\n",
      "step 3473 loss: 2.590970277786255\n",
      "step 3474 loss: 2.7050437927246094\n",
      "step 3475 loss: 2.675124406814575\n",
      "step 3476 loss: 2.6514012813568115\n",
      "step 3477 loss: 2.6329257488250732\n",
      "step 3478 loss: 2.616900682449341\n",
      "step 3479 loss: 2.8018221855163574\n",
      "step 3480 loss: 2.6305387020111084\n",
      "step 3481 loss: 2.7545976638793945\n",
      "step 3482 loss: 2.720170736312866\n",
      "step 3483 loss: 2.7796239852905273\n",
      "step 3484 loss: 2.6820080280303955\n",
      "step 3485 loss: 2.7406225204467773\n",
      "step 3486 loss: 2.7705531120300293\n",
      "step 3487 loss: 2.6873955726623535\n",
      "step 3488 loss: 2.6788711547851562\n",
      "step 3489 loss: 2.8086938858032227\n",
      "step 3490 loss: 2.739642858505249\n",
      "step 3491 loss: 2.7177915573120117\n",
      "step 3492 loss: 2.776141881942749\n",
      "step 3493 loss: 2.5549607276916504\n",
      "step 3494 loss: 2.6638824939727783\n",
      "step 3495 loss: 2.8094489574432373\n",
      "step 3496 loss: 2.715772867202759\n",
      "step 3497 loss: 2.719059467315674\n",
      "step 3498 loss: 2.9041998386383057\n",
      "step 3499 loss: 2.6635420322418213\n",
      "step 3500 loss: 2.718296766281128\n",
      "step 3501 loss: 2.6755893230438232\n",
      "step 3502 loss: 2.7145304679870605\n",
      "step 3503 loss: 2.6694483757019043\n",
      "step 3504 loss: 2.7652957439422607\n",
      "step 3505 loss: 2.753519058227539\n",
      "step 3506 loss: 2.711836099624634\n",
      "step 3507 loss: 2.6548409461975098\n",
      "step 3508 loss: 2.657921075820923\n",
      "step 3509 loss: 2.7285547256469727\n",
      "step 3510 loss: 2.6546778678894043\n",
      "step 3511 loss: 2.7270591259002686\n",
      "step 3512 loss: 2.6102240085601807\n",
      "step 3513 loss: 2.7640063762664795\n",
      "step 3514 loss: 2.6384332180023193\n",
      "step 3515 loss: 2.6884493827819824\n",
      "step 3516 loss: 2.6775543689727783\n",
      "step 3517 loss: 2.537370443344116\n",
      "step 3518 loss: 2.6646251678466797\n",
      "step 3519 loss: 2.8159666061401367\n",
      "step 3520 loss: 2.7067885398864746\n",
      "step 3521 loss: 2.678663492202759\n",
      "step 3522 loss: 2.7653520107269287\n",
      "step 3523 loss: 2.631394386291504\n",
      "step 3524 loss: 2.6619749069213867\n",
      "step 3525 loss: 2.6867525577545166\n",
      "step 3526 loss: 2.6126999855041504\n",
      "step 3527 loss: 2.7091660499572754\n",
      "step 3528 loss: 2.8208329677581787\n",
      "step 3529 loss: 2.6592140197753906\n",
      "step 3530 loss: 2.7311456203460693\n",
      "step 3531 loss: 2.6718482971191406\n",
      "step 3532 loss: 2.76361346244812\n",
      "step 3533 loss: 2.6077768802642822\n",
      "step 3534 loss: 2.6948437690734863\n",
      "step 3535 loss: 2.6716418266296387\n",
      "step 3536 loss: 2.70523738861084\n",
      "step 3537 loss: 2.590587615966797\n",
      "step 3538 loss: 2.736879587173462\n",
      "step 3539 loss: 2.738909959793091\n",
      "step 3540 loss: 2.6300554275512695\n",
      "step 3541 loss: 2.6658315658569336\n",
      "step 3542 loss: 2.812081813812256\n",
      "step 3543 loss: 2.7354609966278076\n",
      "step 3544 loss: 2.826244831085205\n",
      "step 3545 loss: 2.602294921875\n",
      "step 3546 loss: 2.6592752933502197\n",
      "step 3547 loss: 2.660656690597534\n",
      "step 3548 loss: 2.6290459632873535\n",
      "step 3549 loss: 2.690436840057373\n",
      "step 3550 loss: 2.6397311687469482\n",
      "step 3551 loss: 2.6364102363586426\n",
      "step 3552 loss: 2.815669059753418\n",
      "step 3553 loss: 2.7222390174865723\n",
      "step 3554 loss: 2.7689990997314453\n",
      "step 3555 loss: 2.5966336727142334\n",
      "step 3556 loss: 2.6526074409484863\n",
      "step 3557 loss: 2.7324352264404297\n",
      "step 3558 loss: 2.7392327785491943\n",
      "step 3559 loss: 2.7342429161071777\n",
      "step 3560 loss: 2.7290098667144775\n",
      "step 3561 loss: 2.6562769412994385\n",
      "step 3562 loss: 2.6737377643585205\n",
      "step 3563 loss: 2.742018461227417\n",
      "step 3564 loss: 2.6997573375701904\n",
      "step 3565 loss: 2.664752960205078\n",
      "step 3566 loss: 2.7186291217803955\n",
      "step 3567 loss: 2.6582553386688232\n",
      "step 3568 loss: 2.6389777660369873\n",
      "step 3569 loss: 2.7601513862609863\n",
      "step 3570 loss: 2.759765863418579\n",
      "step 3571 loss: 2.817091464996338\n",
      "step 3572 loss: 2.6720330715179443\n",
      "step 3573 loss: 2.704437494277954\n",
      "step 3574 loss: 2.6302664279937744\n",
      "step 3575 loss: 2.739520788192749\n",
      "step 3576 loss: 2.6875336170196533\n",
      "step 3577 loss: 2.704890489578247\n",
      "step 3578 loss: 2.6687586307525635\n",
      "step 3579 loss: 2.7166388034820557\n",
      "step 3580 loss: 2.8092780113220215\n",
      "step 3581 loss: 2.808363676071167\n",
      "step 3582 loss: 2.660853385925293\n",
      "step 3583 loss: 2.709859609603882\n",
      "step 3584 loss: 2.6188559532165527\n",
      "step 3585 loss: 2.6393332481384277\n",
      "step 3586 loss: 2.6522140502929688\n",
      "step 3587 loss: 2.7524943351745605\n",
      "step 3588 loss: 2.710596799850464\n",
      "step 3589 loss: 2.6954433917999268\n",
      "step 3590 loss: 2.7350921630859375\n",
      "step 3591 loss: 2.643120050430298\n",
      "step 3592 loss: 2.7564001083374023\n",
      "step 3593 loss: 2.800488233566284\n",
      "step 3594 loss: 2.552269697189331\n",
      "step 3595 loss: 2.693190336227417\n",
      "step 3596 loss: 2.5656850337982178\n",
      "step 3597 loss: 2.6465542316436768\n",
      "step 3598 loss: 2.6872806549072266\n",
      "step 3599 loss: 2.700927734375\n",
      "step 3600 loss: 2.714982748031616\n",
      "step 3601 loss: 2.7653276920318604\n",
      "step 3602 loss: 2.6328444480895996\n",
      "step 3603 loss: 2.8533613681793213\n",
      "step 3604 loss: 2.7253410816192627\n",
      "step 3605 loss: 2.7190535068511963\n",
      "step 3606 loss: 2.5776920318603516\n",
      "step 3607 loss: 2.642591714859009\n",
      "step 3608 loss: 2.6942877769470215\n",
      "step 3609 loss: 2.5860724449157715\n",
      "step 3610 loss: 2.580866813659668\n",
      "step 3611 loss: 2.6036577224731445\n",
      "step 3612 loss: 2.784219264984131\n",
      "step 3613 loss: 2.5841329097747803\n",
      "step 3614 loss: 2.7664294242858887\n",
      "step 3615 loss: 2.49817156791687\n",
      "step 3616 loss: 2.594301223754883\n",
      "step 3617 loss: 2.7823522090911865\n",
      "step 3618 loss: 2.6179726123809814\n",
      "step 3619 loss: 2.658745050430298\n",
      "step 3620 loss: 2.5815000534057617\n",
      "step 3621 loss: 2.710278272628784\n",
      "step 3622 loss: 2.739828586578369\n",
      "step 3623 loss: 2.6760525703430176\n",
      "step 3624 loss: 2.6375162601470947\n",
      "step 3625 loss: 2.723792314529419\n",
      "step 3626 loss: 2.8156230449676514\n",
      "step 3627 loss: 2.687035322189331\n",
      "step 3628 loss: 2.599060297012329\n",
      "step 3629 loss: 2.609726667404175\n",
      "step 3630 loss: 2.6823668479919434\n",
      "step 3631 loss: 2.629373073577881\n",
      "step 3632 loss: 2.525395154953003\n",
      "step 3633 loss: 2.595095634460449\n",
      "step 3634 loss: 2.599429130554199\n",
      "step 3635 loss: 2.764796495437622\n",
      "step 3636 loss: 2.752936840057373\n",
      "step 3637 loss: 2.695141553878784\n",
      "step 3638 loss: 2.6282382011413574\n",
      "step 3639 loss: 2.768678903579712\n",
      "step 3640 loss: 2.5685346126556396\n",
      "step 3641 loss: 2.7473337650299072\n",
      "step 3642 loss: 2.6787681579589844\n",
      "step 3643 loss: 2.746033191680908\n",
      "step 3644 loss: 2.597433090209961\n",
      "step 3645 loss: 2.5824337005615234\n",
      "step 3646 loss: 2.5986781120300293\n",
      "step 3647 loss: 2.650301933288574\n",
      "step 3648 loss: 2.7535107135772705\n",
      "step 3649 loss: 2.71244215965271\n",
      "step 3650 loss: 2.7617549896240234\n",
      "step 3651 loss: 2.8063740730285645\n",
      "step 3652 loss: 2.5391716957092285\n",
      "step 3653 loss: 2.730592966079712\n",
      "step 3654 loss: 2.7377867698669434\n",
      "step 3655 loss: 2.72328782081604\n",
      "step 3656 loss: 2.7620437145233154\n",
      "step 3657 loss: 2.783214569091797\n",
      "step 3658 loss: 2.748548984527588\n",
      "step 3659 loss: 2.573136806488037\n",
      "step 3660 loss: 2.694221258163452\n",
      "step 3661 loss: 2.743088483810425\n",
      "step 3662 loss: 2.695207357406616\n",
      "step 3663 loss: 2.6679022312164307\n",
      "step 3664 loss: 2.7594430446624756\n",
      "step 3665 loss: 2.690225601196289\n",
      "step 3666 loss: 2.6109635829925537\n",
      "step 3667 loss: 2.62040376663208\n",
      "step 3668 loss: 2.636427640914917\n",
      "step 3669 loss: 2.5937581062316895\n",
      "step 3670 loss: 2.6559407711029053\n",
      "step 3671 loss: 2.5992093086242676\n",
      "step 3672 loss: 2.70430850982666\n",
      "step 3673 loss: 2.6740715503692627\n",
      "step 3674 loss: 2.6495301723480225\n",
      "step 3675 loss: 2.7166640758514404\n",
      "step 3676 loss: 2.763763904571533\n",
      "step 3677 loss: 2.6998860836029053\n",
      "step 3678 loss: 2.620065450668335\n",
      "step 3679 loss: 2.762552499771118\n",
      "step 3680 loss: 2.6490888595581055\n",
      "step 3681 loss: 2.7830047607421875\n",
      "step 3682 loss: 2.754448890686035\n",
      "step 3683 loss: 2.6796391010284424\n",
      "step 3684 loss: 2.66133189201355\n",
      "step 3685 loss: 2.68878436088562\n",
      "step 3686 loss: 2.6751937866210938\n",
      "step 3687 loss: 2.58730411529541\n",
      "step 3688 loss: 2.6011226177215576\n",
      "step 3689 loss: 2.602888822555542\n",
      "step 3690 loss: 2.640475034713745\n",
      "step 3691 loss: 2.6731114387512207\n",
      "step 3692 loss: 2.662853956222534\n",
      "step 3693 loss: 2.7543587684631348\n",
      "step 3694 loss: 2.6453990936279297\n",
      "step 3695 loss: 2.647176504135132\n",
      "step 3696 loss: 2.762078046798706\n",
      "step 3697 loss: 2.6853222846984863\n",
      "step 3698 loss: 2.6224353313446045\n",
      "step 3699 loss: 2.679544448852539\n",
      "step 3700 loss: 2.606290817260742\n",
      "step 3701 loss: 2.547441244125366\n",
      "step 3702 loss: 2.6810450553894043\n",
      "step 3703 loss: 2.571902275085449\n",
      "step 3704 loss: 2.6686947345733643\n",
      "step 3705 loss: 2.734121084213257\n",
      "step 3706 loss: 2.680213451385498\n",
      "step 3707 loss: 2.6576058864593506\n",
      "step 3708 loss: 2.698932409286499\n",
      "step 3709 loss: 2.7031567096710205\n",
      "step 3710 loss: 2.706166982650757\n",
      "step 3711 loss: 2.696904420852661\n",
      "step 3712 loss: 2.768477201461792\n",
      "step 3713 loss: 2.5508155822753906\n",
      "step 3714 loss: 2.6654701232910156\n",
      "step 3715 loss: 2.653658390045166\n",
      "step 3716 loss: 2.6384732723236084\n",
      "step 3717 loss: 2.6333768367767334\n",
      "step 3718 loss: 2.7046902179718018\n",
      "step 3719 loss: 2.64989972114563\n",
      "step 3720 loss: 2.543543815612793\n",
      "step 3721 loss: 2.7431957721710205\n",
      "step 3722 loss: 2.7066540718078613\n",
      "step 3723 loss: 2.760362386703491\n",
      "step 3724 loss: 2.6717944145202637\n",
      "step 3725 loss: 2.70589542388916\n",
      "step 3726 loss: 2.583125352859497\n",
      "step 3727 loss: 2.624906063079834\n",
      "step 3728 loss: 2.5591461658477783\n",
      "step 3729 loss: 2.680901050567627\n",
      "step 3730 loss: 2.6255431175231934\n",
      "step 3731 loss: 2.6327760219573975\n",
      "step 3732 loss: 2.6694939136505127\n",
      "step 3733 loss: 2.621122360229492\n",
      "step 3734 loss: 2.7346088886260986\n",
      "step 3735 loss: 2.647822380065918\n",
      "step 3736 loss: 2.485494613647461\n",
      "step 3737 loss: 2.5838630199432373\n",
      "step 3738 loss: 2.7422268390655518\n",
      "step 3739 loss: 2.706054925918579\n",
      "step 3740 loss: 2.611839771270752\n",
      "step 3741 loss: 2.7312819957733154\n",
      "step 3742 loss: 2.708261251449585\n",
      "step 3743 loss: 2.696565628051758\n",
      "step 3744 loss: 2.6112380027770996\n",
      "step 3745 loss: 2.6161065101623535\n",
      "step 3746 loss: 2.720991373062134\n",
      "step 3747 loss: 2.708552598953247\n",
      "step 3748 loss: 2.6663641929626465\n",
      "step 3749 loss: 2.5770089626312256\n",
      "step 3750 loss: 2.5509612560272217\n",
      "step 3751 loss: 2.5298399925231934\n",
      "step 3752 loss: 2.637543201446533\n",
      "step 3753 loss: 2.7888245582580566\n",
      "step 3754 loss: 2.540865421295166\n",
      "step 3755 loss: 2.6429781913757324\n",
      "step 3756 loss: 2.6328318119049072\n",
      "step 3757 loss: 2.5756523609161377\n",
      "step 3758 loss: 2.751227617263794\n",
      "step 3759 loss: 2.6365158557891846\n",
      "step 3760 loss: 2.746676445007324\n",
      "step 3761 loss: 2.6314611434936523\n",
      "step 3762 loss: 2.592519998550415\n",
      "step 3763 loss: 2.7297353744506836\n",
      "step 3764 loss: 2.6674041748046875\n",
      "step 3765 loss: 2.680091142654419\n",
      "step 3766 loss: 2.762164831161499\n",
      "step 3767 loss: 2.58931565284729\n",
      "step 3768 loss: 2.7143423557281494\n",
      "step 3769 loss: 2.789975881576538\n",
      "step 3770 loss: 2.581144094467163\n",
      "step 3771 loss: 2.568427324295044\n",
      "step 3772 loss: 2.7594704627990723\n",
      "step 3773 loss: 2.5258426666259766\n",
      "step 3774 loss: 2.56813907623291\n",
      "step 3775 loss: 2.665992021560669\n",
      "step 3776 loss: 2.6838037967681885\n",
      "step 3777 loss: 2.7200586795806885\n",
      "step 3778 loss: 2.655632257461548\n",
      "step 3779 loss: 2.704763174057007\n",
      "step 3780 loss: 2.6309516429901123\n",
      "step 3781 loss: 2.569117784500122\n",
      "step 3782 loss: 2.634805917739868\n",
      "step 3783 loss: 2.7109534740448\n",
      "step 3784 loss: 2.5946569442749023\n",
      "step 3785 loss: 2.559910535812378\n",
      "step 3786 loss: 2.645488977432251\n",
      "step 3787 loss: 2.6974942684173584\n",
      "step 3788 loss: 2.6745526790618896\n",
      "step 3789 loss: 2.663440465927124\n",
      "step 3790 loss: 2.703860282897949\n",
      "step 3791 loss: 2.5580766201019287\n",
      "step 3792 loss: 2.553699254989624\n",
      "step 3793 loss: 2.5894336700439453\n",
      "step 3794 loss: 2.6408920288085938\n",
      "step 3795 loss: 2.5789718627929688\n",
      "step 3796 loss: 2.6579556465148926\n",
      "step 3797 loss: 2.667478084564209\n",
      "step 3798 loss: 2.7247538566589355\n",
      "step 3799 loss: 2.711367607116699\n",
      "step 3800 loss: 2.7237846851348877\n",
      "step 3801 loss: 2.5316414833068848\n",
      "step 3802 loss: 2.6108903884887695\n",
      "step 3803 loss: 2.5982775688171387\n",
      "step 3804 loss: 2.547382354736328\n",
      "step 3805 loss: 2.636841297149658\n",
      "step 3806 loss: 2.8207333087921143\n",
      "step 3807 loss: 2.777574300765991\n",
      "step 3808 loss: 2.7081072330474854\n",
      "step 3809 loss: 2.715985059738159\n",
      "step 3810 loss: 2.7459187507629395\n",
      "step 3811 loss: 2.6379714012145996\n",
      "step 3812 loss: 2.4865458011627197\n",
      "step 3813 loss: 2.70772123336792\n",
      "step 3814 loss: 2.742342233657837\n",
      "step 3815 loss: 2.7351865768432617\n",
      "step 3816 loss: 2.565932035446167\n",
      "step 3817 loss: 2.5037999153137207\n",
      "step 3818 loss: 2.485969066619873\n",
      "step 3819 loss: 2.7589035034179688\n",
      "step 3820 loss: 2.669513702392578\n",
      "step 3821 loss: 2.628532886505127\n",
      "step 3822 loss: 2.6499135494232178\n",
      "step 3823 loss: 2.6770670413970947\n",
      "step 3824 loss: 2.676769971847534\n",
      "step 3825 loss: 2.5906474590301514\n",
      "step 3826 loss: 2.7334189414978027\n",
      "step 3827 loss: 2.6724798679351807\n",
      "step 3828 loss: 2.658642292022705\n",
      "step 3829 loss: 2.656611680984497\n",
      "step 3830 loss: 2.738954782485962\n",
      "step 3831 loss: 2.681363105773926\n",
      "step 3832 loss: 2.6240670680999756\n",
      "step 3833 loss: 2.6501166820526123\n",
      "step 3834 loss: 2.6193339824676514\n",
      "step 3835 loss: 2.66103196144104\n",
      "step 3836 loss: 2.585064172744751\n",
      "step 3837 loss: 2.5649678707122803\n",
      "step 3838 loss: 2.5605664253234863\n",
      "step 3839 loss: 2.654949188232422\n",
      "step 3840 loss: 2.632253646850586\n",
      "step 3841 loss: 2.58261775970459\n",
      "step 3842 loss: 2.657127857208252\n",
      "step 3843 loss: 2.6538655757904053\n",
      "step 3844 loss: 2.745515823364258\n",
      "step 3845 loss: 2.6055426597595215\n",
      "step 3846 loss: 2.6018972396850586\n",
      "step 3847 loss: 2.587238311767578\n",
      "step 3848 loss: 2.6767690181732178\n",
      "step 3849 loss: 2.7900400161743164\n",
      "step 3850 loss: 2.6139378547668457\n",
      "step 3851 loss: 2.7254798412323\n",
      "step 3852 loss: 2.653885841369629\n",
      "step 3853 loss: 2.7101504802703857\n",
      "step 3854 loss: 2.494503974914551\n",
      "step 3855 loss: 2.677090883255005\n",
      "step 3856 loss: 2.638998031616211\n",
      "step 3857 loss: 2.6371805667877197\n",
      "step 3858 loss: 2.636042594909668\n",
      "step 3859 loss: 2.7033820152282715\n",
      "step 3860 loss: 2.6575663089752197\n",
      "step 3861 loss: 2.6727190017700195\n",
      "step 3862 loss: 2.7424428462982178\n",
      "step 3863 loss: 2.747587203979492\n",
      "step 3864 loss: 2.679450273513794\n",
      "step 3865 loss: 2.582693099975586\n",
      "step 3866 loss: 2.6008174419403076\n",
      "step 3867 loss: 2.564302682876587\n",
      "step 3868 loss: 2.610926389694214\n",
      "step 3869 loss: 2.700794219970703\n",
      "step 3870 loss: 2.6419780254364014\n",
      "step 3871 loss: 2.581707000732422\n",
      "step 3872 loss: 2.6535115242004395\n",
      "step 3873 loss: 2.6749560832977295\n",
      "step 3874 loss: 2.5733301639556885\n",
      "step 3875 loss: 2.68666410446167\n",
      "step 3876 loss: 2.6374289989471436\n",
      "step 3877 loss: 2.7457542419433594\n",
      "step 3878 loss: 2.7026121616363525\n",
      "step 3879 loss: 2.6357309818267822\n",
      "step 3880 loss: 2.7453787326812744\n",
      "step 3881 loss: 2.619117021560669\n",
      "step 3882 loss: 2.6156632900238037\n",
      "step 3883 loss: 2.656796455383301\n",
      "step 3884 loss: 2.620274305343628\n",
      "step 3885 loss: 2.6618502140045166\n",
      "step 3886 loss: 2.7319693565368652\n",
      "step 3887 loss: 2.6050357818603516\n",
      "step 3888 loss: 2.6610240936279297\n",
      "step 3889 loss: 2.7530157566070557\n",
      "step 3890 loss: 2.6286299228668213\n",
      "step 3891 loss: 2.7280020713806152\n",
      "step 3892 loss: 2.643955945968628\n",
      "step 3893 loss: 2.6587119102478027\n",
      "step 3894 loss: 2.675554037094116\n",
      "step 3895 loss: 2.6759917736053467\n",
      "step 3896 loss: 2.6632578372955322\n",
      "step 3897 loss: 2.601621627807617\n",
      "step 3898 loss: 2.693021774291992\n",
      "step 3899 loss: 2.6921141147613525\n",
      "step 3900 loss: 2.60630464553833\n",
      "step 3901 loss: 2.571875810623169\n",
      "step 3902 loss: 2.544991970062256\n",
      "step 3903 loss: 2.7540535926818848\n",
      "step 3904 loss: 2.6523075103759766\n",
      "step 3905 loss: 2.7128283977508545\n",
      "step 3906 loss: 2.665311813354492\n",
      "step 3907 loss: 2.6538028717041016\n",
      "step 3908 loss: 2.6522951126098633\n",
      "step 3909 loss: 2.655557870864868\n",
      "step 3910 loss: 2.6576297283172607\n",
      "step 3911 loss: 2.7102673053741455\n",
      "step 3912 loss: 2.709096670150757\n",
      "step 3913 loss: 2.6635286808013916\n",
      "step 3914 loss: 2.6543242931365967\n",
      "step 3915 loss: 2.715492010116577\n",
      "step 3916 loss: 2.6229026317596436\n",
      "step 3917 loss: 2.709594488143921\n",
      "step 3918 loss: 2.6960597038269043\n",
      "step 3919 loss: 2.68184232711792\n",
      "step 3920 loss: 2.7268576622009277\n",
      "step 3921 loss: 2.5815868377685547\n",
      "step 3922 loss: 2.7954649925231934\n",
      "step 3923 loss: 2.745576858520508\n",
      "step 3924 loss: 2.630626916885376\n",
      "step 3925 loss: 2.6476399898529053\n",
      "step 3926 loss: 2.631965398788452\n",
      "step 3927 loss: 2.571317195892334\n",
      "step 3928 loss: 2.6815125942230225\n",
      "step 3929 loss: 2.692741870880127\n",
      "step 3930 loss: 2.612210512161255\n",
      "step 3931 loss: 2.5680620670318604\n",
      "step 3932 loss: 2.6661152839660645\n",
      "step 3933 loss: 2.6542673110961914\n",
      "step 3934 loss: 2.6651906967163086\n",
      "step 3935 loss: 2.575070381164551\n",
      "step 3936 loss: 2.7242934703826904\n",
      "step 3937 loss: 2.605242967605591\n",
      "step 3938 loss: 2.593487501144409\n",
      "step 3939 loss: 2.6517629623413086\n",
      "step 3940 loss: 2.670287847518921\n",
      "step 3941 loss: 2.6197257041931152\n",
      "step 3942 loss: 2.7114970684051514\n",
      "step 3943 loss: 2.719996213912964\n",
      "step 3944 loss: 2.566331148147583\n",
      "step 3945 loss: 2.664994478225708\n",
      "step 3946 loss: 2.700359582901001\n",
      "step 3947 loss: 2.54420804977417\n",
      "step 3948 loss: 2.6153290271759033\n",
      "step 3949 loss: 2.70548415184021\n",
      "step 3950 loss: 2.7089850902557373\n",
      "step 3951 loss: 2.576737642288208\n",
      "step 3952 loss: 2.6305744647979736\n",
      "step 3953 loss: 2.6508617401123047\n",
      "step 3954 loss: 2.718363046646118\n",
      "step 3955 loss: 2.5804872512817383\n",
      "step 3956 loss: 2.588641881942749\n",
      "step 3957 loss: 2.5945885181427\n",
      "step 3958 loss: 2.6845548152923584\n",
      "step 3959 loss: 2.690030336380005\n",
      "step 3960 loss: 2.6619298458099365\n",
      "step 3961 loss: 2.5667262077331543\n",
      "step 3962 loss: 2.5669710636138916\n",
      "step 3963 loss: 2.6541290283203125\n",
      "step 3964 loss: 2.6164181232452393\n",
      "step 3965 loss: 2.5856194496154785\n",
      "step 3966 loss: 2.5814220905303955\n",
      "step 3967 loss: 2.6387746334075928\n",
      "step 3968 loss: 2.637938976287842\n",
      "step 3969 loss: 2.6936697959899902\n",
      "step 3970 loss: 2.596134662628174\n",
      "step 3971 loss: 2.6869072914123535\n",
      "step 3972 loss: 2.5832977294921875\n",
      "step 3973 loss: 2.7153372764587402\n",
      "step 3974 loss: 2.590935468673706\n",
      "step 3975 loss: 2.7381789684295654\n",
      "step 3976 loss: 2.7535133361816406\n",
      "step 3977 loss: 2.636061429977417\n",
      "step 3978 loss: 2.55704927444458\n",
      "step 3979 loss: 2.699624538421631\n",
      "step 3980 loss: 2.74163556098938\n",
      "step 3981 loss: 2.564775228500366\n",
      "step 3982 loss: 2.643850088119507\n",
      "step 3983 loss: 2.6081786155700684\n",
      "step 3984 loss: 2.5285394191741943\n",
      "step 3985 loss: 2.667022705078125\n",
      "step 3986 loss: 2.641435384750366\n",
      "step 3987 loss: 2.6678454875946045\n",
      "step 3988 loss: 2.631422996520996\n",
      "step 3989 loss: 2.6088430881500244\n",
      "step 3990 loss: 2.665390729904175\n",
      "step 3991 loss: 2.6428043842315674\n",
      "step 3992 loss: 2.5579190254211426\n",
      "step 3993 loss: 2.5949108600616455\n",
      "step 3994 loss: 2.6659255027770996\n",
      "step 3995 loss: 2.5821712017059326\n",
      "step 3996 loss: 2.6615657806396484\n",
      "step 3997 loss: 2.632517099380493\n",
      "step 3998 loss: 2.7412567138671875\n",
      "step 3999 loss: 2.554206609725952\n",
      "step 4000 loss: 2.703908681869507\n",
      "step 4001 loss: 2.6401755809783936\n",
      "step 4002 loss: 2.6134145259857178\n",
      "step 4003 loss: 2.5729641914367676\n",
      "step 4004 loss: 2.5810086727142334\n",
      "step 4005 loss: 2.648890733718872\n",
      "step 4006 loss: 2.7527918815612793\n",
      "step 4007 loss: 2.547234535217285\n",
      "step 4008 loss: 2.631660223007202\n",
      "step 4009 loss: 2.576416015625\n",
      "step 4010 loss: 2.658902406692505\n",
      "step 4011 loss: 2.6558010578155518\n",
      "step 4012 loss: 2.6004886627197266\n",
      "step 4013 loss: 2.6270928382873535\n",
      "step 4014 loss: 2.577524185180664\n",
      "step 4015 loss: 2.7688159942626953\n",
      "step 4016 loss: 2.6059086322784424\n",
      "step 4017 loss: 2.698427438735962\n",
      "step 4018 loss: 2.632871627807617\n",
      "step 4019 loss: 2.556175470352173\n",
      "step 4020 loss: 2.638946056365967\n",
      "step 4021 loss: 2.5539638996124268\n",
      "step 4022 loss: 2.587284803390503\n",
      "step 4023 loss: 2.597792148590088\n",
      "step 4024 loss: 2.6078991889953613\n",
      "step 4025 loss: 2.5631725788116455\n",
      "step 4026 loss: 2.5905187129974365\n",
      "step 4027 loss: 2.5801708698272705\n",
      "step 4028 loss: 2.7837655544281006\n",
      "step 4029 loss: 2.6591954231262207\n",
      "step 4030 loss: 2.7555580139160156\n",
      "step 4031 loss: 2.7004001140594482\n",
      "step 4032 loss: 2.5485610961914062\n",
      "step 4033 loss: 2.695340871810913\n",
      "step 4034 loss: 2.6183385848999023\n",
      "step 4035 loss: 2.522270679473877\n",
      "step 4036 loss: 2.695301055908203\n",
      "step 4037 loss: 2.5084259510040283\n",
      "step 4038 loss: 2.6538281440734863\n",
      "step 4039 loss: 2.6341724395751953\n",
      "step 4040 loss: 2.548797845840454\n",
      "step 4041 loss: 2.6983792781829834\n",
      "step 4042 loss: 2.5390427112579346\n",
      "step 4043 loss: 2.7120048999786377\n",
      "step 4044 loss: 2.7465121746063232\n",
      "step 4045 loss: 2.555105209350586\n",
      "step 4046 loss: 2.640451192855835\n",
      "step 4047 loss: 2.572575807571411\n",
      "step 4048 loss: 2.7253482341766357\n",
      "step 4049 loss: 2.632509708404541\n",
      "step 4050 loss: 2.681185245513916\n",
      "step 4051 loss: 2.6268413066864014\n",
      "step 4052 loss: 2.719849109649658\n",
      "step 4053 loss: 2.6346659660339355\n",
      "step 4054 loss: 2.658562660217285\n",
      "step 4055 loss: 2.4971134662628174\n",
      "step 4056 loss: 2.613506555557251\n",
      "step 4057 loss: 2.739903211593628\n",
      "step 4058 loss: 2.5554909706115723\n",
      "step 4059 loss: 2.523505210876465\n",
      "step 4060 loss: 2.6018543243408203\n",
      "step 4061 loss: 2.711099863052368\n",
      "step 4062 loss: 2.5212695598602295\n",
      "step 4063 loss: 2.699475049972534\n",
      "step 4064 loss: 2.4952480792999268\n",
      "step 4065 loss: 2.605320453643799\n",
      "step 4066 loss: 2.6663424968719482\n",
      "step 4067 loss: 2.629063129425049\n",
      "step 4068 loss: 2.7326066493988037\n",
      "step 4069 loss: 2.6156952381134033\n",
      "step 4070 loss: 2.6370365619659424\n",
      "step 4071 loss: 2.5713937282562256\n",
      "step 4072 loss: 2.6234912872314453\n",
      "step 4073 loss: 2.5975098609924316\n",
      "step 4074 loss: 2.704258441925049\n",
      "step 4075 loss: 2.7485649585723877\n",
      "step 4076 loss: 2.620847702026367\n",
      "step 4077 loss: 2.649991273880005\n",
      "step 4078 loss: 2.6545519828796387\n",
      "step 4079 loss: 2.6495978832244873\n",
      "step 4080 loss: 2.5776195526123047\n",
      "step 4081 loss: 2.693298578262329\n",
      "step 4082 loss: 2.6038565635681152\n",
      "step 4083 loss: 2.5797932147979736\n",
      "step 4084 loss: 2.667250871658325\n",
      "step 4085 loss: 2.6735787391662598\n",
      "step 4086 loss: 2.6457371711730957\n",
      "step 4087 loss: 2.604917049407959\n",
      "step 4088 loss: 2.624840259552002\n",
      "step 4089 loss: 2.593259572982788\n",
      "step 4090 loss: 2.6980559825897217\n",
      "step 4091 loss: 2.690739631652832\n",
      "step 4092 loss: 2.5909008979797363\n",
      "step 4093 loss: 2.7290008068084717\n",
      "step 4094 loss: 2.5123729705810547\n",
      "step 4095 loss: 2.6355228424072266\n",
      "step 4096 loss: 2.646376609802246\n",
      "step 4097 loss: 2.575469970703125\n",
      "step 4098 loss: 2.7505576610565186\n",
      "step 4099 loss: 2.609548568725586\n",
      "step 4100 loss: 2.7407634258270264\n",
      "step 4101 loss: 2.482607126235962\n",
      "step 4102 loss: 2.679124355316162\n",
      "step 4103 loss: 2.571767807006836\n",
      "step 4104 loss: 2.648468255996704\n",
      "step 4105 loss: 2.6755223274230957\n",
      "step 4106 loss: 2.6785717010498047\n",
      "step 4107 loss: 2.6390836238861084\n",
      "step 4108 loss: 2.5205392837524414\n",
      "step 4109 loss: 2.6305809020996094\n",
      "step 4110 loss: 2.6484243869781494\n",
      "step 4111 loss: 2.585073709487915\n",
      "step 4112 loss: 2.730067491531372\n",
      "step 4113 loss: 2.4396612644195557\n",
      "step 4114 loss: 2.6247048377990723\n",
      "step 4115 loss: 2.6351583003997803\n",
      "step 4116 loss: 2.640164375305176\n",
      "step 4117 loss: 2.7442283630371094\n",
      "step 4118 loss: 2.6919493675231934\n",
      "step 4119 loss: 2.604682445526123\n",
      "step 4120 loss: 2.729698896408081\n",
      "step 4121 loss: 2.624629020690918\n",
      "step 4122 loss: 2.7037885189056396\n",
      "step 4123 loss: 2.637246608734131\n",
      "step 4124 loss: 2.641087532043457\n",
      "step 4125 loss: 2.596661329269409\n",
      "step 4126 loss: 2.5821099281311035\n",
      "step 4127 loss: 2.6091017723083496\n",
      "step 4128 loss: 2.663914442062378\n",
      "step 4129 loss: 2.5934433937072754\n",
      "step 4130 loss: 2.6147208213806152\n",
      "step 4131 loss: 2.5043599605560303\n",
      "step 4132 loss: 2.6535463333129883\n",
      "step 4133 loss: 2.549778699874878\n",
      "step 4134 loss: 2.72808575630188\n",
      "step 4135 loss: 2.7523677349090576\n",
      "step 4136 loss: 2.5628628730773926\n",
      "step 4137 loss: 2.6650619506835938\n",
      "step 4138 loss: 2.767050266265869\n",
      "step 4139 loss: 2.601792573928833\n",
      "step 4140 loss: 2.6700665950775146\n",
      "step 4141 loss: 2.601710796356201\n",
      "step 4142 loss: 2.5514800548553467\n",
      "step 4143 loss: 2.6924495697021484\n",
      "step 4144 loss: 2.561291217803955\n",
      "step 4145 loss: 2.663633346557617\n",
      "step 4146 loss: 2.67081618309021\n",
      "step 4147 loss: 2.500410318374634\n",
      "step 4148 loss: 2.6594505310058594\n",
      "step 4149 loss: 2.6052956581115723\n",
      "step 4150 loss: 2.606994867324829\n",
      "step 4151 loss: 2.663726568222046\n",
      "step 4152 loss: 2.588125228881836\n",
      "step 4153 loss: 2.5676887035369873\n",
      "step 4154 loss: 2.627362012863159\n",
      "step 4155 loss: 2.855562686920166\n",
      "step 4156 loss: 2.4944231510162354\n",
      "step 4157 loss: 2.665682315826416\n",
      "step 4158 loss: 2.601990222930908\n",
      "step 4159 loss: 2.641772508621216\n",
      "step 4160 loss: 2.608126401901245\n",
      "step 4161 loss: 2.624215602874756\n",
      "step 4162 loss: 2.6273176670074463\n",
      "step 4163 loss: 2.5194292068481445\n",
      "step 4164 loss: 2.6876838207244873\n",
      "step 4165 loss: 2.751619815826416\n",
      "step 4166 loss: 2.5088260173797607\n",
      "step 4167 loss: 2.652981758117676\n",
      "step 4168 loss: 2.6664533615112305\n",
      "step 4169 loss: 2.6330034732818604\n",
      "step 4170 loss: 2.5893263816833496\n",
      "step 4171 loss: 2.6506614685058594\n",
      "step 4172 loss: 2.631073474884033\n",
      "step 4173 loss: 2.731666088104248\n",
      "step 4174 loss: 2.729337692260742\n",
      "step 4175 loss: 2.6666722297668457\n",
      "step 4176 loss: 2.696152687072754\n",
      "step 4177 loss: 2.7290453910827637\n",
      "step 4178 loss: 2.5953469276428223\n",
      "step 4179 loss: 2.673593521118164\n",
      "step 4180 loss: 2.5753941535949707\n",
      "step 4181 loss: 2.5400593280792236\n",
      "step 4182 loss: 2.5560903549194336\n",
      "step 4183 loss: 2.5816445350646973\n",
      "step 4184 loss: 2.5553059577941895\n",
      "step 4185 loss: 2.57763934135437\n",
      "step 4186 loss: 2.7621827125549316\n",
      "step 4187 loss: 2.5610787868499756\n",
      "step 4188 loss: 2.5442652702331543\n",
      "step 4189 loss: 2.7541611194610596\n",
      "step 4190 loss: 2.5778379440307617\n",
      "step 4191 loss: 2.6936802864074707\n",
      "step 4192 loss: 2.482240915298462\n",
      "step 4193 loss: 2.6386117935180664\n",
      "step 4194 loss: 2.6854496002197266\n",
      "step 4195 loss: 2.6652231216430664\n",
      "step 4196 loss: 2.5661585330963135\n",
      "step 4197 loss: 2.597862720489502\n",
      "step 4198 loss: 2.669222831726074\n",
      "step 4199 loss: 2.669198989868164\n",
      "step 4200 loss: 2.6153855323791504\n",
      "step 4201 loss: 2.648979902267456\n",
      "step 4202 loss: 2.6549437046051025\n",
      "step 4203 loss: 2.651456117630005\n",
      "step 4204 loss: 2.617540121078491\n",
      "step 4205 loss: 2.625588893890381\n",
      "step 4206 loss: 2.783984899520874\n",
      "step 4207 loss: 2.609924554824829\n",
      "step 4208 loss: 2.6491587162017822\n",
      "step 4209 loss: 2.6193456649780273\n",
      "step 4210 loss: 2.6904752254486084\n",
      "step 4211 loss: 2.6332554817199707\n",
      "step 4212 loss: 2.587153434753418\n",
      "step 4213 loss: 2.6360034942626953\n",
      "step 4214 loss: 2.6214122772216797\n",
      "step 4215 loss: 2.602072238922119\n",
      "step 4216 loss: 2.622431993484497\n",
      "step 4217 loss: 2.6244235038757324\n",
      "step 4218 loss: 2.597543716430664\n",
      "step 4219 loss: 2.6485166549682617\n",
      "step 4220 loss: 2.590235948562622\n",
      "step 4221 loss: 2.723836660385132\n",
      "step 4222 loss: 2.6180763244628906\n",
      "step 4223 loss: 2.555999994277954\n",
      "step 4224 loss: 2.6126868724823\n",
      "step 4225 loss: 2.6109392642974854\n",
      "step 4226 loss: 2.620213508605957\n",
      "step 4227 loss: 2.6350433826446533\n",
      "step 4228 loss: 2.6162049770355225\n",
      "step 4229 loss: 2.627241373062134\n",
      "step 4230 loss: 2.5647990703582764\n",
      "step 4231 loss: 2.571497678756714\n",
      "step 4232 loss: 2.5732622146606445\n",
      "step 4233 loss: 2.7721002101898193\n",
      "step 4234 loss: 2.6600162982940674\n",
      "step 4235 loss: 2.5204718112945557\n",
      "step 4236 loss: 2.7129664421081543\n",
      "step 4237 loss: 2.7791740894317627\n",
      "step 4238 loss: 2.719651937484741\n",
      "step 4239 loss: 2.551072359085083\n",
      "step 4240 loss: 2.755777359008789\n",
      "step 4241 loss: 2.726283073425293\n",
      "step 4242 loss: 2.658985137939453\n",
      "step 4243 loss: 2.480847120285034\n",
      "step 4244 loss: 2.5674660205841064\n",
      "step 4245 loss: 2.6388213634490967\n",
      "step 4246 loss: 2.6153810024261475\n",
      "step 4247 loss: 2.5366368293762207\n",
      "step 4248 loss: 2.55767822265625\n",
      "step 4249 loss: 2.7254765033721924\n",
      "step 4250 loss: 2.607043504714966\n",
      "step 4251 loss: 2.647275924682617\n",
      "step 4252 loss: 2.575655937194824\n",
      "step 4253 loss: 2.5879361629486084\n",
      "step 4254 loss: 2.574124574661255\n",
      "step 4255 loss: 2.57553768157959\n",
      "step 4256 loss: 2.5138332843780518\n",
      "step 4257 loss: 2.5197460651397705\n",
      "step 4258 loss: 2.7218739986419678\n",
      "step 4259 loss: 2.5857648849487305\n",
      "step 4260 loss: 2.593319892883301\n",
      "step 4261 loss: 2.5007500648498535\n",
      "step 4262 loss: 2.5233168601989746\n",
      "step 4263 loss: 2.587601661682129\n",
      "step 4264 loss: 2.557835340499878\n",
      "step 4265 loss: 2.7028074264526367\n",
      "step 4266 loss: 2.579153299331665\n",
      "step 4267 loss: 2.613809823989868\n",
      "step 4268 loss: 2.707134962081909\n",
      "step 4269 loss: 2.7885921001434326\n",
      "step 4270 loss: 2.5207626819610596\n",
      "step 4271 loss: 2.621929168701172\n",
      "step 4272 loss: 2.613661766052246\n",
      "step 4273 loss: 2.5924510955810547\n",
      "step 4274 loss: 2.5903797149658203\n",
      "step 4275 loss: 2.761258602142334\n",
      "step 4276 loss: 2.72943377494812\n",
      "step 4277 loss: 2.5448827743530273\n",
      "step 4278 loss: 2.6507956981658936\n",
      "step 4279 loss: 2.5375330448150635\n",
      "step 4280 loss: 2.4920413494110107\n",
      "step 4281 loss: 2.610960006713867\n",
      "step 4282 loss: 2.6321377754211426\n",
      "step 4283 loss: 2.6455419063568115\n",
      "step 4284 loss: 2.7538440227508545\n",
      "step 4285 loss: 2.7579588890075684\n",
      "step 4286 loss: 2.6290061473846436\n",
      "step 4287 loss: 2.512234687805176\n",
      "step 4288 loss: 2.74123215675354\n",
      "step 4289 loss: 2.6276943683624268\n",
      "step 4290 loss: 2.5519394874572754\n",
      "step 4291 loss: 2.5309410095214844\n",
      "step 4292 loss: 2.6952743530273438\n",
      "step 4293 loss: 2.662489652633667\n",
      "step 4294 loss: 2.6925928592681885\n",
      "step 4295 loss: 2.6380670070648193\n",
      "step 4296 loss: 2.6523263454437256\n",
      "step 4297 loss: 2.584970712661743\n",
      "step 4298 loss: 2.624420166015625\n",
      "step 4299 loss: 2.491083860397339\n",
      "step 4300 loss: 2.6925723552703857\n",
      "step 4301 loss: 2.697910785675049\n",
      "step 4302 loss: 2.4929122924804688\n",
      "step 4303 loss: 2.5164356231689453\n",
      "step 4304 loss: 2.584381103515625\n",
      "step 4305 loss: 2.55590558052063\n",
      "step 4306 loss: 2.6730597019195557\n",
      "step 4307 loss: 2.6393260955810547\n",
      "step 4308 loss: 2.5661869049072266\n",
      "step 4309 loss: 2.577106475830078\n",
      "step 4310 loss: 2.6831533908843994\n",
      "step 4311 loss: 2.7557735443115234\n",
      "step 4312 loss: 2.525697946548462\n",
      "step 4313 loss: 2.6654605865478516\n",
      "step 4314 loss: 2.6613142490386963\n",
      "step 4315 loss: 2.5459303855895996\n",
      "step 4316 loss: 2.7281370162963867\n",
      "step 4317 loss: 2.5839850902557373\n",
      "step 4318 loss: 2.6177101135253906\n",
      "step 4319 loss: 2.5642378330230713\n",
      "step 4320 loss: 2.5609452724456787\n",
      "step 4321 loss: 2.742152214050293\n",
      "step 4322 loss: 2.622633218765259\n",
      "step 4323 loss: 2.5646846294403076\n",
      "step 4324 loss: 2.5877721309661865\n",
      "step 4325 loss: 2.734589099884033\n",
      "step 4326 loss: 2.46659517288208\n",
      "step 4327 loss: 2.5047271251678467\n",
      "step 4328 loss: 2.5907328128814697\n",
      "step 4329 loss: 2.605414628982544\n",
      "step 4330 loss: 2.5681099891662598\n",
      "step 4331 loss: 2.570720672607422\n",
      "step 4332 loss: 2.5569100379943848\n",
      "step 4333 loss: 2.523684024810791\n",
      "step 4334 loss: 2.660789728164673\n",
      "step 4335 loss: 2.5098183155059814\n",
      "step 4336 loss: 2.6199355125427246\n",
      "step 4337 loss: 2.6422271728515625\n",
      "step 4338 loss: 2.625075578689575\n",
      "step 4339 loss: 2.547144651412964\n",
      "step 4340 loss: 2.5735933780670166\n",
      "step 4341 loss: 2.56361722946167\n",
      "step 4342 loss: 2.6574440002441406\n",
      "step 4343 loss: 2.644223213195801\n",
      "step 4344 loss: 2.6380667686462402\n",
      "step 4345 loss: 2.5998551845550537\n",
      "step 4346 loss: 2.700335741043091\n",
      "step 4347 loss: 2.5623104572296143\n",
      "step 4348 loss: 2.7186787128448486\n",
      "step 4349 loss: 2.5936903953552246\n",
      "step 4350 loss: 2.4813148975372314\n",
      "step 4351 loss: 2.6186180114746094\n",
      "step 4352 loss: 2.5437283515930176\n",
      "step 4353 loss: 2.593566656112671\n",
      "step 4354 loss: 2.8286855220794678\n",
      "step 4355 loss: 2.564408540725708\n",
      "step 4356 loss: 2.6918270587921143\n",
      "step 4357 loss: 2.6256113052368164\n",
      "step 4358 loss: 2.6181437969207764\n",
      "step 4359 loss: 2.605595827102661\n",
      "step 4360 loss: 2.5634732246398926\n",
      "step 4361 loss: 2.500692367553711\n",
      "step 4362 loss: 2.399198532104492\n",
      "step 4363 loss: 2.5280191898345947\n",
      "step 4364 loss: 2.5726895332336426\n",
      "step 4365 loss: 2.5447070598602295\n",
      "step 4366 loss: 2.5836119651794434\n",
      "step 4367 loss: 2.577652931213379\n",
      "step 4368 loss: 2.6536595821380615\n",
      "step 4369 loss: 2.5569190979003906\n",
      "step 4370 loss: 2.691885232925415\n",
      "step 4371 loss: 2.6089658737182617\n",
      "step 4372 loss: 2.601696252822876\n",
      "step 4373 loss: 2.5868759155273438\n",
      "step 4374 loss: 2.6540515422821045\n",
      "step 4375 loss: 2.6268701553344727\n",
      "step 4376 loss: 2.4797329902648926\n",
      "step 4377 loss: 2.5644118785858154\n",
      "step 4378 loss: 2.5034096240997314\n",
      "step 4379 loss: 2.5124850273132324\n",
      "step 4380 loss: 2.6014814376831055\n",
      "step 4381 loss: 2.580075979232788\n",
      "step 4382 loss: 2.552190065383911\n",
      "step 4383 loss: 2.6661131381988525\n",
      "step 4384 loss: 2.5999412536621094\n",
      "step 4385 loss: 2.63122296333313\n",
      "step 4386 loss: 2.59124755859375\n",
      "step 4387 loss: 2.506448745727539\n",
      "step 4388 loss: 2.498844623565674\n",
      "step 4389 loss: 2.651829957962036\n",
      "step 4390 loss: 2.599158763885498\n",
      "step 4391 loss: 2.6123239994049072\n",
      "step 4392 loss: 2.4339098930358887\n",
      "step 4393 loss: 2.4537503719329834\n",
      "step 4394 loss: 2.583815336227417\n",
      "step 4395 loss: 2.5473179817199707\n",
      "step 4396 loss: 2.5125036239624023\n",
      "step 4397 loss: 2.5775327682495117\n",
      "step 4398 loss: 2.5936410427093506\n",
      "step 4399 loss: 2.7197489738464355\n",
      "step 4400 loss: 2.5623557567596436\n",
      "step 4401 loss: 2.6353843212127686\n",
      "step 4402 loss: 2.493417739868164\n",
      "step 4403 loss: 2.6432416439056396\n",
      "step 4404 loss: 2.6136474609375\n",
      "step 4405 loss: 2.5652146339416504\n",
      "step 4406 loss: 2.6807422637939453\n",
      "step 4407 loss: 2.4777863025665283\n",
      "step 4408 loss: 2.6626265048980713\n",
      "step 4409 loss: 2.5390071868896484\n",
      "step 4410 loss: 2.5980544090270996\n",
      "step 4411 loss: 2.615079879760742\n",
      "step 4412 loss: 2.6775200366973877\n",
      "step 4413 loss: 2.5312414169311523\n",
      "step 4414 loss: 2.643284797668457\n",
      "step 4415 loss: 2.6160013675689697\n",
      "step 4416 loss: 2.6709201335906982\n",
      "step 4417 loss: 2.6458094120025635\n",
      "step 4418 loss: 2.6400468349456787\n",
      "step 4419 loss: 2.6805272102355957\n",
      "step 4420 loss: 2.4438416957855225\n",
      "step 4421 loss: 2.50478458404541\n",
      "step 4422 loss: 2.6972827911376953\n",
      "step 4423 loss: 2.5813961029052734\n",
      "step 4424 loss: 2.5657782554626465\n",
      "step 4425 loss: 2.5567398071289062\n",
      "step 4426 loss: 2.4385666847229004\n",
      "step 4427 loss: 2.5365614891052246\n",
      "step 4428 loss: 2.596933126449585\n",
      "step 4429 loss: 2.5567314624786377\n",
      "step 4430 loss: 2.5622472763061523\n",
      "step 4431 loss: 2.742748975753784\n",
      "step 4432 loss: 2.4716649055480957\n",
      "step 4433 loss: 2.599339246749878\n",
      "step 4434 loss: 2.7791574001312256\n",
      "step 4435 loss: 2.621647596359253\n",
      "step 4436 loss: 2.52982234954834\n",
      "step 4437 loss: 2.654477834701538\n",
      "step 4438 loss: 2.5743417739868164\n",
      "step 4439 loss: 2.561277389526367\n",
      "step 4440 loss: 2.691655158996582\n",
      "step 4441 loss: 2.602895975112915\n",
      "step 4442 loss: 2.5170795917510986\n",
      "step 4443 loss: 2.531856060028076\n",
      "step 4444 loss: 2.6135597229003906\n",
      "step 4445 loss: 2.63773775100708\n",
      "step 4446 loss: 2.610100030899048\n",
      "step 4447 loss: 2.5235204696655273\n",
      "step 4448 loss: 2.6086297035217285\n",
      "step 4449 loss: 2.588942289352417\n",
      "step 4450 loss: 2.641969680786133\n",
      "step 4451 loss: 2.627270460128784\n",
      "step 4452 loss: 2.504868268966675\n",
      "step 4453 loss: 2.6229093074798584\n",
      "step 4454 loss: 2.6909472942352295\n",
      "step 4455 loss: 2.6756432056427\n",
      "step 4456 loss: 2.543790102005005\n",
      "step 4457 loss: 2.6787281036376953\n",
      "step 4458 loss: 2.5669476985931396\n",
      "step 4459 loss: 2.589056968688965\n",
      "step 4460 loss: 2.6458122730255127\n",
      "step 4461 loss: 2.645263671875\n",
      "step 4462 loss: 2.6461050510406494\n",
      "step 4463 loss: 2.6460418701171875\n",
      "step 4464 loss: 2.5333664417266846\n",
      "step 4465 loss: 2.6731977462768555\n",
      "step 4466 loss: 2.557340621948242\n",
      "step 4467 loss: 2.620439291000366\n",
      "step 4468 loss: 2.557265520095825\n",
      "step 4469 loss: 2.6743342876434326\n",
      "step 4470 loss: 2.4542489051818848\n",
      "step 4471 loss: 2.5410401821136475\n",
      "step 4472 loss: 2.5694122314453125\n",
      "step 4473 loss: 2.569114923477173\n",
      "step 4474 loss: 2.5250611305236816\n",
      "step 4475 loss: 2.551945447921753\n",
      "step 4476 loss: 2.588473320007324\n",
      "step 4477 loss: 2.6529436111450195\n",
      "step 4478 loss: 2.4932000637054443\n",
      "step 4479 loss: 2.64469838142395\n",
      "step 4480 loss: 2.5399749279022217\n",
      "step 4481 loss: 2.600527763366699\n",
      "step 4482 loss: 2.5237293243408203\n",
      "step 4483 loss: 2.5696046352386475\n",
      "step 4484 loss: 2.654611110687256\n",
      "step 4485 loss: 2.652639389038086\n",
      "step 4486 loss: 2.642177104949951\n",
      "step 4487 loss: 2.541194200515747\n",
      "step 4488 loss: 2.715482234954834\n",
      "step 4489 loss: 2.670058488845825\n",
      "step 4490 loss: 2.644909620285034\n",
      "step 4491 loss: 2.5450499057769775\n",
      "step 4492 loss: 2.586411714553833\n",
      "step 4493 loss: 2.660161018371582\n",
      "step 4494 loss: 2.5760245323181152\n",
      "step 4495 loss: 2.6245741844177246\n",
      "step 4496 loss: 2.704092025756836\n",
      "step 4497 loss: 2.621932029724121\n",
      "step 4498 loss: 2.5478694438934326\n",
      "step 4499 loss: 2.620234727859497\n",
      "step 4500 loss: 2.6690523624420166\n",
      "step 4501 loss: 2.4664382934570312\n",
      "step 4502 loss: 2.687746047973633\n",
      "step 4503 loss: 2.675523042678833\n",
      "step 4504 loss: 2.630706548690796\n",
      "step 4505 loss: 2.5725879669189453\n",
      "step 4506 loss: 2.667074203491211\n",
      "step 4507 loss: 2.6283020973205566\n",
      "step 4508 loss: 2.5677757263183594\n",
      "step 4509 loss: 2.577880382537842\n",
      "step 4510 loss: 2.596389055252075\n",
      "step 4511 loss: 2.734269142150879\n",
      "step 4512 loss: 2.659203052520752\n",
      "step 4513 loss: 2.649890899658203\n",
      "step 4514 loss: 2.556379795074463\n",
      "step 4515 loss: 2.537501335144043\n",
      "step 4516 loss: 2.6743741035461426\n",
      "step 4517 loss: 2.5641489028930664\n",
      "step 4518 loss: 2.664517879486084\n",
      "step 4519 loss: 2.5943820476531982\n",
      "step 4520 loss: 2.435208559036255\n",
      "step 4521 loss: 2.4278504848480225\n",
      "step 4522 loss: 2.538421154022217\n",
      "step 4523 loss: 2.591059446334839\n",
      "step 4524 loss: 2.5057172775268555\n",
      "step 4525 loss: 2.599414587020874\n",
      "step 4526 loss: 2.5721521377563477\n",
      "step 4527 loss: 2.6280879974365234\n",
      "step 4528 loss: 2.5243706703186035\n",
      "step 4529 loss: 2.6600418090820312\n",
      "step 4530 loss: 2.6559066772460938\n",
      "step 4531 loss: 2.5240228176116943\n",
      "step 4532 loss: 2.5180704593658447\n",
      "step 4533 loss: 2.6137847900390625\n",
      "step 4534 loss: 2.5790348052978516\n",
      "step 4535 loss: 2.645054340362549\n",
      "step 4536 loss: 2.4359235763549805\n",
      "step 4537 loss: 2.7258570194244385\n",
      "step 4538 loss: 2.624973773956299\n",
      "step 4539 loss: 2.5839881896972656\n",
      "step 4540 loss: 2.5479180812835693\n",
      "step 4541 loss: 2.581254005432129\n",
      "step 4542 loss: 2.500535726547241\n",
      "step 4543 loss: 2.5421042442321777\n",
      "step 4544 loss: 2.5554096698760986\n",
      "step 4545 loss: 2.704591751098633\n",
      "step 4546 loss: 2.6252589225769043\n",
      "step 4547 loss: 2.664146661758423\n",
      "step 4548 loss: 2.70137882232666\n",
      "step 4549 loss: 2.724808692932129\n",
      "step 4550 loss: 2.5508623123168945\n",
      "step 4551 loss: 2.5806782245635986\n",
      "step 4552 loss: 2.649413585662842\n",
      "step 4553 loss: 2.5861644744873047\n",
      "step 4554 loss: 2.5748162269592285\n",
      "step 4555 loss: 2.6207973957061768\n",
      "step 4556 loss: 2.6679019927978516\n",
      "step 4557 loss: 2.5454421043395996\n",
      "step 4558 loss: 2.531263828277588\n",
      "step 4559 loss: 2.494027853012085\n",
      "step 4560 loss: 2.5615689754486084\n",
      "step 4561 loss: 2.626038074493408\n",
      "step 4562 loss: 2.5457661151885986\n",
      "step 4563 loss: 2.5912885665893555\n",
      "step 4564 loss: 2.644376754760742\n",
      "step 4565 loss: 2.5689878463745117\n",
      "step 4566 loss: 2.605367660522461\n",
      "step 4567 loss: 2.6276097297668457\n",
      "step 4568 loss: 2.623095989227295\n",
      "step 4569 loss: 2.5739169120788574\n",
      "step 4570 loss: 2.6495413780212402\n",
      "step 4571 loss: 2.5847277641296387\n",
      "step 4572 loss: 2.7064661979675293\n",
      "step 4573 loss: 2.583927869796753\n",
      "step 4574 loss: 2.6990108489990234\n",
      "step 4575 loss: 2.605341672897339\n",
      "step 4576 loss: 2.763951539993286\n",
      "step 4577 loss: 2.5938520431518555\n",
      "step 4578 loss: 2.7098052501678467\n",
      "step 4579 loss: 2.661701202392578\n",
      "step 4580 loss: 2.6621854305267334\n",
      "step 4581 loss: 2.5019145011901855\n",
      "step 4582 loss: 2.519139289855957\n",
      "step 4583 loss: 2.54685378074646\n",
      "step 4584 loss: 2.495830774307251\n",
      "step 4585 loss: 2.548923969268799\n",
      "step 4586 loss: 2.4990017414093018\n",
      "step 4587 loss: 2.595926523208618\n",
      "step 4588 loss: 2.5095672607421875\n",
      "step 4589 loss: 2.5002849102020264\n",
      "step 4590 loss: 2.656789541244507\n",
      "step 4591 loss: 2.5370559692382812\n",
      "step 4592 loss: 2.643084764480591\n",
      "step 4593 loss: 2.6434292793273926\n",
      "step 4594 loss: 2.5022196769714355\n",
      "step 4595 loss: 2.6728649139404297\n",
      "step 4596 loss: 2.616790771484375\n",
      "step 4597 loss: 2.58365535736084\n",
      "step 4598 loss: 2.5197198390960693\n",
      "step 4599 loss: 2.6471493244171143\n",
      "step 4600 loss: 2.595306396484375\n",
      "step 4601 loss: 2.5891661643981934\n",
      "step 4602 loss: 2.5603387355804443\n",
      "step 4603 loss: 2.6347901821136475\n",
      "step 4604 loss: 2.571960687637329\n",
      "step 4605 loss: 2.4659793376922607\n",
      "step 4606 loss: 2.5065770149230957\n",
      "step 4607 loss: 2.548290729522705\n",
      "step 4608 loss: 2.518946647644043\n",
      "step 4609 loss: 2.5446200370788574\n",
      "step 4610 loss: 2.5212645530700684\n",
      "step 4611 loss: 2.530146598815918\n",
      "step 4612 loss: 2.6094305515289307\n",
      "step 4613 loss: 2.7175960540771484\n",
      "step 4614 loss: 2.618656635284424\n",
      "step 4615 loss: 2.6047520637512207\n",
      "step 4616 loss: 2.487029552459717\n",
      "step 4617 loss: 2.5881905555725098\n",
      "step 4618 loss: 2.556931257247925\n",
      "step 4619 loss: 2.5896637439727783\n",
      "step 4620 loss: 2.6420934200286865\n",
      "step 4621 loss: 2.5017244815826416\n",
      "step 4622 loss: 2.590583086013794\n",
      "step 4623 loss: 2.6095540523529053\n",
      "step 4624 loss: 2.608564615249634\n",
      "step 4625 loss: 2.577953577041626\n",
      "step 4626 loss: 2.564429759979248\n",
      "step 4627 loss: 2.518322467803955\n",
      "step 4628 loss: 2.5179738998413086\n",
      "step 4629 loss: 2.6189656257629395\n",
      "step 4630 loss: 2.5708835124969482\n",
      "step 4631 loss: 2.6906940937042236\n",
      "step 4632 loss: 2.5981743335723877\n",
      "step 4633 loss: 2.454523801803589\n",
      "step 4634 loss: 2.5045599937438965\n",
      "step 4635 loss: 2.569040060043335\n",
      "step 4636 loss: 2.6263577938079834\n",
      "step 4637 loss: 2.6282248497009277\n",
      "step 4638 loss: 2.6289498805999756\n",
      "step 4639 loss: 2.5171656608581543\n",
      "step 4640 loss: 2.5999293327331543\n",
      "step 4641 loss: 2.47891902923584\n",
      "step 4642 loss: 2.597008228302002\n",
      "step 4643 loss: 2.6164305210113525\n",
      "step 4644 loss: 2.523998260498047\n",
      "step 4645 loss: 2.550234317779541\n",
      "step 4646 loss: 2.615841865539551\n",
      "step 4647 loss: 2.6044952869415283\n",
      "step 4648 loss: 2.5902531147003174\n",
      "step 4649 loss: 2.5322020053863525\n",
      "step 4650 loss: 2.5144870281219482\n",
      "step 4651 loss: 2.496790885925293\n",
      "step 4652 loss: 2.5209829807281494\n",
      "step 4653 loss: 2.617324113845825\n",
      "step 4654 loss: 2.5789332389831543\n",
      "step 4655 loss: 2.5095531940460205\n",
      "step 4656 loss: 2.5396556854248047\n",
      "step 4657 loss: 2.64198637008667\n",
      "step 4658 loss: 2.6151957511901855\n",
      "step 4659 loss: 2.5639424324035645\n",
      "step 4660 loss: 2.541959762573242\n",
      "step 4661 loss: 2.59956431388855\n",
      "step 4662 loss: 2.5284175872802734\n",
      "step 4663 loss: 2.626776933670044\n",
      "step 4664 loss: 2.6256749629974365\n",
      "step 4665 loss: 2.486776351928711\n",
      "step 4666 loss: 2.599658727645874\n",
      "step 4667 loss: 2.6367897987365723\n",
      "step 4668 loss: 2.4159440994262695\n",
      "step 4669 loss: 2.474958658218384\n",
      "step 4670 loss: 2.6187965869903564\n",
      "step 4671 loss: 2.618818998336792\n",
      "step 4672 loss: 2.689185619354248\n",
      "step 4673 loss: 2.516479253768921\n",
      "step 4674 loss: 2.5959861278533936\n",
      "step 4675 loss: 2.709752082824707\n",
      "step 4676 loss: 2.574904680252075\n",
      "step 4677 loss: 2.6589276790618896\n",
      "step 4678 loss: 2.6172704696655273\n",
      "step 4679 loss: 2.564908027648926\n",
      "step 4680 loss: 2.5520195960998535\n",
      "step 4681 loss: 2.709508180618286\n",
      "step 4682 loss: 2.519676923751831\n",
      "step 4683 loss: 2.576340436935425\n",
      "step 4684 loss: 2.658270835876465\n",
      "step 4685 loss: 2.442530632019043\n",
      "step 4686 loss: 2.658735990524292\n",
      "step 4687 loss: 2.573178768157959\n",
      "step 4688 loss: 2.5693416595458984\n",
      "step 4689 loss: 2.5509655475616455\n",
      "step 4690 loss: 2.517407178878784\n",
      "step 4691 loss: 2.54860258102417\n",
      "step 4692 loss: 2.7145273685455322\n",
      "step 4693 loss: 2.6288154125213623\n",
      "step 4694 loss: 2.598102569580078\n",
      "step 4695 loss: 2.5239226818084717\n",
      "step 4696 loss: 2.451972484588623\n",
      "step 4697 loss: 2.6173651218414307\n",
      "step 4698 loss: 2.635847568511963\n",
      "step 4699 loss: 2.5484986305236816\n",
      "step 4700 loss: 2.5762505531311035\n",
      "step 4701 loss: 2.476919412612915\n",
      "step 4702 loss: 2.5605061054229736\n",
      "step 4703 loss: 2.5306496620178223\n",
      "step 4704 loss: 2.4581894874572754\n",
      "step 4705 loss: 2.6404364109039307\n",
      "step 4706 loss: 2.6207540035247803\n",
      "step 4707 loss: 2.6564199924468994\n",
      "step 4708 loss: 2.5600221157073975\n",
      "step 4709 loss: 2.6443910598754883\n",
      "step 4710 loss: 2.616105556488037\n",
      "step 4711 loss: 2.570774793624878\n",
      "step 4712 loss: 2.7034971714019775\n",
      "step 4713 loss: 2.5439703464508057\n",
      "step 4714 loss: 2.54206919670105\n",
      "step 4715 loss: 2.5469422340393066\n",
      "step 4716 loss: 2.5140938758850098\n",
      "step 4717 loss: 2.691800832748413\n",
      "step 4718 loss: 2.4608166217803955\n",
      "step 4719 loss: 2.580291748046875\n",
      "step 4720 loss: 2.6138510704040527\n",
      "step 4721 loss: 2.538604736328125\n",
      "step 4722 loss: 2.5935115814208984\n",
      "step 4723 loss: 2.53745174407959\n",
      "step 4724 loss: 2.5609445571899414\n",
      "step 4725 loss: 2.5899908542633057\n",
      "step 4726 loss: 2.6603896617889404\n",
      "step 4727 loss: 2.5350182056427\n",
      "step 4728 loss: 2.6772634983062744\n",
      "step 4729 loss: 2.7458510398864746\n",
      "step 4730 loss: 2.623493194580078\n",
      "step 4731 loss: 2.5845582485198975\n",
      "step 4732 loss: 2.5450656414031982\n",
      "step 4733 loss: 2.6004254817962646\n",
      "step 4734 loss: 2.4779603481292725\n",
      "step 4735 loss: 2.415252447128296\n",
      "step 4736 loss: 2.5783398151397705\n",
      "step 4737 loss: 2.5763051509857178\n",
      "step 4738 loss: 2.499690532684326\n",
      "step 4739 loss: 2.4829890727996826\n",
      "step 4740 loss: 2.52250075340271\n",
      "step 4741 loss: 2.669219732284546\n",
      "step 4742 loss: 2.5227229595184326\n",
      "step 4743 loss: 2.586935043334961\n",
      "step 4744 loss: 2.546783447265625\n",
      "step 4745 loss: 2.6525447368621826\n",
      "step 4746 loss: 2.681018352508545\n",
      "step 4747 loss: 2.5921828746795654\n",
      "step 4748 loss: 2.586869478225708\n",
      "step 4749 loss: 2.525808572769165\n",
      "step 4750 loss: 2.5675623416900635\n",
      "step 4751 loss: 2.602081060409546\n",
      "step 4752 loss: 2.555023670196533\n",
      "step 4753 loss: 2.5632307529449463\n",
      "step 4754 loss: 2.6458616256713867\n",
      "step 4755 loss: 2.6180310249328613\n",
      "step 4756 loss: 2.689358949661255\n",
      "step 4757 loss: 2.5574393272399902\n",
      "step 4758 loss: 2.587446451187134\n",
      "step 4759 loss: 2.5653867721557617\n",
      "step 4760 loss: 2.50056791305542\n",
      "step 4761 loss: 2.5593745708465576\n",
      "step 4762 loss: 2.5071184635162354\n",
      "step 4763 loss: 2.5781149864196777\n",
      "step 4764 loss: 2.5432326793670654\n",
      "step 4765 loss: 2.598865270614624\n",
      "step 4766 loss: 2.622375726699829\n",
      "step 4767 loss: 2.6178126335144043\n",
      "step 4768 loss: 2.471299171447754\n",
      "step 4769 loss: 2.5638625621795654\n",
      "step 4770 loss: 2.5751190185546875\n",
      "step 4771 loss: 2.7206225395202637\n",
      "step 4772 loss: 2.5724427700042725\n",
      "step 4773 loss: 2.5659751892089844\n",
      "step 4774 loss: 2.6768457889556885\n",
      "step 4775 loss: 2.5416243076324463\n",
      "step 4776 loss: 2.5735180377960205\n",
      "step 4777 loss: 2.5453386306762695\n",
      "step 4778 loss: 2.6279051303863525\n",
      "step 4779 loss: 2.537598133087158\n",
      "step 4780 loss: 2.5590717792510986\n",
      "step 4781 loss: 2.514275074005127\n",
      "step 4782 loss: 2.436958074569702\n",
      "step 4783 loss: 2.5651471614837646\n",
      "step 4784 loss: 2.517944812774658\n",
      "step 4785 loss: 2.558926820755005\n",
      "step 4786 loss: 2.5324184894561768\n",
      "step 4787 loss: 2.6255924701690674\n",
      "step 4788 loss: 2.69216251373291\n",
      "step 4789 loss: 2.5968310832977295\n",
      "step 4790 loss: 2.683168888092041\n",
      "step 4791 loss: 2.523134708404541\n",
      "step 4792 loss: 2.551891565322876\n",
      "step 4793 loss: 2.6516571044921875\n",
      "step 4794 loss: 2.6077308654785156\n",
      "step 4795 loss: 2.683668613433838\n",
      "step 4796 loss: 2.6029882431030273\n",
      "step 4797 loss: 2.525434970855713\n",
      "step 4798 loss: 2.5571184158325195\n",
      "step 4799 loss: 2.519212245941162\n",
      "step 4800 loss: 2.5814590454101562\n",
      "step 4801 loss: 2.5282113552093506\n",
      "step 4802 loss: 2.5249528884887695\n",
      "step 4803 loss: 2.396149158477783\n",
      "step 4804 loss: 2.5541324615478516\n",
      "step 4805 loss: 2.538163900375366\n",
      "step 4806 loss: 2.5172019004821777\n",
      "step 4807 loss: 2.4621033668518066\n",
      "step 4808 loss: 2.706662178039551\n",
      "step 4809 loss: 2.640679359436035\n",
      "step 4810 loss: 2.5818214416503906\n",
      "step 4811 loss: 2.5835657119750977\n",
      "step 4812 loss: 2.4390809535980225\n",
      "step 4813 loss: 2.669045925140381\n",
      "step 4814 loss: 2.7439608573913574\n",
      "step 4815 loss: 2.5365238189697266\n",
      "step 4816 loss: 2.5349650382995605\n",
      "step 4817 loss: 2.6148722171783447\n",
      "step 4818 loss: 2.6076717376708984\n",
      "step 4819 loss: 2.539839506149292\n",
      "step 4820 loss: 2.5110204219818115\n",
      "step 4821 loss: 2.5481491088867188\n",
      "step 4822 loss: 2.7277350425720215\n",
      "step 4823 loss: 2.640409469604492\n",
      "step 4824 loss: 2.525998830795288\n",
      "step 4825 loss: 2.6075470447540283\n",
      "step 4826 loss: 2.51574969291687\n",
      "step 4827 loss: 2.598874807357788\n",
      "step 4828 loss: 2.605998992919922\n",
      "step 4829 loss: 2.6548261642456055\n",
      "step 4830 loss: 2.4724373817443848\n",
      "step 4831 loss: 2.620680093765259\n",
      "step 4832 loss: 2.5695536136627197\n",
      "step 4833 loss: 2.5216100215911865\n",
      "step 4834 loss: 2.548269510269165\n",
      "step 4835 loss: 2.5189900398254395\n",
      "step 4836 loss: 2.6637136936187744\n",
      "step 4837 loss: 2.563772678375244\n",
      "step 4838 loss: 2.5104048252105713\n",
      "step 4839 loss: 2.578639507293701\n",
      "step 4840 loss: 2.5815060138702393\n",
      "step 4841 loss: 2.7092528343200684\n",
      "step 4842 loss: 2.545722723007202\n",
      "step 4843 loss: 2.493403673171997\n",
      "step 4844 loss: 2.5669126510620117\n",
      "step 4845 loss: 2.478867292404175\n",
      "step 4846 loss: 2.493513345718384\n",
      "step 4847 loss: 2.6240272521972656\n",
      "step 4848 loss: 2.606653928756714\n",
      "step 4849 loss: 2.4645273685455322\n",
      "step 4850 loss: 2.6471173763275146\n",
      "step 4851 loss: 2.619907855987549\n",
      "step 4852 loss: 2.557833433151245\n",
      "step 4853 loss: 2.5523934364318848\n",
      "step 4854 loss: 2.4980669021606445\n",
      "step 4855 loss: 2.6262471675872803\n",
      "step 4856 loss: 2.628098726272583\n",
      "step 4857 loss: 2.5205907821655273\n",
      "step 4858 loss: 2.463073253631592\n",
      "step 4859 loss: 2.5505714416503906\n",
      "step 4860 loss: 2.525449752807617\n",
      "step 4861 loss: 2.519254684448242\n",
      "step 4862 loss: 2.4888393878936768\n",
      "step 4863 loss: 2.6583762168884277\n",
      "step 4864 loss: 2.6275346279144287\n",
      "step 4865 loss: 2.594377279281616\n",
      "step 4866 loss: 2.5190937519073486\n",
      "step 4867 loss: 2.5488064289093018\n",
      "step 4868 loss: 2.4601240158081055\n",
      "step 4869 loss: 2.558537483215332\n",
      "step 4870 loss: 2.566645860671997\n",
      "step 4871 loss: 2.53996205329895\n",
      "step 4872 loss: 2.5803678035736084\n",
      "step 4873 loss: 2.601555109024048\n",
      "step 4874 loss: 2.5974042415618896\n",
      "step 4875 loss: 2.581946611404419\n",
      "step 4876 loss: 2.6250417232513428\n",
      "step 4877 loss: 2.712831497192383\n",
      "step 4878 loss: 2.5215377807617188\n",
      "step 4879 loss: 2.5176806449890137\n",
      "step 4880 loss: 2.5533602237701416\n",
      "step 4881 loss: 2.4978268146514893\n",
      "step 4882 loss: 2.517324686050415\n",
      "step 4883 loss: 2.5759971141815186\n",
      "step 4884 loss: 2.533034324645996\n",
      "step 4885 loss: 2.666814088821411\n",
      "step 4886 loss: 2.5512850284576416\n",
      "step 4887 loss: 2.561777114868164\n",
      "step 4888 loss: 2.4579217433929443\n",
      "step 4889 loss: 2.517228841781616\n",
      "step 4890 loss: 2.5985026359558105\n",
      "step 4891 loss: 2.6415317058563232\n",
      "step 4892 loss: 2.5534756183624268\n",
      "step 4893 loss: 2.535705804824829\n",
      "step 4894 loss: 2.5521299839019775\n",
      "step 4895 loss: 2.522226333618164\n",
      "step 4896 loss: 2.554854154586792\n",
      "step 4897 loss: 2.5458059310913086\n",
      "step 4898 loss: 2.4966323375701904\n",
      "step 4899 loss: 2.7131011486053467\n",
      "step 4900 loss: 2.531094789505005\n",
      "step 4901 loss: 2.482861280441284\n",
      "step 4902 loss: 2.696436882019043\n",
      "step 4903 loss: 2.581190586090088\n",
      "step 4904 loss: 2.602978229522705\n",
      "step 4905 loss: 2.4901134967803955\n",
      "step 4906 loss: 2.485226631164551\n",
      "step 4907 loss: 2.631608247756958\n",
      "step 4908 loss: 2.589358329772949\n",
      "step 4909 loss: 2.496206521987915\n",
      "step 4910 loss: 2.430617332458496\n",
      "step 4911 loss: 2.513728141784668\n",
      "step 4912 loss: 2.6712660789489746\n",
      "step 4913 loss: 2.665750741958618\n",
      "step 4914 loss: 2.62463116645813\n",
      "step 4915 loss: 2.69895076751709\n",
      "step 4916 loss: 2.570192575454712\n",
      "step 4917 loss: 2.5644702911376953\n",
      "step 4918 loss: 2.6164467334747314\n",
      "step 4919 loss: 2.541447401046753\n",
      "step 4920 loss: 2.5918657779693604\n",
      "step 4921 loss: 2.4857614040374756\n",
      "step 4922 loss: 2.5815227031707764\n",
      "step 4923 loss: 2.476491689682007\n",
      "step 4924 loss: 2.4429521560668945\n",
      "step 4925 loss: 2.4839322566986084\n",
      "step 4926 loss: 2.623786687850952\n",
      "step 4927 loss: 2.576120376586914\n",
      "step 4928 loss: 2.6252710819244385\n",
      "step 4929 loss: 2.5466740131378174\n",
      "step 4930 loss: 2.605548143386841\n",
      "step 4931 loss: 2.570058584213257\n",
      "step 4932 loss: 2.5817253589630127\n",
      "step 4933 loss: 2.5841386318206787\n",
      "step 4934 loss: 2.53627872467041\n",
      "step 4935 loss: 2.595717668533325\n",
      "step 4936 loss: 2.6026504039764404\n",
      "step 4937 loss: 2.544550657272339\n",
      "step 4938 loss: 2.624976634979248\n",
      "step 4939 loss: 2.578209638595581\n",
      "step 4940 loss: 2.5169382095336914\n",
      "step 4941 loss: 2.561490058898926\n",
      "step 4942 loss: 2.4683046340942383\n",
      "step 4943 loss: 2.599116802215576\n",
      "step 4944 loss: 2.5293514728546143\n",
      "step 4945 loss: 2.536900758743286\n",
      "step 4946 loss: 2.506706953048706\n",
      "step 4947 loss: 2.4960644245147705\n",
      "step 4948 loss: 2.5126097202301025\n",
      "step 4949 loss: 2.5766401290893555\n",
      "step 4950 loss: 2.634124755859375\n",
      "step 4951 loss: 2.5158753395080566\n",
      "step 4952 loss: 2.5105583667755127\n",
      "step 4953 loss: 2.5700507164001465\n",
      "step 4954 loss: 2.5440456867218018\n",
      "step 4955 loss: 2.6257104873657227\n",
      "step 4956 loss: 2.5356125831604004\n",
      "step 4957 loss: 2.434647560119629\n",
      "step 4958 loss: 2.505326271057129\n",
      "step 4959 loss: 2.5555531978607178\n",
      "step 4960 loss: 2.6102569103240967\n",
      "step 4961 loss: 2.4905874729156494\n",
      "step 4962 loss: 2.5579044818878174\n",
      "step 4963 loss: 2.561173439025879\n",
      "step 4964 loss: 2.536501884460449\n",
      "step 4965 loss: 2.5021891593933105\n",
      "step 4966 loss: 2.6562869548797607\n",
      "step 4967 loss: 2.7625701427459717\n",
      "step 4968 loss: 2.667046546936035\n",
      "step 4969 loss: 2.605034828186035\n",
      "step 4970 loss: 2.450627326965332\n",
      "step 4971 loss: 2.551783323287964\n",
      "step 4972 loss: 2.4499855041503906\n",
      "step 4973 loss: 2.4674389362335205\n",
      "step 4974 loss: 2.421515464782715\n",
      "step 4975 loss: 2.635676145553589\n",
      "step 4976 loss: 2.543360948562622\n",
      "step 4977 loss: 2.5240707397460938\n",
      "step 4978 loss: 2.5289809703826904\n",
      "step 4979 loss: 2.5101218223571777\n",
      "step 4980 loss: 2.68878436088562\n",
      "step 4981 loss: 2.6332316398620605\n",
      "step 4982 loss: 2.575751781463623\n",
      "step 4983 loss: 2.490499973297119\n",
      "step 4984 loss: 2.646756887435913\n",
      "step 4985 loss: 2.436109781265259\n",
      "step 4986 loss: 2.586451768875122\n",
      "step 4987 loss: 2.5344126224517822\n",
      "step 4988 loss: 2.610459804534912\n",
      "step 4989 loss: 2.6034014225006104\n",
      "step 4990 loss: 2.4938361644744873\n",
      "step 4991 loss: 2.495565176010132\n",
      "step 4992 loss: 2.558687210083008\n",
      "step 4993 loss: 2.4306209087371826\n",
      "step 4994 loss: 2.592252254486084\n",
      "step 4995 loss: 2.45784854888916\n",
      "step 4996 loss: 2.58321475982666\n",
      "step 4997 loss: 2.5510902404785156\n",
      "step 4998 loss: 2.557363986968994\n",
      "step 4999 loss: 2.536130905151367\n",
      "step 5000 loss: 2.515348434448242\n",
      "step 5001 loss: 2.4744279384613037\n",
      "step 5002 loss: 2.6085002422332764\n",
      "step 5003 loss: 2.6852359771728516\n",
      "step 5004 loss: 2.440641403198242\n",
      "step 5005 loss: 2.5239131450653076\n",
      "step 5006 loss: 2.583158016204834\n",
      "step 5007 loss: 2.5586931705474854\n",
      "step 5008 loss: 2.5336263179779053\n",
      "step 5009 loss: 2.5082459449768066\n",
      "step 5010 loss: 2.563934087753296\n",
      "step 5011 loss: 2.4099056720733643\n",
      "step 5012 loss: 2.549560070037842\n",
      "step 5013 loss: 2.627998113632202\n",
      "step 5014 loss: 2.597146511077881\n",
      "step 5015 loss: 2.6242775917053223\n",
      "step 5016 loss: 2.485126256942749\n",
      "step 5017 loss: 2.5880191326141357\n",
      "step 5018 loss: 2.552927017211914\n",
      "step 5019 loss: 2.498333215713501\n",
      "step 5020 loss: 2.521122455596924\n",
      "step 5021 loss: 2.5981087684631348\n",
      "step 5022 loss: 2.56059193611145\n",
      "step 5023 loss: 2.47790789604187\n",
      "step 5024 loss: 2.60870361328125\n",
      "step 5025 loss: 2.561020851135254\n",
      "step 5026 loss: 2.542356014251709\n",
      "step 5027 loss: 2.6399805545806885\n",
      "step 5028 loss: 2.698282480239868\n",
      "step 5029 loss: 2.541898250579834\n",
      "step 5030 loss: 2.5504090785980225\n",
      "step 5031 loss: 2.561579704284668\n",
      "step 5032 loss: 2.502772569656372\n",
      "step 5033 loss: 2.5175554752349854\n",
      "step 5034 loss: 2.681809663772583\n",
      "step 5035 loss: 2.603564977645874\n",
      "step 5036 loss: 2.5262417793273926\n",
      "step 5037 loss: 2.6207547187805176\n",
      "step 5038 loss: 2.538642168045044\n",
      "step 5039 loss: 2.5304956436157227\n",
      "step 5040 loss: 2.4993271827697754\n",
      "step 5041 loss: 2.671292304992676\n",
      "step 5042 loss: 2.6358695030212402\n",
      "step 5043 loss: 2.540466785430908\n",
      "step 5044 loss: 2.637118339538574\n",
      "step 5045 loss: 2.6086719036102295\n",
      "step 5046 loss: 2.593581438064575\n",
      "step 5047 loss: 2.52583646774292\n",
      "step 5048 loss: 2.579291343688965\n",
      "step 5049 loss: 2.4875173568725586\n",
      "step 5050 loss: 2.552349328994751\n",
      "step 5051 loss: 2.628919839859009\n",
      "step 5052 loss: 2.711430549621582\n",
      "step 5053 loss: 2.5554680824279785\n",
      "step 5054 loss: 2.4894022941589355\n",
      "step 5055 loss: 2.503077745437622\n",
      "step 5056 loss: 2.6182849407196045\n",
      "step 5057 loss: 2.5582029819488525\n",
      "step 5058 loss: 2.608365058898926\n",
      "step 5059 loss: 2.4718029499053955\n",
      "step 5060 loss: 2.507847309112549\n",
      "step 5061 loss: 2.589648962020874\n",
      "step 5062 loss: 2.5158352851867676\n",
      "step 5063 loss: 2.4925405979156494\n",
      "step 5064 loss: 2.5380494594573975\n",
      "step 5065 loss: 2.617243766784668\n",
      "step 5066 loss: 2.4109203815460205\n",
      "step 5067 loss: 2.6228506565093994\n",
      "step 5068 loss: 2.5525126457214355\n",
      "step 5069 loss: 2.6780455112457275\n",
      "step 5070 loss: 2.5741817951202393\n",
      "step 5071 loss: 2.4839026927948\n",
      "step 5072 loss: 2.5557072162628174\n",
      "step 5073 loss: 2.7229816913604736\n",
      "step 5074 loss: 2.5009288787841797\n",
      "step 5075 loss: 2.619807481765747\n",
      "step 5076 loss: 2.638096809387207\n",
      "step 5077 loss: 2.5129761695861816\n",
      "step 5078 loss: 2.4798192977905273\n",
      "step 5079 loss: 2.5664920806884766\n",
      "step 5080 loss: 2.515793561935425\n",
      "step 5081 loss: 2.5715935230255127\n",
      "step 5082 loss: 2.5462307929992676\n",
      "step 5083 loss: 2.5792837142944336\n",
      "step 5084 loss: 2.6862916946411133\n",
      "step 5085 loss: 2.4733943939208984\n",
      "step 5086 loss: 2.5147509574890137\n",
      "step 5087 loss: 2.524639129638672\n",
      "step 5088 loss: 2.588909864425659\n",
      "step 5089 loss: 2.632650375366211\n",
      "step 5090 loss: 2.733048677444458\n",
      "step 5091 loss: 2.4842464923858643\n",
      "step 5092 loss: 2.5909650325775146\n",
      "step 5093 loss: 2.561230182647705\n",
      "step 5094 loss: 2.5288808345794678\n",
      "step 5095 loss: 2.65134334564209\n",
      "step 5096 loss: 2.587110757827759\n",
      "step 5097 loss: 2.569359540939331\n",
      "step 5098 loss: 2.4174787998199463\n",
      "step 5099 loss: 2.4629318714141846\n",
      "step 5100 loss: 2.4976727962493896\n",
      "step 5101 loss: 2.473238706588745\n",
      "step 5102 loss: 2.5295848846435547\n",
      "step 5103 loss: 2.5540103912353516\n",
      "step 5104 loss: 2.6150107383728027\n",
      "step 5105 loss: 2.530414581298828\n",
      "step 5106 loss: 2.479557514190674\n",
      "step 5107 loss: 2.5136897563934326\n",
      "step 5108 loss: 2.5505282878875732\n",
      "step 5109 loss: 2.497313976287842\n",
      "step 5110 loss: 2.4484851360321045\n",
      "step 5111 loss: 2.4803884029388428\n",
      "step 5112 loss: 2.5390126705169678\n",
      "step 5113 loss: 2.5508337020874023\n",
      "step 5114 loss: 2.509243965148926\n",
      "step 5115 loss: 2.488892078399658\n",
      "step 5116 loss: 2.4691007137298584\n",
      "step 5117 loss: 2.4478015899658203\n",
      "step 5118 loss: 2.5284976959228516\n",
      "step 5119 loss: 2.5643184185028076\n",
      "step 5120 loss: 2.5659265518188477\n",
      "step 5121 loss: 2.451662063598633\n",
      "step 5122 loss: 2.5021116733551025\n",
      "step 5123 loss: 2.5849575996398926\n",
      "step 5124 loss: 2.683339834213257\n",
      "step 5125 loss: 2.606792688369751\n",
      "step 5126 loss: 2.6209332942962646\n",
      "step 5127 loss: 2.5691871643066406\n",
      "step 5128 loss: 2.649615526199341\n",
      "step 5129 loss: 2.5028011798858643\n",
      "step 5130 loss: 2.5175254344940186\n",
      "step 5131 loss: 2.5026795864105225\n",
      "step 5132 loss: 2.5131173133850098\n",
      "step 5133 loss: 2.5105679035186768\n",
      "step 5134 loss: 2.550595760345459\n",
      "step 5135 loss: 2.6369311809539795\n",
      "step 5136 loss: 2.6147100925445557\n",
      "step 5137 loss: 2.690343141555786\n",
      "step 5138 loss: 2.562919855117798\n",
      "step 5139 loss: 2.3999528884887695\n",
      "step 5140 loss: 2.578763723373413\n",
      "step 5141 loss: 2.495887041091919\n",
      "step 5142 loss: 2.493730068206787\n",
      "step 5143 loss: 2.604865789413452\n",
      "step 5144 loss: 2.6751630306243896\n",
      "step 5145 loss: 2.448639392852783\n",
      "step 5146 loss: 2.57289457321167\n",
      "step 5147 loss: 2.5326476097106934\n",
      "step 5148 loss: 2.507199287414551\n",
      "step 5149 loss: 2.5484471321105957\n",
      "step 5150 loss: 2.6058411598205566\n",
      "step 5151 loss: 2.56767201423645\n",
      "step 5152 loss: 2.548499345779419\n",
      "step 5153 loss: 2.5756633281707764\n",
      "step 5154 loss: 2.571404218673706\n",
      "step 5155 loss: 2.5985171794891357\n",
      "step 5156 loss: 2.5322113037109375\n",
      "step 5157 loss: 2.704390048980713\n",
      "step 5158 loss: 2.5395216941833496\n",
      "step 5159 loss: 2.353243589401245\n",
      "step 5160 loss: 2.559633255004883\n",
      "step 5161 loss: 2.4497036933898926\n",
      "step 5162 loss: 2.591620922088623\n",
      "step 5163 loss: 2.6536705493927\n",
      "step 5164 loss: 2.603247880935669\n",
      "step 5165 loss: 2.562772750854492\n",
      "step 5166 loss: 2.4376091957092285\n",
      "step 5167 loss: 2.562817096710205\n",
      "step 5168 loss: 2.5365712642669678\n",
      "step 5169 loss: 2.5352084636688232\n",
      "step 5170 loss: 2.5721821784973145\n",
      "step 5171 loss: 2.6367151737213135\n",
      "step 5172 loss: 2.4463448524475098\n",
      "step 5173 loss: 2.60443377494812\n",
      "step 5174 loss: 2.6385080814361572\n",
      "step 5175 loss: 2.6053102016448975\n",
      "step 5176 loss: 2.6737847328186035\n",
      "step 5177 loss: 2.5072343349456787\n",
      "step 5178 loss: 2.5473241806030273\n",
      "step 5179 loss: 2.4666175842285156\n",
      "step 5180 loss: 2.5201005935668945\n",
      "step 5181 loss: 2.531552791595459\n",
      "step 5182 loss: 2.513608455657959\n",
      "step 5183 loss: 2.648341178894043\n",
      "step 5184 loss: 2.6133782863616943\n",
      "step 5185 loss: 2.5804190635681152\n",
      "step 5186 loss: 2.64776349067688\n",
      "step 5187 loss: 2.5534749031066895\n",
      "step 5188 loss: 2.595421552658081\n",
      "step 5189 loss: 2.5513525009155273\n",
      "step 5190 loss: 2.7171523571014404\n",
      "step 5191 loss: 2.5546038150787354\n",
      "step 5192 loss: 2.5533697605133057\n",
      "step 5193 loss: 2.514216899871826\n",
      "step 5194 loss: 2.5426642894744873\n",
      "step 5195 loss: 2.5078728199005127\n",
      "step 5196 loss: 2.582852363586426\n",
      "step 5197 loss: 2.6107635498046875\n",
      "step 5198 loss: 2.527393102645874\n",
      "step 5199 loss: 2.5543107986450195\n",
      "step 5200 loss: 2.5762481689453125\n",
      "step 5201 loss: 2.6111085414886475\n",
      "step 5202 loss: 2.480576515197754\n",
      "step 5203 loss: 2.643373489379883\n",
      "step 5204 loss: 2.5361039638519287\n",
      "step 5205 loss: 2.412558078765869\n",
      "step 5206 loss: 2.629307985305786\n",
      "step 5207 loss: 2.42496919631958\n",
      "step 5208 loss: 2.5393893718719482\n",
      "step 5209 loss: 2.5809450149536133\n",
      "step 5210 loss: 2.6272552013397217\n",
      "step 5211 loss: 2.538390636444092\n",
      "step 5212 loss: 2.460411787033081\n",
      "step 5213 loss: 2.543203353881836\n",
      "step 5214 loss: 2.5230417251586914\n",
      "step 5215 loss: 2.5791819095611572\n",
      "step 5216 loss: 2.5186400413513184\n",
      "step 5217 loss: 2.5032994747161865\n",
      "step 5218 loss: 2.517732620239258\n",
      "step 5219 loss: 2.444058656692505\n",
      "step 5220 loss: 2.4282374382019043\n",
      "step 5221 loss: 2.566833257675171\n",
      "step 5222 loss: 2.5428900718688965\n",
      "step 5223 loss: 2.505316734313965\n",
      "step 5224 loss: 2.5994651317596436\n",
      "step 5225 loss: 2.5210437774658203\n",
      "step 5226 loss: 2.5981616973876953\n",
      "step 5227 loss: 2.5519018173217773\n",
      "step 5228 loss: 2.523390293121338\n",
      "step 5229 loss: 2.6089394092559814\n",
      "step 5230 loss: 2.605074167251587\n",
      "step 5231 loss: 2.5292906761169434\n",
      "step 5232 loss: 2.47769832611084\n",
      "step 5233 loss: 2.636167049407959\n",
      "step 5234 loss: 2.472609519958496\n",
      "step 5235 loss: 2.6160881519317627\n",
      "step 5236 loss: 2.558712959289551\n",
      "step 5237 loss: 2.4349911212921143\n",
      "step 5238 loss: 2.6391656398773193\n",
      "step 5239 loss: 2.5496928691864014\n",
      "step 5240 loss: 2.5262179374694824\n",
      "step 5241 loss: 2.586059331893921\n",
      "step 5242 loss: 2.4378364086151123\n",
      "step 5243 loss: 2.510484457015991\n",
      "step 5244 loss: 2.5146636962890625\n",
      "step 5245 loss: 2.5023083686828613\n",
      "step 5246 loss: 2.6120243072509766\n",
      "step 5247 loss: 2.5882155895233154\n",
      "step 5248 loss: 2.4859912395477295\n",
      "step 5249 loss: 2.526777744293213\n",
      "step 5250 loss: 2.440721035003662\n",
      "step 5251 loss: 2.521601438522339\n",
      "step 5252 loss: 2.6397907733917236\n",
      "step 5253 loss: 2.5025453567504883\n",
      "step 5254 loss: 2.4921209812164307\n",
      "step 5255 loss: 2.5859804153442383\n",
      "step 5256 loss: 2.6337194442749023\n",
      "step 5257 loss: 2.6686267852783203\n",
      "step 5258 loss: 2.591538906097412\n",
      "step 5259 loss: 2.601344347000122\n",
      "step 5260 loss: 2.5697543621063232\n",
      "step 5261 loss: 2.5710859298706055\n",
      "step 5262 loss: 2.5892295837402344\n",
      "step 5263 loss: 2.4656105041503906\n",
      "step 5264 loss: 2.566267251968384\n",
      "step 5265 loss: 2.403441905975342\n",
      "step 5266 loss: 2.549597978591919\n",
      "step 5267 loss: 2.4957220554351807\n",
      "step 5268 loss: 2.5841453075408936\n",
      "step 5269 loss: 2.610226631164551\n",
      "step 5270 loss: 2.6502573490142822\n",
      "step 5271 loss: 2.5403952598571777\n",
      "step 5272 loss: 2.519595146179199\n",
      "step 5273 loss: 2.560298204421997\n",
      "step 5274 loss: 2.5035667419433594\n",
      "step 5275 loss: 2.5707154273986816\n",
      "step 5276 loss: 2.5975310802459717\n",
      "step 5277 loss: 2.6585965156555176\n",
      "step 5278 loss: 2.6427838802337646\n",
      "step 5279 loss: 2.620600938796997\n",
      "step 5280 loss: 2.635521173477173\n",
      "step 5281 loss: 2.5751490592956543\n",
      "step 5282 loss: 2.608844757080078\n",
      "step 5283 loss: 2.5228874683380127\n",
      "step 5284 loss: 2.712003231048584\n",
      "step 5285 loss: 2.527402639389038\n",
      "step 5286 loss: 2.511909008026123\n",
      "step 5287 loss: 2.5219924449920654\n",
      "step 5288 loss: 2.501962423324585\n",
      "step 5289 loss: 2.4659597873687744\n",
      "step 5290 loss: 2.487985134124756\n",
      "step 5291 loss: 2.5837135314941406\n",
      "step 5292 loss: 2.4793717861175537\n",
      "step 5293 loss: 2.661221981048584\n",
      "step 5294 loss: 2.529550790786743\n",
      "step 5295 loss: 2.373443841934204\n",
      "step 5296 loss: 2.5885541439056396\n",
      "step 5297 loss: 2.6171000003814697\n",
      "step 5298 loss: 2.594308614730835\n",
      "step 5299 loss: 2.6235179901123047\n",
      "step 5300 loss: 2.6668245792388916\n",
      "step 5301 loss: 2.521925210952759\n",
      "step 5302 loss: 2.394853115081787\n",
      "step 5303 loss: 2.633679151535034\n",
      "step 5304 loss: 2.534379243850708\n",
      "step 5305 loss: 2.5555059909820557\n",
      "step 5306 loss: 2.5086429119110107\n",
      "step 5307 loss: 2.5598416328430176\n",
      "step 5308 loss: 2.6376819610595703\n",
      "step 5309 loss: 2.60494065284729\n",
      "step 5310 loss: 2.5592358112335205\n",
      "step 5311 loss: 2.505201816558838\n",
      "step 5312 loss: 2.5220155715942383\n",
      "step 5313 loss: 2.5588674545288086\n",
      "step 5314 loss: 2.507755756378174\n",
      "step 5315 loss: 2.416364908218384\n",
      "step 5316 loss: 2.6006035804748535\n",
      "step 5317 loss: 2.5119245052337646\n",
      "step 5318 loss: 2.527879476547241\n",
      "step 5319 loss: 2.6326775550842285\n",
      "step 5320 loss: 2.6659698486328125\n",
      "step 5321 loss: 2.5455307960510254\n",
      "step 5322 loss: 2.5482077598571777\n",
      "step 5323 loss: 2.474053144454956\n",
      "step 5324 loss: 2.502432346343994\n",
      "step 5325 loss: 2.5257954597473145\n",
      "step 5326 loss: 2.521160125732422\n",
      "step 5327 loss: 2.56514835357666\n",
      "step 5328 loss: 2.488574504852295\n",
      "step 5329 loss: 2.4930646419525146\n",
      "step 5330 loss: 2.5500895977020264\n",
      "step 5331 loss: 2.483952522277832\n",
      "step 5332 loss: 2.4936022758483887\n",
      "step 5333 loss: 2.47560453414917\n",
      "step 5334 loss: 2.5456552505493164\n",
      "step 5335 loss: 2.655433416366577\n",
      "step 5336 loss: 2.7037315368652344\n",
      "step 5337 loss: 2.4331109523773193\n",
      "step 5338 loss: 2.460306167602539\n",
      "step 5339 loss: 2.5808281898498535\n",
      "step 5340 loss: 2.6334335803985596\n",
      "step 5341 loss: 2.4758145809173584\n",
      "step 5342 loss: 2.502502202987671\n",
      "step 5343 loss: 2.5576961040496826\n",
      "step 5344 loss: 2.491187334060669\n",
      "step 5345 loss: 2.6030828952789307\n",
      "step 5346 loss: 2.6057024002075195\n",
      "step 5347 loss: 2.5093212127685547\n",
      "step 5348 loss: 2.531545639038086\n",
      "step 5349 loss: 2.474905014038086\n",
      "step 5350 loss: 2.547677516937256\n",
      "step 5351 loss: 2.513502836227417\n",
      "step 5352 loss: 2.6217076778411865\n",
      "step 5353 loss: 2.5418665409088135\n",
      "step 5354 loss: 2.551302194595337\n",
      "step 5355 loss: 2.5491549968719482\n",
      "step 5356 loss: 2.4019992351531982\n",
      "step 5357 loss: 2.433485746383667\n",
      "step 5358 loss: 2.455937147140503\n",
      "step 5359 loss: 2.5409603118896484\n",
      "step 5360 loss: 2.5124099254608154\n",
      "step 5361 loss: 2.5814576148986816\n",
      "step 5362 loss: 2.4671456813812256\n",
      "step 5363 loss: 2.626145839691162\n",
      "step 5364 loss: 2.3860702514648438\n",
      "step 5365 loss: 2.5083580017089844\n",
      "step 5366 loss: 2.5791261196136475\n",
      "step 5367 loss: 2.5390048027038574\n",
      "step 5368 loss: 2.4925637245178223\n",
      "step 5369 loss: 2.502051591873169\n",
      "step 5370 loss: 2.6097354888916016\n",
      "step 5371 loss: 2.559948682785034\n",
      "step 5372 loss: 2.5057029724121094\n",
      "step 5373 loss: 2.583557367324829\n",
      "step 5374 loss: 2.6728436946868896\n",
      "step 5375 loss: 2.5913262367248535\n",
      "step 5376 loss: 2.428402900695801\n",
      "step 5377 loss: 2.6162612438201904\n",
      "step 5378 loss: 2.51090407371521\n",
      "step 5379 loss: 2.41387677192688\n",
      "step 5380 loss: 2.572499990463257\n",
      "step 5381 loss: 2.6482372283935547\n",
      "step 5382 loss: 2.4723687171936035\n",
      "step 5383 loss: 2.563202142715454\n",
      "step 5384 loss: 2.5022029876708984\n",
      "step 5385 loss: 2.5854012966156006\n",
      "step 5386 loss: 2.5410590171813965\n",
      "step 5387 loss: 2.546017646789551\n",
      "step 5388 loss: 2.5929250717163086\n",
      "step 5389 loss: 2.538710355758667\n",
      "step 5390 loss: 2.569399118423462\n",
      "step 5391 loss: 2.523317813873291\n",
      "step 5392 loss: 2.415806770324707\n",
      "step 5393 loss: 2.512216091156006\n",
      "step 5394 loss: 2.5275168418884277\n",
      "step 5395 loss: 2.5595738887786865\n",
      "step 5396 loss: 2.6742501258850098\n",
      "step 5397 loss: 2.3543734550476074\n",
      "step 5398 loss: 2.465381383895874\n",
      "step 5399 loss: 2.7500998973846436\n",
      "step 5400 loss: 2.5578126907348633\n",
      "step 5401 loss: 2.633100748062134\n",
      "step 5402 loss: 2.5034546852111816\n",
      "step 5403 loss: 2.482607364654541\n",
      "step 5404 loss: 2.4403579235076904\n",
      "step 5405 loss: 2.5525033473968506\n",
      "step 5406 loss: 2.659552574157715\n",
      "step 5407 loss: 2.4819021224975586\n",
      "step 5408 loss: 2.5394392013549805\n",
      "step 5409 loss: 2.4906013011932373\n",
      "step 5410 loss: 2.3796212673187256\n",
      "step 5411 loss: 2.5122087001800537\n",
      "step 5412 loss: 2.503883123397827\n",
      "step 5413 loss: 2.6173555850982666\n",
      "step 5414 loss: 2.46379017829895\n",
      "step 5415 loss: 2.3684022426605225\n",
      "step 5416 loss: 2.5286502838134766\n",
      "step 5417 loss: 2.548680305480957\n",
      "step 5418 loss: 2.435699462890625\n",
      "step 5419 loss: 2.6023645401000977\n",
      "step 5420 loss: 2.5744359493255615\n",
      "step 5421 loss: 2.4479258060455322\n",
      "step 5422 loss: 2.6169180870056152\n",
      "step 5423 loss: 2.5771074295043945\n",
      "step 5424 loss: 2.486957311630249\n",
      "step 5425 loss: 2.616748571395874\n",
      "step 5426 loss: 2.500450372695923\n",
      "step 5427 loss: 2.507148504257202\n",
      "step 5428 loss: 2.5649592876434326\n",
      "step 5429 loss: 2.648907423019409\n",
      "step 5430 loss: 2.60799503326416\n",
      "step 5431 loss: 2.5208170413970947\n",
      "step 5432 loss: 2.514235258102417\n",
      "step 5433 loss: 2.6106584072113037\n",
      "step 5434 loss: 2.5683937072753906\n",
      "step 5435 loss: 2.5818800926208496\n",
      "step 5436 loss: 2.5181005001068115\n",
      "step 5437 loss: 2.4996910095214844\n",
      "step 5438 loss: 2.4574010372161865\n",
      "step 5439 loss: 2.49613881111145\n",
      "step 5440 loss: 2.684173345565796\n",
      "step 5441 loss: 2.542572259902954\n",
      "step 5442 loss: 2.508951425552368\n",
      "step 5443 loss: 2.4763972759246826\n",
      "step 5444 loss: 2.536564350128174\n",
      "step 5445 loss: 2.418393611907959\n",
      "step 5446 loss: 2.5473616123199463\n",
      "step 5447 loss: 2.457357406616211\n",
      "step 5448 loss: 2.599877119064331\n",
      "step 5449 loss: 2.561490535736084\n",
      "step 5450 loss: 2.45082950592041\n",
      "step 5451 loss: 2.563413619995117\n",
      "step 5452 loss: 2.561967372894287\n",
      "step 5453 loss: 2.564202070236206\n",
      "step 5454 loss: 2.542001962661743\n",
      "step 5455 loss: 2.5769498348236084\n",
      "step 5456 loss: 2.478315591812134\n",
      "step 5457 loss: 2.522603750228882\n",
      "step 5458 loss: 2.518371343612671\n",
      "step 5459 loss: 2.5656211376190186\n",
      "step 5460 loss: 2.6400299072265625\n",
      "step 5461 loss: 2.4833426475524902\n",
      "step 5462 loss: 2.538947343826294\n",
      "step 5463 loss: 2.5428144931793213\n",
      "step 5464 loss: 2.435088634490967\n",
      "step 5465 loss: 2.5133278369903564\n",
      "step 5466 loss: 2.5776002407073975\n",
      "step 5467 loss: 2.48968243598938\n",
      "step 5468 loss: 2.6057424545288086\n",
      "step 5469 loss: 2.6368017196655273\n",
      "step 5470 loss: 2.511593818664551\n",
      "step 5471 loss: 2.5287835597991943\n",
      "step 5472 loss: 2.4805896282196045\n",
      "step 5473 loss: 2.5686442852020264\n",
      "step 5474 loss: 2.418375015258789\n",
      "step 5475 loss: 2.4082157611846924\n",
      "step 5476 loss: 2.5344886779785156\n",
      "step 5477 loss: 2.3795955181121826\n",
      "step 5478 loss: 2.479761838912964\n",
      "step 5479 loss: 2.597646951675415\n",
      "step 5480 loss: 2.555213451385498\n",
      "step 5481 loss: 2.639958381652832\n",
      "step 5482 loss: 2.422121524810791\n",
      "step 5483 loss: 2.5360567569732666\n",
      "step 5484 loss: 2.63175106048584\n",
      "step 5485 loss: 2.500086784362793\n",
      "step 5486 loss: 2.5121328830718994\n",
      "step 5487 loss: 2.584827423095703\n",
      "step 5488 loss: 2.4815123081207275\n",
      "step 5489 loss: 2.562528610229492\n",
      "step 5490 loss: 2.5390701293945312\n",
      "step 5491 loss: 2.4714181423187256\n",
      "step 5492 loss: 2.538461685180664\n",
      "step 5493 loss: 2.524933099746704\n",
      "step 5494 loss: 2.6288716793060303\n",
      "step 5495 loss: 2.5230674743652344\n",
      "step 5496 loss: 2.544816732406616\n",
      "step 5497 loss: 2.48787784576416\n",
      "step 5498 loss: 2.5682034492492676\n",
      "step 5499 loss: 2.475376605987549\n",
      "step 5500 loss: 2.557616710662842\n",
      "step 5501 loss: 2.418024778366089\n",
      "step 5502 loss: 2.508300542831421\n",
      "step 5503 loss: 2.5428404808044434\n",
      "step 5504 loss: 2.5748391151428223\n",
      "step 5505 loss: 2.523177146911621\n",
      "step 5506 loss: 2.4775445461273193\n",
      "step 5507 loss: 2.4865214824676514\n",
      "step 5508 loss: 2.6255812644958496\n",
      "step 5509 loss: 2.504477024078369\n",
      "step 5510 loss: 2.4981119632720947\n",
      "step 5511 loss: 2.634167432785034\n",
      "step 5512 loss: 2.5908126831054688\n",
      "step 5513 loss: 2.6255955696105957\n",
      "step 5514 loss: 2.507403612136841\n",
      "step 5515 loss: 2.4651525020599365\n",
      "step 5516 loss: 2.6904478073120117\n",
      "step 5517 loss: 2.5371272563934326\n",
      "step 5518 loss: 2.626204252243042\n",
      "step 5519 loss: 2.6042559146881104\n",
      "step 5520 loss: 2.5200557708740234\n",
      "step 5521 loss: 2.4775683879852295\n",
      "step 5522 loss: 2.6607728004455566\n",
      "step 5523 loss: 2.559974193572998\n",
      "step 5524 loss: 2.5050575733184814\n",
      "step 5525 loss: 2.473857879638672\n",
      "step 5526 loss: 2.498833656311035\n",
      "step 5527 loss: 2.4698827266693115\n",
      "step 5528 loss: 2.5845718383789062\n",
      "step 5529 loss: 2.5773744583129883\n",
      "step 5530 loss: 2.5387299060821533\n",
      "step 5531 loss: 2.445845127105713\n",
      "step 5532 loss: 2.6624324321746826\n",
      "step 5533 loss: 2.5832109451293945\n",
      "step 5534 loss: 2.5171241760253906\n",
      "step 5535 loss: 2.433692693710327\n",
      "step 5536 loss: 2.3772761821746826\n",
      "step 5537 loss: 2.491349697113037\n",
      "step 5538 loss: 2.5440945625305176\n",
      "step 5539 loss: 2.5481438636779785\n",
      "step 5540 loss: 2.4665465354919434\n",
      "step 5541 loss: 2.466015338897705\n",
      "step 5542 loss: 2.4789106845855713\n",
      "step 5543 loss: 2.4675872325897217\n",
      "step 5544 loss: 2.466761589050293\n",
      "step 5545 loss: 2.5148613452911377\n",
      "step 5546 loss: 2.6634745597839355\n",
      "step 5547 loss: 2.4646902084350586\n",
      "step 5548 loss: 2.5009844303131104\n",
      "step 5549 loss: 2.460146427154541\n",
      "step 5550 loss: 2.4716508388519287\n",
      "step 5551 loss: 2.5764009952545166\n",
      "step 5552 loss: 2.481637716293335\n",
      "step 5553 loss: 2.4682905673980713\n",
      "step 5554 loss: 2.628584861755371\n",
      "step 5555 loss: 2.5266029834747314\n",
      "step 5556 loss: 2.443925619125366\n",
      "step 5557 loss: 2.485930919647217\n",
      "step 5558 loss: 2.4492299556732178\n",
      "step 5559 loss: 2.6385557651519775\n",
      "step 5560 loss: 2.538809061050415\n",
      "step 5561 loss: 2.5030789375305176\n",
      "step 5562 loss: 2.537625789642334\n",
      "step 5563 loss: 2.6016218662261963\n",
      "step 5564 loss: 2.505460262298584\n",
      "step 5565 loss: 2.4872829914093018\n",
      "step 5566 loss: 2.5264320373535156\n",
      "step 5567 loss: 2.6402900218963623\n",
      "step 5568 loss: 2.4545741081237793\n",
      "step 5569 loss: 2.5897793769836426\n",
      "step 5570 loss: 2.5236475467681885\n",
      "step 5571 loss: 2.3361430168151855\n",
      "step 5572 loss: 2.5391199588775635\n",
      "step 5573 loss: 2.52799916267395\n",
      "step 5574 loss: 2.3655529022216797\n",
      "step 5575 loss: 2.647254705429077\n",
      "step 5576 loss: 2.5693295001983643\n",
      "step 5577 loss: 2.5235707759857178\n",
      "step 5578 loss: 2.4438931941986084\n",
      "step 5579 loss: 2.4841718673706055\n",
      "step 5580 loss: 2.5240590572357178\n",
      "step 5581 loss: 2.605950117111206\n",
      "step 5582 loss: 2.498731851577759\n",
      "step 5583 loss: 2.4500255584716797\n",
      "step 5584 loss: 2.601696491241455\n",
      "step 5585 loss: 2.4553823471069336\n",
      "step 5586 loss: 2.423614501953125\n",
      "step 5587 loss: 2.6125612258911133\n",
      "step 5588 loss: 2.467172384262085\n",
      "step 5589 loss: 2.61538028717041\n",
      "step 5590 loss: 2.47873854637146\n",
      "step 5591 loss: 2.5011026859283447\n",
      "step 5592 loss: 2.413935422897339\n",
      "step 5593 loss: 2.6185390949249268\n",
      "step 5594 loss: 2.4894142150878906\n",
      "step 5595 loss: 2.5416512489318848\n",
      "step 5596 loss: 2.516800880432129\n",
      "step 5597 loss: 2.54650616645813\n",
      "step 5598 loss: 2.5084714889526367\n",
      "step 5599 loss: 2.476710319519043\n",
      "step 5600 loss: 2.5119683742523193\n",
      "step 5601 loss: 2.6323955059051514\n",
      "step 5602 loss: 2.538531541824341\n",
      "step 5603 loss: 2.4566588401794434\n",
      "step 5604 loss: 2.4962267875671387\n",
      "step 5605 loss: 2.567582130432129\n",
      "step 5606 loss: 2.4775795936584473\n",
      "step 5607 loss: 2.5819530487060547\n",
      "step 5608 loss: 2.6099026203155518\n",
      "step 5609 loss: 2.5020365715026855\n",
      "step 5610 loss: 2.444875717163086\n",
      "step 5611 loss: 2.411142587661743\n",
      "step 5612 loss: 2.513244867324829\n",
      "step 5613 loss: 2.4484989643096924\n",
      "step 5614 loss: 2.4975132942199707\n",
      "step 5615 loss: 2.544541597366333\n",
      "step 5616 loss: 2.511906862258911\n",
      "step 5617 loss: 2.6158406734466553\n",
      "step 5618 loss: 2.4833779335021973\n",
      "step 5619 loss: 2.435405969619751\n",
      "step 5620 loss: 2.555536985397339\n",
      "step 5621 loss: 2.4525399208068848\n",
      "step 5622 loss: 2.635439157485962\n",
      "step 5623 loss: 2.4998841285705566\n",
      "step 5624 loss: 2.5414960384368896\n",
      "step 5625 loss: 2.450291633605957\n",
      "step 5626 loss: 2.5555880069732666\n",
      "step 5627 loss: 2.600405216217041\n",
      "step 5628 loss: 2.4899890422821045\n",
      "step 5629 loss: 2.507463216781616\n",
      "step 5630 loss: 2.565524101257324\n",
      "step 5631 loss: 2.616245746612549\n",
      "step 5632 loss: 2.508692502975464\n",
      "step 5633 loss: 2.684272050857544\n",
      "step 5634 loss: 2.5951526165008545\n",
      "step 5635 loss: 2.557279586791992\n",
      "step 5636 loss: 2.4440646171569824\n",
      "step 5637 loss: 2.5615530014038086\n",
      "step 5638 loss: 2.539574146270752\n",
      "step 5639 loss: 2.469503164291382\n",
      "step 5640 loss: 2.583531379699707\n",
      "step 5641 loss: 2.5260019302368164\n",
      "step 5642 loss: 2.575453996658325\n",
      "step 5643 loss: 2.5849502086639404\n",
      "step 5644 loss: 2.528874635696411\n",
      "step 5645 loss: 2.444594383239746\n",
      "step 5646 loss: 2.6942801475524902\n",
      "step 5647 loss: 2.520146131515503\n",
      "step 5648 loss: 2.404700517654419\n",
      "step 5649 loss: 2.4709391593933105\n",
      "step 5650 loss: 2.49530291557312\n",
      "step 5651 loss: 2.554751396179199\n",
      "step 5652 loss: 2.4890854358673096\n",
      "step 5653 loss: 2.576083183288574\n",
      "step 5654 loss: 2.5607352256774902\n",
      "step 5655 loss: 2.5189666748046875\n",
      "step 5656 loss: 2.458739757537842\n",
      "step 5657 loss: 2.5135555267333984\n",
      "step 5658 loss: 2.365112066268921\n",
      "step 5659 loss: 2.47634220123291\n",
      "step 5660 loss: 2.512753486633301\n",
      "step 5661 loss: 2.4717166423797607\n",
      "step 5662 loss: 2.4672508239746094\n",
      "step 5663 loss: 2.4148902893066406\n",
      "step 5664 loss: 2.514896869659424\n",
      "step 5665 loss: 2.521144151687622\n",
      "step 5666 loss: 2.5546867847442627\n",
      "step 5667 loss: 2.673478126525879\n",
      "step 5668 loss: 2.575486898422241\n",
      "step 5669 loss: 2.574958324432373\n",
      "step 5670 loss: 2.4999196529388428\n",
      "step 5671 loss: 2.563716173171997\n",
      "step 5672 loss: 2.55558180809021\n",
      "step 5673 loss: 2.592287302017212\n",
      "step 5674 loss: 2.544597625732422\n",
      "step 5675 loss: 2.5046191215515137\n",
      "step 5676 loss: 2.6802499294281006\n",
      "step 5677 loss: 2.6158924102783203\n",
      "step 5678 loss: 2.379093647003174\n",
      "step 5679 loss: 2.525271415710449\n",
      "step 5680 loss: 2.4007880687713623\n",
      "step 5681 loss: 2.518592119216919\n",
      "step 5682 loss: 2.523369789123535\n",
      "step 5683 loss: 2.314037561416626\n",
      "step 5684 loss: 2.583832263946533\n",
      "step 5685 loss: 2.425292491912842\n",
      "step 5686 loss: 2.6317169666290283\n",
      "step 5687 loss: 2.6076931953430176\n",
      "step 5688 loss: 2.407705783843994\n",
      "step 5689 loss: 2.4528913497924805\n",
      "step 5690 loss: 2.538907527923584\n",
      "step 5691 loss: 2.509553909301758\n",
      "step 5692 loss: 2.497328281402588\n",
      "step 5693 loss: 2.623136043548584\n",
      "step 5694 loss: 2.4658448696136475\n",
      "step 5695 loss: 2.5481326580047607\n",
      "step 5696 loss: 2.431957483291626\n",
      "step 5697 loss: 2.5225770473480225\n",
      "step 5698 loss: 2.486145257949829\n",
      "step 5699 loss: 2.5094215869903564\n",
      "step 5700 loss: 2.5358901023864746\n",
      "step 5701 loss: 2.5591769218444824\n",
      "step 5702 loss: 2.5091464519500732\n",
      "step 5703 loss: 2.5856902599334717\n",
      "step 5704 loss: 2.478715658187866\n",
      "step 5705 loss: 2.525587797164917\n",
      "step 5706 loss: 2.5065104961395264\n",
      "step 5707 loss: 2.5593011379241943\n",
      "step 5708 loss: 2.4803013801574707\n",
      "step 5709 loss: 2.5253703594207764\n",
      "step 5710 loss: 2.56357741355896\n",
      "step 5711 loss: 2.402783155441284\n",
      "step 5712 loss: 2.5802557468414307\n",
      "step 5713 loss: 2.523869514465332\n",
      "step 5714 loss: 2.5836710929870605\n",
      "step 5715 loss: 2.436920404434204\n",
      "step 5716 loss: 2.409947395324707\n",
      "step 5717 loss: 2.572488784790039\n",
      "step 5718 loss: 2.6089518070220947\n",
      "step 5719 loss: 2.5899477005004883\n",
      "step 5720 loss: 2.5814878940582275\n",
      "step 5721 loss: 2.4978487491607666\n",
      "step 5722 loss: 2.5067152976989746\n",
      "step 5723 loss: 2.6023380756378174\n",
      "step 5724 loss: 2.533599615097046\n",
      "step 5725 loss: 2.533398389816284\n",
      "step 5726 loss: 2.4594430923461914\n",
      "step 5727 loss: 2.6071994304656982\n",
      "step 5728 loss: 2.4937524795532227\n",
      "step 5729 loss: 2.6598353385925293\n",
      "step 5730 loss: 2.4281554222106934\n",
      "step 5731 loss: 2.580105781555176\n",
      "step 5732 loss: 2.553168773651123\n",
      "step 5733 loss: 2.4926748275756836\n",
      "step 5734 loss: 2.5348732471466064\n",
      "step 5735 loss: 2.503955841064453\n",
      "step 5736 loss: 2.6297521591186523\n",
      "step 5737 loss: 2.5363428592681885\n",
      "step 5738 loss: 2.5876893997192383\n",
      "step 5739 loss: 2.4871749877929688\n",
      "step 5740 loss: 2.5154294967651367\n",
      "step 5741 loss: 2.6226589679718018\n",
      "step 5742 loss: 2.582073926925659\n",
      "step 5743 loss: 2.4952313899993896\n",
      "step 5744 loss: 2.4742887020111084\n",
      "step 5745 loss: 2.5745890140533447\n",
      "step 5746 loss: 2.474491834640503\n",
      "step 5747 loss: 2.423521041870117\n",
      "step 5748 loss: 2.48541259765625\n",
      "step 5749 loss: 2.5687472820281982\n",
      "step 5750 loss: 2.596815586090088\n",
      "step 5751 loss: 2.508707046508789\n",
      "step 5752 loss: 2.6219348907470703\n",
      "step 5753 loss: 2.492018461227417\n",
      "step 5754 loss: 2.4735405445098877\n",
      "step 5755 loss: 2.4829752445220947\n",
      "step 5756 loss: 2.563880681991577\n",
      "step 5757 loss: 2.5364370346069336\n",
      "step 5758 loss: 2.5075602531433105\n",
      "step 5759 loss: 2.4443557262420654\n",
      "step 5760 loss: 2.42046856880188\n",
      "step 5761 loss: 2.5456409454345703\n",
      "step 5762 loss: 2.433802366256714\n",
      "step 5763 loss: 2.4661154747009277\n",
      "step 5764 loss: 2.5717742443084717\n",
      "step 5765 loss: 2.500375747680664\n",
      "step 5766 loss: 2.5858592987060547\n",
      "step 5767 loss: 2.5375962257385254\n",
      "step 5768 loss: 2.4652974605560303\n",
      "step 5769 loss: 2.4869306087493896\n",
      "step 5770 loss: 2.618274211883545\n",
      "step 5771 loss: 2.5873827934265137\n",
      "step 5772 loss: 2.5272340774536133\n",
      "step 5773 loss: 2.641477346420288\n",
      "step 5774 loss: 2.6091809272766113\n",
      "step 5775 loss: 2.489079236984253\n",
      "step 5776 loss: 2.6111788749694824\n",
      "step 5777 loss: 2.58843731880188\n",
      "step 5778 loss: 2.4577999114990234\n",
      "step 5779 loss: 2.496115207672119\n",
      "step 5780 loss: 2.5464839935302734\n",
      "step 5781 loss: 2.420240640640259\n",
      "step 5782 loss: 2.350257158279419\n",
      "step 5783 loss: 2.427131414413452\n",
      "step 5784 loss: 2.6001853942871094\n",
      "step 5785 loss: 2.4667482376098633\n",
      "step 5786 loss: 2.441288471221924\n",
      "step 5787 loss: 2.495059013366699\n",
      "step 5788 loss: 2.4003491401672363\n",
      "step 5789 loss: 2.5273756980895996\n",
      "step 5790 loss: 2.4041531085968018\n",
      "step 5791 loss: 2.503816843032837\n",
      "step 5792 loss: 2.4249649047851562\n",
      "step 5793 loss: 2.5555310249328613\n",
      "step 5794 loss: 2.4999029636383057\n",
      "step 5795 loss: 2.5353147983551025\n",
      "step 5796 loss: 2.563231945037842\n",
      "step 5797 loss: 2.4027099609375\n",
      "step 5798 loss: 2.524442434310913\n",
      "step 5799 loss: 2.4332737922668457\n",
      "step 5800 loss: 2.5115206241607666\n",
      "step 5801 loss: 2.553220272064209\n",
      "step 5802 loss: 2.551931142807007\n",
      "step 5803 loss: 2.5615005493164062\n",
      "step 5804 loss: 2.5060200691223145\n",
      "step 5805 loss: 2.362806558609009\n",
      "step 5806 loss: 2.5415055751800537\n",
      "step 5807 loss: 2.4973418712615967\n",
      "step 5808 loss: 2.5291361808776855\n",
      "step 5809 loss: 2.471358060836792\n",
      "step 5810 loss: 2.3859519958496094\n",
      "step 5811 loss: 2.554710865020752\n",
      "step 5812 loss: 2.388137102127075\n",
      "step 5813 loss: 2.4841177463531494\n",
      "step 5814 loss: 2.5993690490722656\n",
      "step 5815 loss: 2.408684492111206\n",
      "step 5816 loss: 2.4934158325195312\n",
      "step 5817 loss: 2.476212501525879\n",
      "step 5818 loss: 2.632399320602417\n",
      "step 5819 loss: 2.5048704147338867\n",
      "step 5820 loss: 2.484269857406616\n",
      "step 5821 loss: 2.5075366497039795\n",
      "step 5822 loss: 2.494636058807373\n",
      "step 5823 loss: 2.5358941555023193\n",
      "step 5824 loss: 2.6061463356018066\n",
      "step 5825 loss: 2.4666450023651123\n",
      "step 5826 loss: 2.383615255355835\n",
      "step 5827 loss: 2.5282466411590576\n",
      "step 5828 loss: 2.458664894104004\n",
      "step 5829 loss: 2.557116746902466\n",
      "step 5830 loss: 2.507018804550171\n",
      "step 5831 loss: 2.6037819385528564\n",
      "step 5832 loss: 2.417249917984009\n",
      "step 5833 loss: 2.4651060104370117\n",
      "step 5834 loss: 2.5488600730895996\n",
      "step 5835 loss: 2.4544360637664795\n",
      "step 5836 loss: 2.441525459289551\n",
      "step 5837 loss: 2.394024610519409\n",
      "step 5838 loss: 2.586778402328491\n",
      "step 5839 loss: 2.531433582305908\n",
      "step 5840 loss: 2.6294033527374268\n",
      "step 5841 loss: 2.483769416809082\n",
      "step 5842 loss: 2.57212495803833\n",
      "step 5843 loss: 2.5019876956939697\n",
      "step 5844 loss: 2.522676706314087\n",
      "step 5845 loss: 2.6399309635162354\n",
      "step 5846 loss: 2.4781417846679688\n",
      "step 5847 loss: 2.6267595291137695\n",
      "step 5848 loss: 2.4602556228637695\n",
      "step 5849 loss: 2.5883045196533203\n",
      "step 5850 loss: 2.4473023414611816\n",
      "step 5851 loss: 2.5242607593536377\n",
      "step 5852 loss: 2.444283962249756\n",
      "step 5853 loss: 2.529008626937866\n",
      "step 5854 loss: 2.7422425746917725\n",
      "step 5855 loss: 2.5832245349884033\n",
      "step 5856 loss: 2.4410171508789062\n",
      "step 5857 loss: 2.608792304992676\n",
      "step 5858 loss: 2.4275317192077637\n",
      "step 5859 loss: 2.5063860416412354\n",
      "step 5860 loss: 2.481088638305664\n",
      "step 5861 loss: 2.445183038711548\n",
      "step 5862 loss: 2.5148775577545166\n",
      "step 5863 loss: 2.497274875640869\n",
      "step 5864 loss: 2.5118117332458496\n",
      "step 5865 loss: 2.596134901046753\n",
      "step 5866 loss: 2.4805593490600586\n",
      "step 5867 loss: 2.5339314937591553\n",
      "step 5868 loss: 2.448678493499756\n",
      "step 5869 loss: 2.4138402938842773\n",
      "step 5870 loss: 2.569666862487793\n",
      "step 5871 loss: 2.639859199523926\n",
      "step 5872 loss: 2.518024444580078\n",
      "step 5873 loss: 2.4222850799560547\n",
      "step 5874 loss: 2.5078561305999756\n",
      "step 5875 loss: 2.487363576889038\n",
      "step 5876 loss: 2.5199594497680664\n",
      "step 5877 loss: 2.604872941970825\n",
      "step 5878 loss: 2.416435956954956\n",
      "step 5879 loss: 2.540370225906372\n",
      "step 5880 loss: 2.5835721492767334\n",
      "step 5881 loss: 2.563666343688965\n",
      "step 5882 loss: 2.4722554683685303\n",
      "step 5883 loss: 2.5108625888824463\n",
      "step 5884 loss: 2.544717311859131\n",
      "step 5885 loss: 2.4947662353515625\n",
      "step 5886 loss: 2.451663017272949\n",
      "step 5887 loss: 2.470398426055908\n",
      "step 5888 loss: 2.442322015762329\n",
      "step 5889 loss: 2.4452030658721924\n",
      "step 5890 loss: 2.3613481521606445\n",
      "step 5891 loss: 2.6210389137268066\n",
      "step 5892 loss: 2.351738452911377\n",
      "step 5893 loss: 2.646247148513794\n",
      "step 5894 loss: 2.4494316577911377\n",
      "step 5895 loss: 2.486420154571533\n",
      "step 5896 loss: 2.5827527046203613\n",
      "step 5897 loss: 2.475023031234741\n",
      "step 5898 loss: 2.5633130073547363\n",
      "step 5899 loss: 2.555173873901367\n",
      "step 5900 loss: 2.5397989749908447\n",
      "step 5901 loss: 2.573719024658203\n",
      "step 5902 loss: 2.657355308532715\n",
      "step 5903 loss: 2.529003381729126\n",
      "step 5904 loss: 2.5731263160705566\n",
      "step 5905 loss: 2.512573719024658\n",
      "step 5906 loss: 2.481461524963379\n",
      "step 5907 loss: 2.5731778144836426\n",
      "step 5908 loss: 2.569413661956787\n",
      "step 5909 loss: 2.536378860473633\n",
      "step 5910 loss: 2.528465509414673\n",
      "step 5911 loss: 2.5037033557891846\n",
      "step 5912 loss: 2.472465991973877\n",
      "step 5913 loss: 2.4909377098083496\n",
      "step 5914 loss: 2.529175281524658\n",
      "step 5915 loss: 2.626946210861206\n",
      "step 5916 loss: 2.573621988296509\n",
      "step 5917 loss: 2.506028175354004\n",
      "step 5918 loss: 2.587589979171753\n",
      "step 5919 loss: 2.569059371948242\n",
      "step 5920 loss: 2.5439579486846924\n",
      "step 5921 loss: 2.452422857284546\n",
      "step 5922 loss: 2.454779863357544\n",
      "step 5923 loss: 2.6482741832733154\n",
      "step 5924 loss: 2.554298162460327\n",
      "step 5925 loss: 2.5084502696990967\n",
      "step 5926 loss: 2.511368751525879\n",
      "step 5927 loss: 2.4014780521392822\n",
      "step 5928 loss: 2.470454216003418\n",
      "step 5929 loss: 2.5136969089508057\n",
      "step 5930 loss: 2.644826889038086\n",
      "step 5931 loss: 2.5837607383728027\n",
      "step 5932 loss: 2.539679527282715\n",
      "step 5933 loss: 2.5844104290008545\n",
      "step 5934 loss: 2.4865498542785645\n",
      "step 5935 loss: 2.629927158355713\n",
      "step 5936 loss: 2.568491220474243\n",
      "step 5937 loss: 2.4985721111297607\n",
      "step 5938 loss: 2.587162733078003\n",
      "step 5939 loss: 2.5185015201568604\n",
      "step 5940 loss: 2.3980653285980225\n",
      "step 5941 loss: 2.636092185974121\n",
      "step 5942 loss: 2.528271436691284\n",
      "step 5943 loss: 2.5121264457702637\n",
      "step 5944 loss: 2.4402546882629395\n",
      "step 5945 loss: 2.4921274185180664\n",
      "step 5946 loss: 2.5120022296905518\n",
      "step 5947 loss: 2.587003469467163\n",
      "step 5948 loss: 2.598554849624634\n",
      "step 5949 loss: 2.4638898372650146\n",
      "step 5950 loss: 2.5231828689575195\n",
      "step 5951 loss: 2.6374642848968506\n",
      "step 5952 loss: 2.50012469291687\n",
      "step 5953 loss: 2.4966237545013428\n",
      "step 5954 loss: 2.455172300338745\n",
      "step 5955 loss: 2.514031410217285\n",
      "step 5956 loss: 2.595294713973999\n",
      "step 5957 loss: 2.5699045658111572\n",
      "step 5958 loss: 2.5654149055480957\n",
      "step 5959 loss: 2.5205442905426025\n",
      "step 5960 loss: 2.6228814125061035\n",
      "step 5961 loss: 2.563596487045288\n",
      "step 5962 loss: 2.620922565460205\n",
      "step 5963 loss: 2.413381814956665\n",
      "step 5964 loss: 2.498748779296875\n",
      "step 5965 loss: 2.5948526859283447\n",
      "step 5966 loss: 2.5358340740203857\n",
      "step 5967 loss: 2.538302421569824\n",
      "step 5968 loss: 2.514030933380127\n",
      "step 5969 loss: 2.7208993434906006\n",
      "step 5970 loss: 2.502934217453003\n",
      "step 5971 loss: 2.491044521331787\n",
      "step 5972 loss: 2.51157808303833\n",
      "step 5973 loss: 2.526641368865967\n",
      "step 5974 loss: 2.5312798023223877\n",
      "step 5975 loss: 2.4593324661254883\n",
      "step 5976 loss: 2.5470595359802246\n",
      "step 5977 loss: 2.5791852474212646\n",
      "step 5978 loss: 2.6101927757263184\n",
      "step 5979 loss: 2.4868404865264893\n",
      "step 5980 loss: 2.4484291076660156\n",
      "step 5981 loss: 2.6117451190948486\n",
      "step 5982 loss: 2.6546242237091064\n",
      "step 5983 loss: 2.4436497688293457\n",
      "step 5984 loss: 2.5455052852630615\n",
      "step 5985 loss: 2.445361614227295\n",
      "step 5986 loss: 2.5229110717773438\n",
      "step 5987 loss: 2.509678363800049\n",
      "step 5988 loss: 2.4890410900115967\n",
      "step 5989 loss: 2.654905319213867\n",
      "step 5990 loss: 2.643584966659546\n",
      "step 5991 loss: 2.43272066116333\n",
      "step 5992 loss: 2.586228609085083\n",
      "step 5993 loss: 2.5682551860809326\n",
      "step 5994 loss: 2.4432125091552734\n",
      "step 5995 loss: 2.460948944091797\n",
      "step 5996 loss: 2.620072841644287\n",
      "step 5997 loss: 2.521291494369507\n",
      "step 5998 loss: 2.4789481163024902\n",
      "step 5999 loss: 2.6574454307556152\n",
      "step 6000 loss: 2.4889943599700928\n",
      "step 6001 loss: 2.4340763092041016\n",
      "step 6002 loss: 2.466341257095337\n",
      "step 6003 loss: 2.7236180305480957\n",
      "step 6004 loss: 2.5183932781219482\n",
      "step 6005 loss: 2.5455784797668457\n",
      "step 6006 loss: 2.500025749206543\n",
      "step 6007 loss: 2.5297000408172607\n",
      "step 6008 loss: 2.528557538986206\n",
      "step 6009 loss: 2.5562851428985596\n",
      "step 6010 loss: 2.53141713142395\n",
      "step 6011 loss: 2.5062942504882812\n",
      "step 6012 loss: 2.5020782947540283\n",
      "step 6013 loss: 2.4814705848693848\n",
      "step 6014 loss: 2.5016520023345947\n",
      "step 6015 loss: 2.517937183380127\n",
      "step 6016 loss: 2.6455228328704834\n",
      "step 6017 loss: 2.5015299320220947\n",
      "step 6018 loss: 2.56152081489563\n",
      "step 6019 loss: 2.5212717056274414\n",
      "step 6020 loss: 2.4384560585021973\n",
      "step 6021 loss: 2.5005927085876465\n",
      "step 6022 loss: 2.5614054203033447\n",
      "step 6023 loss: 2.4350900650024414\n",
      "step 6024 loss: 2.5972607135772705\n",
      "step 6025 loss: 2.4785916805267334\n",
      "step 6026 loss: 2.500349998474121\n",
      "step 6027 loss: 2.4180142879486084\n",
      "step 6028 loss: 2.5589823722839355\n",
      "step 6029 loss: 2.5225038528442383\n",
      "step 6030 loss: 2.533292531967163\n",
      "step 6031 loss: 2.530951738357544\n",
      "step 6032 loss: 2.584599256515503\n",
      "step 6033 loss: 2.5843253135681152\n",
      "step 6034 loss: 2.5429208278656006\n",
      "step 6035 loss: 2.4606566429138184\n",
      "step 6036 loss: 2.5568885803222656\n",
      "step 6037 loss: 2.5335962772369385\n",
      "step 6038 loss: 2.5456268787384033\n",
      "step 6039 loss: 2.5494625568389893\n",
      "step 6040 loss: 2.587604284286499\n",
      "step 6041 loss: 2.512326955795288\n",
      "step 6042 loss: 2.510371685028076\n",
      "step 6043 loss: 2.551992654800415\n",
      "step 6044 loss: 2.4314396381378174\n",
      "step 6045 loss: 2.445117950439453\n",
      "step 6046 loss: 2.5069265365600586\n",
      "step 6047 loss: 2.511827230453491\n",
      "step 6048 loss: 2.56541109085083\n",
      "step 6049 loss: 2.5175249576568604\n",
      "step 6050 loss: 2.5336337089538574\n",
      "step 6051 loss: 2.4581215381622314\n",
      "step 6052 loss: 2.5215368270874023\n",
      "step 6053 loss: 2.510855197906494\n",
      "step 6054 loss: 2.418192148208618\n",
      "step 6055 loss: 2.4099602699279785\n",
      "step 6056 loss: 2.5855259895324707\n",
      "step 6057 loss: 2.6710269451141357\n",
      "step 6058 loss: 2.4915947914123535\n",
      "step 6059 loss: 2.5624215602874756\n",
      "step 6060 loss: 2.5548105239868164\n",
      "step 6061 loss: 2.483393907546997\n",
      "step 6062 loss: 2.4884302616119385\n",
      "step 6063 loss: 2.422914981842041\n",
      "step 6064 loss: 2.3886961936950684\n",
      "step 6065 loss: 2.421938419342041\n",
      "step 6066 loss: 2.5642569065093994\n",
      "step 6067 loss: 2.5412533283233643\n",
      "step 6068 loss: 2.522686243057251\n",
      "step 6069 loss: 2.4987633228302\n",
      "step 6070 loss: 2.5385353565216064\n",
      "step 6071 loss: 2.489445686340332\n",
      "step 6072 loss: 2.5751121044158936\n",
      "step 6073 loss: 2.4846878051757812\n",
      "step 6074 loss: 2.4911611080169678\n",
      "step 6075 loss: 2.641326904296875\n",
      "step 6076 loss: 2.507917881011963\n",
      "step 6077 loss: 2.512089729309082\n",
      "step 6078 loss: 2.4579806327819824\n",
      "step 6079 loss: 2.6339967250823975\n",
      "step 6080 loss: 2.3468611240386963\n",
      "step 6081 loss: 2.587322950363159\n",
      "step 6082 loss: 2.4591495990753174\n",
      "step 6083 loss: 2.6033995151519775\n",
      "step 6084 loss: 2.6269867420196533\n",
      "step 6085 loss: 2.461054801940918\n",
      "step 6086 loss: 2.440171718597412\n",
      "step 6087 loss: 2.562129020690918\n",
      "step 6088 loss: 2.492833375930786\n",
      "step 6089 loss: 2.4845833778381348\n",
      "step 6090 loss: 2.4307076930999756\n",
      "step 6091 loss: 2.6490628719329834\n",
      "step 6092 loss: 2.499572277069092\n",
      "step 6093 loss: 2.51204776763916\n",
      "step 6094 loss: 2.4337804317474365\n",
      "step 6095 loss: 2.516697645187378\n",
      "step 6096 loss: 2.5429701805114746\n",
      "step 6097 loss: 2.4585447311401367\n",
      "step 6098 loss: 2.6789519786834717\n",
      "step 6099 loss: 2.4331276416778564\n",
      "step 6100 loss: 2.5222768783569336\n",
      "step 6101 loss: 2.5473926067352295\n",
      "step 6102 loss: 2.5983314514160156\n",
      "step 6103 loss: 2.517059803009033\n",
      "step 6104 loss: 2.5552761554718018\n",
      "step 6105 loss: 2.5241787433624268\n",
      "step 6106 loss: 2.4677951335906982\n",
      "step 6107 loss: 2.528367280960083\n",
      "step 6108 loss: 2.445929527282715\n",
      "step 6109 loss: 2.4959464073181152\n",
      "step 6110 loss: 2.492920160293579\n",
      "step 6111 loss: 2.4783287048339844\n",
      "step 6112 loss: 2.479975461959839\n",
      "step 6113 loss: 2.44356369972229\n",
      "step 6114 loss: 2.4947919845581055\n",
      "step 6115 loss: 2.5147647857666016\n",
      "step 6116 loss: 2.4776806831359863\n",
      "step 6117 loss: 2.5447182655334473\n",
      "step 6118 loss: 2.4173007011413574\n",
      "step 6119 loss: 2.4676449298858643\n",
      "step 6120 loss: 2.5287678241729736\n",
      "step 6121 loss: 2.5221362113952637\n",
      "step 6122 loss: 2.5537948608398438\n",
      "step 6123 loss: 2.5303828716278076\n",
      "step 6124 loss: 2.5557596683502197\n",
      "step 6125 loss: 2.446882963180542\n",
      "step 6126 loss: 2.4917726516723633\n",
      "step 6127 loss: 2.5818917751312256\n",
      "step 6128 loss: 2.44631290435791\n",
      "step 6129 loss: 2.4879496097564697\n",
      "step 6130 loss: 2.592478036880493\n",
      "step 6131 loss: 2.4368913173675537\n",
      "step 6132 loss: 2.3805367946624756\n",
      "step 6133 loss: 2.4679129123687744\n",
      "step 6134 loss: 2.444683790206909\n",
      "step 6135 loss: 2.505841016769409\n",
      "step 6136 loss: 2.3967347145080566\n",
      "step 6137 loss: 2.440978765487671\n",
      "step 6138 loss: 2.4639790058135986\n",
      "step 6139 loss: 2.618922710418701\n",
      "step 6140 loss: 2.471104145050049\n",
      "step 6141 loss: 2.5013225078582764\n",
      "step 6142 loss: 2.4884140491485596\n",
      "step 6143 loss: 2.4377851486206055\n",
      "step 6144 loss: 2.5841622352600098\n",
      "step 6145 loss: 2.6735613346099854\n",
      "step 6146 loss: 2.4770076274871826\n",
      "step 6147 loss: 2.5240111351013184\n",
      "step 6148 loss: 2.562688112258911\n",
      "step 6149 loss: 2.395402193069458\n",
      "step 6150 loss: 2.547297716140747\n",
      "step 6151 loss: 2.49649715423584\n",
      "step 6152 loss: 2.6076834201812744\n",
      "step 6153 loss: 2.588634490966797\n",
      "step 6154 loss: 2.482307195663452\n",
      "step 6155 loss: 2.5032358169555664\n",
      "step 6156 loss: 2.4560136795043945\n",
      "step 6157 loss: 2.528762102127075\n",
      "step 6158 loss: 2.541567802429199\n",
      "step 6159 loss: 2.520369529724121\n",
      "step 6160 loss: 2.4955029487609863\n",
      "step 6161 loss: 2.4353363513946533\n",
      "step 6162 loss: 2.4290010929107666\n",
      "step 6163 loss: 2.4747331142425537\n",
      "step 6164 loss: 2.5455920696258545\n",
      "step 6165 loss: 2.444885492324829\n",
      "step 6166 loss: 2.4518635272979736\n",
      "step 6167 loss: 2.425340175628662\n",
      "step 6168 loss: 2.6128454208374023\n",
      "step 6169 loss: 2.492615222930908\n",
      "step 6170 loss: 2.448896646499634\n",
      "step 6171 loss: 2.4605743885040283\n",
      "step 6172 loss: 2.330232620239258\n",
      "step 6173 loss: 2.5268354415893555\n",
      "step 6174 loss: 2.4889168739318848\n",
      "step 6175 loss: 2.5132460594177246\n",
      "step 6176 loss: 2.4221160411834717\n",
      "step 6177 loss: 2.4484431743621826\n",
      "step 6178 loss: 2.571335554122925\n",
      "step 6179 loss: 2.4197049140930176\n",
      "step 6180 loss: 2.5283498764038086\n",
      "step 6181 loss: 2.424804925918579\n",
      "step 6182 loss: 2.460829019546509\n",
      "step 6183 loss: 2.543177843093872\n",
      "step 6184 loss: 2.4644737243652344\n",
      "step 6185 loss: 2.603790760040283\n",
      "step 6186 loss: 2.4567980766296387\n",
      "step 6187 loss: 2.5288639068603516\n",
      "step 6188 loss: 2.588785409927368\n",
      "step 6189 loss: 2.50447940826416\n",
      "step 6190 loss: 2.429945945739746\n",
      "step 6191 loss: 2.591292142868042\n",
      "step 6192 loss: 2.381376028060913\n",
      "step 6193 loss: 2.409803628921509\n",
      "step 6194 loss: 2.4722113609313965\n",
      "step 6195 loss: 2.45796275138855\n",
      "step 6196 loss: 2.5036609172821045\n",
      "step 6197 loss: 2.561464548110962\n",
      "step 6198 loss: 2.4823129177093506\n",
      "step 6199 loss: 2.5738167762756348\n",
      "step 6200 loss: 2.5372188091278076\n",
      "step 6201 loss: 2.4901082515716553\n",
      "step 6202 loss: 2.556627035140991\n",
      "step 6203 loss: 2.5524098873138428\n",
      "step 6204 loss: 2.5239360332489014\n",
      "step 6205 loss: 2.4209282398223877\n",
      "step 6206 loss: 2.584019422531128\n",
      "step 6207 loss: 2.5241215229034424\n",
      "step 6208 loss: 2.499159336090088\n",
      "step 6209 loss: 2.5917551517486572\n",
      "step 6210 loss: 2.464411497116089\n",
      "step 6211 loss: 2.5604350566864014\n",
      "step 6212 loss: 2.4971067905426025\n",
      "step 6213 loss: 2.707974433898926\n",
      "step 6214 loss: 2.468653678894043\n",
      "step 6215 loss: 2.4657654762268066\n",
      "step 6216 loss: 2.5239756107330322\n",
      "step 6217 loss: 2.487166404724121\n",
      "step 6218 loss: 2.482254981994629\n",
      "step 6219 loss: 2.5586740970611572\n",
      "step 6220 loss: 2.432788133621216\n",
      "step 6221 loss: 2.3398053646087646\n",
      "step 6222 loss: 2.5207884311676025\n",
      "step 6223 loss: 2.440204620361328\n",
      "step 6224 loss: 2.4311468601226807\n",
      "step 6225 loss: 2.566105365753174\n",
      "step 6226 loss: 2.4467613697052\n",
      "step 6227 loss: 2.5481724739074707\n",
      "step 6228 loss: 2.664353370666504\n",
      "step 6229 loss: 2.5107498168945312\n",
      "step 6230 loss: 2.59855318069458\n",
      "step 6231 loss: 2.5025439262390137\n",
      "step 6232 loss: 2.49151349067688\n",
      "step 6233 loss: 2.4451138973236084\n",
      "step 6234 loss: 2.522833824157715\n",
      "step 6235 loss: 2.437112808227539\n",
      "step 6236 loss: 2.3829505443573\n",
      "step 6237 loss: 2.4464969635009766\n",
      "step 6238 loss: 2.4034507274627686\n",
      "step 6239 loss: 2.4622573852539062\n",
      "step 6240 loss: 2.5026931762695312\n",
      "step 6241 loss: 2.5838897228240967\n",
      "step 6242 loss: 2.5153961181640625\n",
      "step 6243 loss: 2.631765365600586\n",
      "step 6244 loss: 2.5984768867492676\n",
      "step 6245 loss: 2.539644241333008\n",
      "step 6246 loss: 2.45637583732605\n",
      "step 6247 loss: 2.482766628265381\n",
      "step 6248 loss: 2.5627129077911377\n",
      "step 6249 loss: 2.408834457397461\n",
      "step 6250 loss: 2.388582944869995\n",
      "step 6251 loss: 2.434124231338501\n",
      "step 6252 loss: 2.588097333908081\n",
      "step 6253 loss: 2.5375773906707764\n",
      "step 6254 loss: 2.4048984050750732\n",
      "step 6255 loss: 2.532665491104126\n",
      "step 6256 loss: 2.529693603515625\n",
      "step 6257 loss: 2.551161527633667\n",
      "step 6258 loss: 2.595033884048462\n",
      "step 6259 loss: 2.5950803756713867\n",
      "step 6260 loss: 2.4985365867614746\n",
      "step 6261 loss: 2.3383595943450928\n",
      "step 6262 loss: 2.526066541671753\n",
      "step 6263 loss: 2.4985690116882324\n",
      "step 6264 loss: 2.531358242034912\n",
      "step 6265 loss: 2.471496820449829\n",
      "step 6266 loss: 2.575629949569702\n",
      "step 6267 loss: 2.405613422393799\n",
      "step 6268 loss: 2.498875379562378\n",
      "step 6269 loss: 2.4948055744171143\n",
      "step 6270 loss: 2.4314913749694824\n",
      "step 6271 loss: 2.505035400390625\n",
      "step 6272 loss: 2.6110074520111084\n",
      "step 6273 loss: 2.6154332160949707\n",
      "step 6274 loss: 2.479504108428955\n",
      "step 6275 loss: 2.5866940021514893\n",
      "step 6276 loss: 2.6482574939727783\n",
      "step 6277 loss: 2.442578077316284\n",
      "step 6278 loss: 2.462918758392334\n",
      "step 6279 loss: 2.5103566646575928\n",
      "step 6280 loss: 2.623692512512207\n",
      "step 6281 loss: 2.4340426921844482\n",
      "step 6282 loss: 2.589705467224121\n",
      "step 6283 loss: 2.5726845264434814\n",
      "step 6284 loss: 2.5469508171081543\n",
      "step 6285 loss: 2.588550090789795\n",
      "step 6286 loss: 2.4356689453125\n",
      "step 6287 loss: 2.541741371154785\n",
      "step 6288 loss: 2.4372289180755615\n",
      "step 6289 loss: 2.5673668384552\n",
      "step 6290 loss: 2.478891134262085\n",
      "step 6291 loss: 2.3485569953918457\n",
      "step 6292 loss: 2.471015691757202\n",
      "step 6293 loss: 2.5831079483032227\n",
      "step 6294 loss: 2.5986826419830322\n",
      "step 6295 loss: 2.3910858631134033\n",
      "step 6296 loss: 2.5262451171875\n",
      "step 6297 loss: 2.4970815181732178\n",
      "step 6298 loss: 2.5561840534210205\n",
      "step 6299 loss: 2.490140199661255\n",
      "step 6300 loss: 2.5158963203430176\n",
      "step 6301 loss: 2.4414350986480713\n",
      "step 6302 loss: 2.46307373046875\n",
      "step 6303 loss: 2.395911693572998\n",
      "step 6304 loss: 2.5765154361724854\n",
      "step 6305 loss: 2.5380706787109375\n",
      "step 6306 loss: 2.457014322280884\n",
      "step 6307 loss: 2.5779190063476562\n",
      "step 6308 loss: 2.4694626331329346\n",
      "step 6309 loss: 2.5410664081573486\n",
      "step 6310 loss: 2.4884910583496094\n",
      "step 6311 loss: 2.498292922973633\n",
      "step 6312 loss: 2.5023481845855713\n",
      "step 6313 loss: 2.419468879699707\n",
      "step 6314 loss: 2.6044070720672607\n",
      "step 6315 loss: 2.713184356689453\n",
      "step 6316 loss: 2.5261967182159424\n",
      "step 6317 loss: 2.471278667449951\n",
      "step 6318 loss: 2.5122621059417725\n",
      "step 6319 loss: 2.4427618980407715\n",
      "step 6320 loss: 2.462913751602173\n",
      "step 6321 loss: 2.5583693981170654\n",
      "step 6322 loss: 2.5981428623199463\n",
      "step 6323 loss: 2.4359865188598633\n",
      "step 6324 loss: 2.4807586669921875\n",
      "step 6325 loss: 2.554163932800293\n",
      "step 6326 loss: 2.505713939666748\n",
      "step 6327 loss: 2.523102283477783\n",
      "step 6328 loss: 2.6092751026153564\n",
      "step 6329 loss: 2.5326719284057617\n",
      "step 6330 loss: 2.494844675064087\n",
      "step 6331 loss: 2.467618227005005\n",
      "step 6332 loss: 2.4303460121154785\n",
      "step 6333 loss: 2.555570602416992\n",
      "step 6334 loss: 2.3498947620391846\n",
      "step 6335 loss: 2.5543158054351807\n",
      "step 6336 loss: 2.5724267959594727\n",
      "step 6337 loss: 2.5540828704833984\n",
      "step 6338 loss: 2.4118573665618896\n",
      "step 6339 loss: 2.6034553050994873\n",
      "step 6340 loss: 2.4570987224578857\n",
      "step 6341 loss: 2.6922106742858887\n",
      "step 6342 loss: 2.702852487564087\n",
      "step 6343 loss: 2.6197497844696045\n",
      "step 6344 loss: 2.610590696334839\n",
      "step 6345 loss: 2.5319113731384277\n",
      "step 6346 loss: 2.563265085220337\n",
      "step 6347 loss: 2.4981143474578857\n",
      "step 6348 loss: 2.464540958404541\n",
      "step 6349 loss: 2.4278550148010254\n",
      "step 6350 loss: 2.582381010055542\n",
      "step 6351 loss: 2.4608683586120605\n",
      "step 6352 loss: 2.464484691619873\n",
      "step 6353 loss: 2.5141360759735107\n",
      "step 6354 loss: 2.550079584121704\n",
      "step 6355 loss: 2.4157044887542725\n",
      "step 6356 loss: 2.534353017807007\n",
      "step 6357 loss: 2.480222463607788\n",
      "step 6358 loss: 2.509288787841797\n",
      "step 6359 loss: 2.4084458351135254\n",
      "step 6360 loss: 2.568795680999756\n",
      "step 6361 loss: 2.4811859130859375\n",
      "step 6362 loss: 2.5693883895874023\n",
      "step 6363 loss: 2.5393755435943604\n",
      "step 6364 loss: 2.555199146270752\n",
      "step 6365 loss: 2.460131883621216\n",
      "step 6366 loss: 2.449159860610962\n",
      "step 6367 loss: 2.4471378326416016\n",
      "step 6368 loss: 2.386883020401001\n",
      "step 6369 loss: 2.5349438190460205\n",
      "step 6370 loss: 2.4267055988311768\n",
      "step 6371 loss: 2.449979305267334\n",
      "step 6372 loss: 2.4056780338287354\n",
      "step 6373 loss: 2.6737747192382812\n",
      "step 6374 loss: 2.526956796646118\n",
      "step 6375 loss: 2.511350154876709\n",
      "step 6376 loss: 2.4879868030548096\n",
      "step 6377 loss: 2.6949777603149414\n",
      "step 6378 loss: 2.5472161769866943\n",
      "step 6379 loss: 2.3090927600860596\n",
      "step 6380 loss: 2.605955123901367\n",
      "step 6381 loss: 2.5650582313537598\n",
      "step 6382 loss: 2.499920129776001\n",
      "step 6383 loss: 2.533086061477661\n",
      "step 6384 loss: 2.5389440059661865\n",
      "step 6385 loss: 2.5541484355926514\n",
      "step 6386 loss: 2.45434832572937\n",
      "step 6387 loss: 2.383164167404175\n",
      "step 6388 loss: 2.43213152885437\n",
      "step 6389 loss: 2.5448873043060303\n",
      "step 6390 loss: 2.4088034629821777\n",
      "step 6391 loss: 2.5625340938568115\n",
      "step 6392 loss: 2.5815138816833496\n",
      "step 6393 loss: 2.569819211959839\n",
      "step 6394 loss: 2.474392890930176\n",
      "step 6395 loss: 2.4563724994659424\n",
      "step 6396 loss: 2.559103488922119\n",
      "step 6397 loss: 2.6127190589904785\n",
      "step 6398 loss: 2.4807584285736084\n",
      "step 6399 loss: 2.5645625591278076\n",
      "step 6400 loss: 2.50382924079895\n",
      "step 6401 loss: 2.475637912750244\n",
      "step 6402 loss: 2.4659781455993652\n",
      "step 6403 loss: 2.458754062652588\n",
      "step 6404 loss: 2.5365872383117676\n",
      "step 6405 loss: 2.4704880714416504\n",
      "step 6406 loss: 2.4507761001586914\n",
      "step 6407 loss: 2.465972661972046\n",
      "step 6408 loss: 2.4832844734191895\n",
      "step 6409 loss: 2.4013009071350098\n",
      "step 6410 loss: 2.400887966156006\n",
      "step 6411 loss: 2.5146610736846924\n",
      "step 6412 loss: 2.5204572677612305\n",
      "step 6413 loss: 2.523362874984741\n",
      "step 6414 loss: 2.552781820297241\n",
      "step 6415 loss: 2.5562708377838135\n",
      "step 6416 loss: 2.4574391841888428\n",
      "step 6417 loss: 2.497455596923828\n",
      "step 6418 loss: 2.5101983547210693\n",
      "step 6419 loss: 2.523338794708252\n",
      "step 6420 loss: 2.449749708175659\n",
      "step 6421 loss: 2.5818498134613037\n",
      "step 6422 loss: 2.552598476409912\n",
      "step 6423 loss: 2.394205331802368\n",
      "step 6424 loss: 2.5104944705963135\n",
      "step 6425 loss: 2.466167449951172\n",
      "step 6426 loss: 2.6252167224884033\n",
      "step 6427 loss: 2.4191336631774902\n",
      "step 6428 loss: 2.580206871032715\n",
      "step 6429 loss: 2.3579294681549072\n",
      "step 6430 loss: 2.541783571243286\n",
      "step 6431 loss: 2.4077348709106445\n",
      "step 6432 loss: 2.5755677223205566\n",
      "step 6433 loss: 2.4327762126922607\n",
      "step 6434 loss: 2.5851008892059326\n",
      "step 6435 loss: 2.4661197662353516\n",
      "step 6436 loss: 2.5679821968078613\n",
      "step 6437 loss: 2.53899884223938\n",
      "step 6438 loss: 2.501727819442749\n",
      "step 6439 loss: 2.5457851886749268\n",
      "step 6440 loss: 2.5566258430480957\n",
      "step 6441 loss: 2.5779614448547363\n",
      "step 6442 loss: 2.458024263381958\n",
      "step 6443 loss: 2.541574478149414\n",
      "step 6444 loss: 2.63698410987854\n",
      "step 6445 loss: 2.525014638900757\n",
      "step 6446 loss: 2.4801530838012695\n",
      "step 6447 loss: 2.61700701713562\n",
      "step 6448 loss: 2.5383942127227783\n",
      "step 6449 loss: 2.593080759048462\n",
      "step 6450 loss: 2.495488405227661\n",
      "step 6451 loss: 2.465090036392212\n",
      "step 6452 loss: 2.451840400695801\n",
      "step 6453 loss: 2.3622236251831055\n",
      "step 6454 loss: 2.6088051795959473\n",
      "step 6455 loss: 2.372976064682007\n",
      "step 6456 loss: 2.528496026992798\n",
      "step 6457 loss: 2.4808969497680664\n",
      "step 6458 loss: 2.412053346633911\n",
      "step 6459 loss: 2.643812656402588\n",
      "step 6460 loss: 2.424020528793335\n",
      "step 6461 loss: 2.5334956645965576\n",
      "step 6462 loss: 2.5602283477783203\n",
      "step 6463 loss: 2.4771804809570312\n",
      "step 6464 loss: 2.4344642162323\n",
      "step 6465 loss: 2.5661470890045166\n",
      "step 6466 loss: 2.4906890392303467\n",
      "step 6467 loss: 2.4622373580932617\n",
      "step 6468 loss: 2.4890995025634766\n",
      "step 6469 loss: 2.7119452953338623\n",
      "step 6470 loss: 2.4939143657684326\n",
      "step 6471 loss: 2.5408315658569336\n",
      "step 6472 loss: 2.5568926334381104\n",
      "step 6473 loss: 2.550121784210205\n",
      "step 6474 loss: 2.6290547847747803\n",
      "step 6475 loss: 2.5348687171936035\n",
      "step 6476 loss: 2.566589593887329\n",
      "step 6477 loss: 2.360896587371826\n",
      "step 6478 loss: 2.547612428665161\n",
      "step 6479 loss: 2.524580240249634\n",
      "step 6480 loss: 2.558074712753296\n",
      "step 6481 loss: 2.5584187507629395\n",
      "step 6482 loss: 2.536874532699585\n",
      "step 6483 loss: 2.4505724906921387\n",
      "step 6484 loss: 2.4830188751220703\n",
      "step 6485 loss: 2.632005214691162\n",
      "step 6486 loss: 2.4896419048309326\n",
      "step 6487 loss: 2.5578248500823975\n",
      "step 6488 loss: 2.610771656036377\n",
      "step 6489 loss: 2.4935898780822754\n",
      "step 6490 loss: 2.421184539794922\n",
      "step 6491 loss: 2.4417123794555664\n",
      "step 6492 loss: 2.387399435043335\n",
      "step 6493 loss: 2.415159225463867\n",
      "step 6494 loss: 2.5750207901000977\n",
      "step 6495 loss: 2.5041685104370117\n",
      "step 6496 loss: 2.501204490661621\n",
      "step 6497 loss: 2.565274238586426\n",
      "step 6498 loss: 2.5213353633880615\n",
      "step 6499 loss: 2.460270881652832\n",
      "step 6500 loss: 2.4178595542907715\n",
      "step 6501 loss: 2.5662841796875\n",
      "step 6502 loss: 2.5471601486206055\n",
      "step 6503 loss: 2.5418777465820312\n",
      "step 6504 loss: 2.4179184436798096\n",
      "step 6505 loss: 2.5388200283050537\n",
      "step 6506 loss: 2.4674744606018066\n",
      "step 6507 loss: 2.465416193008423\n",
      "step 6508 loss: 2.6013121604919434\n",
      "step 6509 loss: 2.470761775970459\n",
      "step 6510 loss: 2.469684362411499\n",
      "step 6511 loss: 2.4191880226135254\n",
      "step 6512 loss: 2.578049659729004\n",
      "step 6513 loss: 2.424516201019287\n",
      "step 6514 loss: 2.4238457679748535\n",
      "step 6515 loss: 2.408860445022583\n",
      "step 6516 loss: 2.5179388523101807\n",
      "step 6517 loss: 2.4607181549072266\n",
      "step 6518 loss: 2.5517585277557373\n",
      "step 6519 loss: 2.4878756999969482\n",
      "step 6520 loss: 2.5152981281280518\n",
      "step 6521 loss: 2.561096668243408\n",
      "step 6522 loss: 2.4900450706481934\n",
      "step 6523 loss: 2.5446317195892334\n",
      "step 6524 loss: 2.4734249114990234\n",
      "step 6525 loss: 2.489933490753174\n",
      "step 6526 loss: 2.4208149909973145\n",
      "step 6527 loss: 2.4183766841888428\n",
      "step 6528 loss: 2.471022844314575\n",
      "step 6529 loss: 2.4580941200256348\n",
      "step 6530 loss: 2.526582717895508\n",
      "step 6531 loss: 2.4829812049865723\n",
      "step 6532 loss: 2.4066357612609863\n",
      "step 6533 loss: 2.446218967437744\n",
      "step 6534 loss: 2.427069902420044\n",
      "step 6535 loss: 2.558523416519165\n",
      "step 6536 loss: 2.568899154663086\n",
      "step 6537 loss: 2.552830934524536\n",
      "step 6538 loss: 2.669557809829712\n",
      "step 6539 loss: 2.5823023319244385\n",
      "step 6540 loss: 2.5251903533935547\n",
      "step 6541 loss: 2.4269521236419678\n",
      "step 6542 loss: 2.5421838760375977\n",
      "step 6543 loss: 2.4975922107696533\n",
      "step 6544 loss: 2.427905321121216\n",
      "step 6545 loss: 2.473118782043457\n",
      "step 6546 loss: 2.5145246982574463\n",
      "step 6547 loss: 2.547755002975464\n",
      "step 6548 loss: 2.4666404724121094\n",
      "step 6549 loss: 2.5369133949279785\n",
      "step 6550 loss: 2.563345193862915\n",
      "step 6551 loss: 2.59259033203125\n",
      "step 6552 loss: 2.5718765258789062\n",
      "step 6553 loss: 2.6170899868011475\n",
      "step 6554 loss: 2.5341062545776367\n",
      "step 6555 loss: 2.505157709121704\n",
      "step 6556 loss: 2.495516300201416\n",
      "step 6557 loss: 2.488698959350586\n",
      "step 6558 loss: 2.462463855743408\n",
      "step 6559 loss: 2.573120355606079\n",
      "step 6560 loss: 2.526181697845459\n",
      "step 6561 loss: 2.5852346420288086\n",
      "step 6562 loss: 2.4483120441436768\n",
      "step 6563 loss: 2.6344404220581055\n",
      "step 6564 loss: 2.487292528152466\n",
      "step 6565 loss: 2.3542537689208984\n",
      "step 6566 loss: 2.5044851303100586\n",
      "step 6567 loss: 2.4917004108428955\n",
      "step 6568 loss: 2.446233034133911\n",
      "step 6569 loss: 2.474050521850586\n",
      "step 6570 loss: 2.4484035968780518\n",
      "step 6571 loss: 2.6532557010650635\n",
      "step 6572 loss: 2.5265679359436035\n",
      "step 6573 loss: 2.511876344680786\n",
      "step 6574 loss: 2.5092921257019043\n",
      "step 6575 loss: 2.5554165840148926\n",
      "step 6576 loss: 2.606174945831299\n",
      "step 6577 loss: 2.4744997024536133\n",
      "step 6578 loss: 2.5016257762908936\n",
      "step 6579 loss: 2.515415668487549\n",
      "step 6580 loss: 2.475801467895508\n",
      "step 6581 loss: 2.5328330993652344\n",
      "step 6582 loss: 2.559828996658325\n",
      "step 6583 loss: 2.5711171627044678\n",
      "step 6584 loss: 2.5360045433044434\n",
      "step 6585 loss: 2.505460739135742\n",
      "step 6586 loss: 2.4814517498016357\n",
      "step 6587 loss: 2.5470774173736572\n",
      "step 6588 loss: 2.5271847248077393\n",
      "step 6589 loss: 2.495387554168701\n",
      "step 6590 loss: 2.5467772483825684\n",
      "step 6591 loss: 2.5825765132904053\n",
      "step 6592 loss: 2.515423536300659\n",
      "step 6593 loss: 2.582576036453247\n",
      "step 6594 loss: 2.406130790710449\n",
      "step 6595 loss: 2.5160162448883057\n",
      "step 6596 loss: 2.5799217224121094\n",
      "step 6597 loss: 2.4345803260803223\n",
      "step 6598 loss: 2.4934213161468506\n",
      "step 6599 loss: 2.4882309436798096\n",
      "step 6600 loss: 2.540128231048584\n",
      "step 6601 loss: 2.6429851055145264\n",
      "step 6602 loss: 2.411004066467285\n",
      "step 6603 loss: 2.440349817276001\n",
      "step 6604 loss: 2.511286497116089\n",
      "step 6605 loss: 2.5420608520507812\n",
      "step 6606 loss: 2.5098931789398193\n",
      "step 6607 loss: 2.419865608215332\n",
      "step 6608 loss: 2.533879041671753\n",
      "step 6609 loss: 2.4476680755615234\n",
      "step 6610 loss: 2.4888811111450195\n",
      "step 6611 loss: 2.4690914154052734\n",
      "step 6612 loss: 2.4551637172698975\n",
      "step 6613 loss: 2.5174009799957275\n",
      "step 6614 loss: 2.481820583343506\n",
      "step 6615 loss: 2.4739701747894287\n",
      "step 6616 loss: 2.583021402359009\n",
      "step 6617 loss: 2.427124500274658\n",
      "step 6618 loss: 2.491525173187256\n",
      "step 6619 loss: 2.544710397720337\n",
      "step 6620 loss: 2.405776262283325\n",
      "step 6621 loss: 2.587355613708496\n",
      "step 6622 loss: 2.528211832046509\n",
      "step 6623 loss: 2.4830539226531982\n",
      "step 6624 loss: 2.485797643661499\n",
      "step 6625 loss: 2.583998680114746\n",
      "step 6626 loss: 2.591331720352173\n",
      "step 6627 loss: 2.456239938735962\n",
      "step 6628 loss: 2.573767900466919\n",
      "step 6629 loss: 2.484363079071045\n",
      "step 6630 loss: 2.448354482650757\n",
      "step 6631 loss: 2.540388584136963\n",
      "step 6632 loss: 2.406430244445801\n",
      "step 6633 loss: 2.6411871910095215\n",
      "step 6634 loss: 2.5579171180725098\n",
      "step 6635 loss: 2.4896388053894043\n",
      "step 6636 loss: 2.5186216831207275\n",
      "step 6637 loss: 2.471865177154541\n",
      "step 6638 loss: 2.5161654949188232\n",
      "step 6639 loss: 2.3758883476257324\n",
      "step 6640 loss: 2.4921035766601562\n",
      "step 6641 loss: 2.491564989089966\n",
      "step 6642 loss: 2.603078603744507\n",
      "step 6643 loss: 2.6564927101135254\n",
      "step 6644 loss: 2.557865858078003\n",
      "step 6645 loss: 2.427339553833008\n",
      "step 6646 loss: 2.508999824523926\n",
      "step 6647 loss: 2.469134569168091\n",
      "step 6648 loss: 2.594748020172119\n",
      "step 6649 loss: 2.4341564178466797\n",
      "step 6650 loss: 2.5125396251678467\n",
      "step 6651 loss: 2.53619647026062\n",
      "step 6652 loss: 2.511611223220825\n",
      "step 6653 loss: 2.450800895690918\n",
      "step 6654 loss: 2.522529363632202\n",
      "step 6655 loss: 2.4299609661102295\n",
      "step 6656 loss: 2.5412120819091797\n",
      "step 6657 loss: 2.631178140640259\n",
      "step 6658 loss: 2.457381248474121\n",
      "step 6659 loss: 2.4662060737609863\n",
      "step 6660 loss: 2.591637134552002\n",
      "step 6661 loss: 2.5137126445770264\n",
      "step 6662 loss: 2.51535701751709\n",
      "step 6663 loss: 2.5749411582946777\n",
      "step 6664 loss: 2.4431588649749756\n",
      "step 6665 loss: 2.447208881378174\n",
      "step 6666 loss: 2.452208995819092\n",
      "step 6667 loss: 2.6465020179748535\n",
      "step 6668 loss: 2.4554941654205322\n",
      "step 6669 loss: 2.5229599475860596\n",
      "step 6670 loss: 2.418825387954712\n",
      "step 6671 loss: 2.3831865787506104\n",
      "step 6672 loss: 2.6176648139953613\n",
      "step 6673 loss: 2.619746685028076\n",
      "step 6674 loss: 2.35937237739563\n",
      "step 6675 loss: 2.4532978534698486\n",
      "step 6676 loss: 2.481581211090088\n",
      "step 6677 loss: 2.4333689212799072\n",
      "step 6678 loss: 2.4819986820220947\n",
      "step 6679 loss: 2.5294888019561768\n",
      "step 6680 loss: 2.452540874481201\n",
      "step 6681 loss: 2.5967063903808594\n",
      "step 6682 loss: 2.4637959003448486\n",
      "step 6683 loss: 2.4759557247161865\n",
      "step 6684 loss: 2.48677921295166\n",
      "step 6685 loss: 2.6147549152374268\n",
      "step 6686 loss: 2.428079128265381\n",
      "step 6687 loss: 2.458460569381714\n",
      "step 6688 loss: 2.586561441421509\n",
      "step 6689 loss: 2.394397258758545\n",
      "step 6690 loss: 2.4737064838409424\n",
      "step 6691 loss: 2.5833468437194824\n",
      "step 6692 loss: 2.593548536300659\n",
      "step 6693 loss: 2.5095367431640625\n",
      "step 6694 loss: 2.573533773422241\n",
      "step 6695 loss: 2.475276470184326\n",
      "step 6696 loss: 2.4621269702911377\n",
      "step 6697 loss: 2.452064275741577\n",
      "step 6698 loss: 2.44996976852417\n",
      "step 6699 loss: 2.4526844024658203\n",
      "step 6700 loss: 2.4389612674713135\n",
      "step 6701 loss: 2.5848875045776367\n",
      "step 6702 loss: 2.542598009109497\n",
      "step 6703 loss: 2.5101234912872314\n",
      "step 6704 loss: 2.4969260692596436\n",
      "step 6705 loss: 2.434950828552246\n",
      "step 6706 loss: 2.645332098007202\n",
      "step 6707 loss: 2.5121638774871826\n",
      "step 6708 loss: 2.512211799621582\n",
      "step 6709 loss: 2.468531847000122\n",
      "step 6710 loss: 2.6135976314544678\n",
      "step 6711 loss: 2.5084846019744873\n",
      "step 6712 loss: 2.4838061332702637\n",
      "step 6713 loss: 2.552107095718384\n",
      "step 6714 loss: 2.524653434753418\n",
      "step 6715 loss: 2.50712251663208\n",
      "step 6716 loss: 2.4773106575012207\n",
      "step 6717 loss: 2.5032126903533936\n",
      "step 6718 loss: 2.5023117065429688\n",
      "step 6719 loss: 2.5349931716918945\n",
      "step 6720 loss: 2.553931951522827\n",
      "step 6721 loss: 2.4847192764282227\n",
      "step 6722 loss: 2.615339756011963\n",
      "step 6723 loss: 2.4584109783172607\n",
      "step 6724 loss: 2.4613595008850098\n",
      "step 6725 loss: 2.418454647064209\n",
      "step 6726 loss: 2.4608168601989746\n",
      "step 6727 loss: 2.5549046993255615\n",
      "step 6728 loss: 2.4445743560791016\n",
      "step 6729 loss: 2.462747812271118\n",
      "step 6730 loss: 2.3941986560821533\n",
      "step 6731 loss: 2.4923815727233887\n",
      "step 6732 loss: 2.5658786296844482\n",
      "step 6733 loss: 2.5140347480773926\n",
      "step 6734 loss: 2.490889072418213\n",
      "step 6735 loss: 2.3652029037475586\n",
      "step 6736 loss: 2.4711410999298096\n",
      "step 6737 loss: 2.4926764965057373\n",
      "step 6738 loss: 2.5602331161499023\n",
      "step 6739 loss: 2.522585391998291\n",
      "step 6740 loss: 2.4435901641845703\n",
      "step 6741 loss: 2.4735288619995117\n",
      "step 6742 loss: 2.4826056957244873\n",
      "step 6743 loss: 2.484992504119873\n",
      "step 6744 loss: 2.426241159439087\n",
      "step 6745 loss: 2.5080158710479736\n",
      "step 6746 loss: 2.5223042964935303\n",
      "step 6747 loss: 2.458799362182617\n",
      "step 6748 loss: 2.432919502258301\n",
      "step 6749 loss: 2.614482879638672\n",
      "step 6750 loss: 2.5821728706359863\n",
      "step 6751 loss: 2.5046536922454834\n",
      "step 6752 loss: 2.494476079940796\n",
      "step 6753 loss: 2.475827693939209\n",
      "step 6754 loss: 2.4028193950653076\n",
      "step 6755 loss: 2.458252191543579\n",
      "step 6756 loss: 2.477691888809204\n",
      "step 6757 loss: 2.499077796936035\n",
      "step 6758 loss: 2.533755302429199\n",
      "step 6759 loss: 2.4547502994537354\n",
      "step 6760 loss: 2.4457900524139404\n",
      "step 6761 loss: 2.558767557144165\n",
      "step 6762 loss: 2.446556806564331\n",
      "step 6763 loss: 2.5008764266967773\n",
      "step 6764 loss: 2.430036783218384\n",
      "step 6765 loss: 2.5748350620269775\n",
      "step 6766 loss: 2.5534472465515137\n",
      "step 6767 loss: 2.3943583965301514\n",
      "step 6768 loss: 2.5496408939361572\n",
      "step 6769 loss: 2.4665329456329346\n",
      "step 6770 loss: 2.5521080493927\n",
      "step 6771 loss: 2.5247013568878174\n",
      "step 6772 loss: 2.3112335205078125\n",
      "step 6773 loss: 2.6088459491729736\n",
      "step 6774 loss: 2.4211108684539795\n",
      "step 6775 loss: 2.5785913467407227\n",
      "step 6776 loss: 2.5174968242645264\n",
      "step 6777 loss: 2.682727336883545\n",
      "step 6778 loss: 2.537970781326294\n",
      "step 6779 loss: 2.4768247604370117\n",
      "step 6780 loss: 2.5675413608551025\n",
      "step 6781 loss: 2.4073925018310547\n",
      "step 6782 loss: 2.462794065475464\n",
      "step 6783 loss: 2.508298397064209\n",
      "step 6784 loss: 2.4829893112182617\n",
      "step 6785 loss: 2.4832098484039307\n",
      "step 6786 loss: 2.411536931991577\n",
      "step 6787 loss: 2.446284770965576\n",
      "step 6788 loss: 2.638436794281006\n",
      "step 6789 loss: 2.538585901260376\n",
      "step 6790 loss: 2.473566770553589\n",
      "step 6791 loss: 2.4972422122955322\n",
      "step 6792 loss: 2.5187504291534424\n",
      "step 6793 loss: 2.567779064178467\n",
      "step 6794 loss: 2.507309913635254\n",
      "step 6795 loss: 2.5238072872161865\n",
      "step 6796 loss: 2.5193932056427\n",
      "step 6797 loss: 2.547361373901367\n",
      "step 6798 loss: 2.464229106903076\n",
      "step 6799 loss: 2.4718167781829834\n",
      "step 6800 loss: 2.6755776405334473\n",
      "step 6801 loss: 2.4756903648376465\n",
      "step 6802 loss: 2.4911067485809326\n",
      "step 6803 loss: 2.523737668991089\n",
      "step 6804 loss: 2.4996135234832764\n",
      "step 6805 loss: 2.558218002319336\n",
      "step 6806 loss: 2.512672185897827\n",
      "step 6807 loss: 2.550443649291992\n",
      "step 6808 loss: 2.5857882499694824\n",
      "step 6809 loss: 2.4377100467681885\n",
      "step 6810 loss: 2.5601584911346436\n",
      "step 6811 loss: 2.458073616027832\n",
      "step 6812 loss: 2.5867128372192383\n",
      "step 6813 loss: 2.3692383766174316\n",
      "step 6814 loss: 2.3724138736724854\n",
      "step 6815 loss: 2.464531183242798\n",
      "step 6816 loss: 2.5416786670684814\n",
      "step 6817 loss: 2.529047727584839\n",
      "step 6818 loss: 2.537433624267578\n",
      "step 6819 loss: 2.4989562034606934\n",
      "step 6820 loss: 2.4882283210754395\n",
      "step 6821 loss: 2.479473352432251\n",
      "step 6822 loss: 2.5341978073120117\n",
      "step 6823 loss: 2.550114154815674\n",
      "step 6824 loss: 2.456721067428589\n",
      "step 6825 loss: 2.458376884460449\n",
      "step 6826 loss: 2.6334476470947266\n",
      "step 6827 loss: 2.4470393657684326\n",
      "step 6828 loss: 2.503957748413086\n",
      "step 6829 loss: 2.470174551010132\n",
      "step 6830 loss: 2.5306570529937744\n",
      "step 6831 loss: 2.676126003265381\n",
      "step 6832 loss: 2.663996458053589\n",
      "step 6833 loss: 2.5080044269561768\n",
      "step 6834 loss: 2.4696853160858154\n",
      "step 6835 loss: 2.535372018814087\n",
      "step 6836 loss: 2.5536937713623047\n",
      "step 6837 loss: 2.562283515930176\n",
      "step 6838 loss: 2.514474391937256\n",
      "step 6839 loss: 2.49259090423584\n",
      "step 6840 loss: 2.4410834312438965\n",
      "step 6841 loss: 2.5467493534088135\n",
      "step 6842 loss: 2.471580743789673\n",
      "step 6843 loss: 2.525716781616211\n",
      "step 6844 loss: 2.5070886611938477\n",
      "step 6845 loss: 2.531538724899292\n",
      "step 6846 loss: 2.5727248191833496\n",
      "step 6847 loss: 2.7268528938293457\n",
      "step 6848 loss: 2.5980417728424072\n",
      "step 6849 loss: 2.445887804031372\n",
      "step 6850 loss: 2.493269681930542\n",
      "step 6851 loss: 2.4567689895629883\n",
      "step 6852 loss: 2.5550830364227295\n",
      "step 6853 loss: 2.4276461601257324\n",
      "step 6854 loss: 2.511749505996704\n",
      "step 6855 loss: 2.5423240661621094\n",
      "step 6856 loss: 2.514638900756836\n",
      "step 6857 loss: 2.4719347953796387\n",
      "step 6858 loss: 2.459770679473877\n",
      "step 6859 loss: 2.428816318511963\n",
      "step 6860 loss: 2.4357898235321045\n",
      "step 6861 loss: 2.4991703033447266\n",
      "step 6862 loss: 2.5890841484069824\n",
      "step 6863 loss: 2.5587780475616455\n",
      "step 6864 loss: 2.5211288928985596\n",
      "step 6865 loss: 2.4933834075927734\n",
      "step 6866 loss: 2.5807199478149414\n",
      "step 6867 loss: 2.6031274795532227\n",
      "step 6868 loss: 2.3920741081237793\n",
      "step 6869 loss: 2.5680041313171387\n",
      "step 6870 loss: 2.4595165252685547\n",
      "step 6871 loss: 2.518500328063965\n",
      "step 6872 loss: 2.4946701526641846\n",
      "step 6873 loss: 2.43064546585083\n",
      "step 6874 loss: 2.3807215690612793\n",
      "step 6875 loss: 2.4099535942077637\n",
      "step 6876 loss: 2.5149033069610596\n",
      "step 6877 loss: 2.497894525527954\n",
      "step 6878 loss: 2.5623245239257812\n",
      "step 6879 loss: 2.396149158477783\n",
      "step 6880 loss: 2.4908065795898438\n",
      "step 6881 loss: 2.593752861022949\n",
      "step 6882 loss: 2.5110020637512207\n",
      "step 6883 loss: 2.6288466453552246\n",
      "step 6884 loss: 2.5914065837860107\n",
      "step 6885 loss: 2.53729510307312\n",
      "step 6886 loss: 2.537080764770508\n",
      "step 6887 loss: 2.5117366313934326\n",
      "step 6888 loss: 2.4462528228759766\n",
      "step 6889 loss: 2.521366834640503\n",
      "step 6890 loss: 2.507099151611328\n",
      "step 6891 loss: 2.5226054191589355\n",
      "step 6892 loss: 2.4897243976593018\n",
      "step 6893 loss: 2.5172834396362305\n",
      "step 6894 loss: 2.475863456726074\n",
      "step 6895 loss: 2.4567108154296875\n",
      "step 6896 loss: 2.4660494327545166\n",
      "step 6897 loss: 2.4459784030914307\n",
      "step 6898 loss: 2.3998849391937256\n",
      "step 6899 loss: 2.3785479068756104\n",
      "step 6900 loss: 2.3983147144317627\n",
      "step 6901 loss: 2.548353910446167\n",
      "step 6902 loss: 2.4506208896636963\n",
      "step 6903 loss: 2.4994683265686035\n",
      "step 6904 loss: 2.445375680923462\n",
      "step 6905 loss: 2.4922168254852295\n",
      "step 6906 loss: 2.414252519607544\n",
      "step 6907 loss: 2.5225040912628174\n",
      "step 6908 loss: 2.662334442138672\n",
      "step 6909 loss: 2.443260669708252\n",
      "step 6910 loss: 2.543137550354004\n",
      "step 6911 loss: 2.508774518966675\n",
      "step 6912 loss: 2.466886043548584\n",
      "step 6913 loss: 2.5196099281311035\n",
      "step 6914 loss: 2.6545164585113525\n",
      "step 6915 loss: 2.6109721660614014\n",
      "step 6916 loss: 2.557373046875\n",
      "step 6917 loss: 2.340472459793091\n",
      "step 6918 loss: 2.5394718647003174\n",
      "step 6919 loss: 2.4900991916656494\n",
      "step 6920 loss: 2.560940980911255\n",
      "step 6921 loss: 2.4697160720825195\n",
      "step 6922 loss: 2.4331114292144775\n",
      "step 6923 loss: 2.479313850402832\n",
      "step 6924 loss: 2.5241692066192627\n",
      "step 6925 loss: 2.5021607875823975\n",
      "step 6926 loss: 2.498645782470703\n",
      "step 6927 loss: 2.4702811241149902\n",
      "step 6928 loss: 2.453805923461914\n",
      "step 6929 loss: 2.475032091140747\n",
      "step 6930 loss: 2.5906238555908203\n",
      "step 6931 loss: 2.474567413330078\n",
      "step 6932 loss: 2.4920260906219482\n",
      "step 6933 loss: 2.5144355297088623\n",
      "step 6934 loss: 2.4659717082977295\n",
      "step 6935 loss: 2.5457651615142822\n",
      "step 6936 loss: 2.6412911415100098\n",
      "step 6937 loss: 2.522127151489258\n",
      "step 6938 loss: 2.366095542907715\n",
      "step 6939 loss: 2.4150052070617676\n",
      "step 6940 loss: 2.363567590713501\n",
      "step 6941 loss: 2.5231306552886963\n",
      "step 6942 loss: 2.5991933345794678\n",
      "step 6943 loss: 2.511162042617798\n",
      "step 6944 loss: 2.565028190612793\n",
      "step 6945 loss: 2.567302703857422\n",
      "step 6946 loss: 2.4109554290771484\n",
      "step 6947 loss: 2.4856109619140625\n",
      "step 6948 loss: 2.620260238647461\n",
      "step 6949 loss: 2.4648051261901855\n",
      "step 6950 loss: 2.598050594329834\n",
      "step 6951 loss: 2.4443018436431885\n",
      "step 6952 loss: 2.439985513687134\n",
      "step 6953 loss: 2.448836088180542\n",
      "step 6954 loss: 2.6367075443267822\n",
      "step 6955 loss: 2.5436971187591553\n",
      "step 6956 loss: 2.5479938983917236\n",
      "step 6957 loss: 2.553588628768921\n",
      "step 6958 loss: 2.5215389728546143\n",
      "step 6959 loss: 2.589815616607666\n",
      "step 6960 loss: 2.6213479042053223\n",
      "step 6961 loss: 2.44950008392334\n",
      "step 6962 loss: 2.3747756481170654\n",
      "step 6963 loss: 2.5435800552368164\n",
      "step 6964 loss: 2.480302333831787\n",
      "step 6965 loss: 2.496279001235962\n",
      "step 6966 loss: 2.619145393371582\n",
      "step 6967 loss: 2.5689773559570312\n",
      "step 6968 loss: 2.5010170936584473\n",
      "step 6969 loss: 2.605435609817505\n",
      "step 6970 loss: 2.633399486541748\n",
      "step 6971 loss: 2.4732627868652344\n",
      "step 6972 loss: 2.5155587196350098\n",
      "step 6973 loss: 2.516005277633667\n",
      "step 6974 loss: 2.456941604614258\n",
      "step 6975 loss: 2.374912738800049\n",
      "step 6976 loss: 2.448765516281128\n",
      "step 6977 loss: 2.5110106468200684\n",
      "step 6978 loss: 2.454470157623291\n",
      "step 6979 loss: 2.5286545753479004\n",
      "step 6980 loss: 2.463322877883911\n",
      "step 6981 loss: 2.4492690563201904\n",
      "step 6982 loss: 2.549516439437866\n",
      "step 6983 loss: 2.544982433319092\n",
      "step 6984 loss: 2.5576977729797363\n",
      "step 6985 loss: 2.421935796737671\n",
      "step 6986 loss: 2.5196382999420166\n",
      "step 6987 loss: 2.5311954021453857\n",
      "step 6988 loss: 2.5750813484191895\n",
      "step 6989 loss: 2.3784215450286865\n",
      "step 6990 loss: 2.452871799468994\n",
      "step 6991 loss: 2.5657777786254883\n",
      "step 6992 loss: 2.5647671222686768\n",
      "step 6993 loss: 2.475177764892578\n",
      "step 6994 loss: 2.4812569618225098\n",
      "step 6995 loss: 2.398681163787842\n",
      "step 6996 loss: 2.497779607772827\n",
      "step 6997 loss: 2.428126573562622\n",
      "step 6998 loss: 2.5597965717315674\n",
      "step 6999 loss: 2.4518814086914062\n",
      "step 7000 loss: 2.514069080352783\n",
      "step 7001 loss: 2.525545358657837\n",
      "step 7002 loss: 2.5032103061676025\n",
      "step 7003 loss: 2.487205743789673\n",
      "step 7004 loss: 2.454289674758911\n",
      "step 7005 loss: 2.528111696243286\n",
      "step 7006 loss: 2.565103530883789\n",
      "step 7007 loss: 2.462754249572754\n",
      "step 7008 loss: 2.5335137844085693\n",
      "step 7009 loss: 2.5132434368133545\n",
      "step 7010 loss: 2.4664673805236816\n",
      "step 7011 loss: 2.4037024974823\n",
      "step 7012 loss: 2.530703544616699\n",
      "step 7013 loss: 2.416959762573242\n",
      "step 7014 loss: 2.481872081756592\n",
      "step 7015 loss: 2.563554048538208\n",
      "step 7016 loss: 2.333467483520508\n",
      "step 7017 loss: 2.461822032928467\n",
      "step 7018 loss: 2.3907394409179688\n",
      "step 7019 loss: 2.323789596557617\n",
      "step 7020 loss: 2.478912115097046\n",
      "step 7021 loss: 2.5359349250793457\n",
      "step 7022 loss: 2.3998911380767822\n",
      "step 7023 loss: 2.510749101638794\n",
      "step 7024 loss: 2.5162973403930664\n",
      "step 7025 loss: 2.4898598194122314\n",
      "step 7026 loss: 2.4498438835144043\n",
      "step 7027 loss: 2.49015212059021\n",
      "step 7028 loss: 2.470071315765381\n",
      "step 7029 loss: 2.4837114810943604\n",
      "step 7030 loss: 2.508465528488159\n",
      "step 7031 loss: 2.512697219848633\n",
      "step 7032 loss: 2.4880409240722656\n",
      "step 7033 loss: 2.5588061809539795\n",
      "step 7034 loss: 2.483600616455078\n",
      "step 7035 loss: 2.4494965076446533\n",
      "step 7036 loss: 2.5656838417053223\n",
      "step 7037 loss: 2.539738655090332\n",
      "step 7038 loss: 2.3367979526519775\n",
      "step 7039 loss: 2.518275022506714\n",
      "step 7040 loss: 2.5906941890716553\n",
      "step 7041 loss: 2.5351030826568604\n",
      "step 7042 loss: 2.5299572944641113\n",
      "step 7043 loss: 2.519648790359497\n",
      "step 7044 loss: 2.417750120162964\n",
      "step 7045 loss: 2.4858624935150146\n",
      "step 7046 loss: 2.4230265617370605\n",
      "step 7047 loss: 2.4396677017211914\n",
      "step 7048 loss: 2.468393325805664\n",
      "step 7049 loss: 2.5744011402130127\n",
      "step 7050 loss: 2.4348320960998535\n",
      "step 7051 loss: 2.5334112644195557\n",
      "step 7052 loss: 2.4869840145111084\n",
      "step 7053 loss: 2.4652342796325684\n",
      "step 7054 loss: 2.6243624687194824\n",
      "step 7055 loss: 2.512650966644287\n",
      "step 7056 loss: 2.3727197647094727\n",
      "step 7057 loss: 2.5633039474487305\n",
      "step 7058 loss: 2.558849334716797\n",
      "step 7059 loss: 2.4538769721984863\n",
      "step 7060 loss: 2.4431705474853516\n",
      "step 7061 loss: 2.3892736434936523\n",
      "step 7062 loss: 2.6405348777770996\n",
      "step 7063 loss: 2.586052417755127\n",
      "step 7064 loss: 2.4888899326324463\n",
      "step 7065 loss: 2.434000015258789\n",
      "step 7066 loss: 2.498068332672119\n",
      "step 7067 loss: 2.488023042678833\n",
      "step 7068 loss: 2.342634677886963\n",
      "step 7069 loss: 2.6141369342803955\n",
      "step 7070 loss: 2.506694793701172\n",
      "step 7071 loss: 2.4489200115203857\n",
      "step 7072 loss: 2.495319366455078\n",
      "step 7073 loss: 2.4780218601226807\n",
      "step 7074 loss: 2.491905450820923\n",
      "step 7075 loss: 2.518181800842285\n",
      "step 7076 loss: 2.358647108078003\n",
      "step 7077 loss: 2.5383434295654297\n",
      "step 7078 loss: 2.665985345840454\n",
      "step 7079 loss: 2.6195600032806396\n",
      "step 7080 loss: 2.5024077892303467\n",
      "step 7081 loss: 2.470215082168579\n",
      "step 7082 loss: 2.600494623184204\n",
      "step 7083 loss: 2.551100730895996\n",
      "step 7084 loss: 2.522822856903076\n",
      "step 7085 loss: 2.4120354652404785\n",
      "step 7086 loss: 2.515920400619507\n",
      "step 7087 loss: 2.40346097946167\n",
      "step 7088 loss: 2.5303854942321777\n",
      "step 7089 loss: 2.4205782413482666\n",
      "step 7090 loss: 2.5545945167541504\n",
      "step 7091 loss: 2.529921293258667\n",
      "step 7092 loss: 2.450493574142456\n",
      "step 7093 loss: 2.5352632999420166\n",
      "step 7094 loss: 2.461845874786377\n",
      "step 7095 loss: 2.608598470687866\n",
      "step 7096 loss: 2.5237350463867188\n",
      "step 7097 loss: 2.3648276329040527\n",
      "step 7098 loss: 2.453265428543091\n",
      "step 7099 loss: 2.5369198322296143\n",
      "step 7100 loss: 2.465423583984375\n",
      "step 7101 loss: 2.418564796447754\n",
      "step 7102 loss: 2.599381446838379\n",
      "step 7103 loss: 2.295360803604126\n",
      "step 7104 loss: 2.577423095703125\n",
      "step 7105 loss: 2.332956314086914\n",
      "step 7106 loss: 2.429130792617798\n",
      "step 7107 loss: 2.5783650875091553\n",
      "step 7108 loss: 2.5815162658691406\n",
      "step 7109 loss: 2.521101951599121\n",
      "step 7110 loss: 2.403329372406006\n",
      "step 7111 loss: 2.514648675918579\n",
      "step 7112 loss: 2.448145627975464\n",
      "step 7113 loss: 2.4662795066833496\n",
      "step 7114 loss: 2.507952928543091\n",
      "step 7115 loss: 2.487779378890991\n",
      "step 7116 loss: 2.505669593811035\n",
      "step 7117 loss: 2.420135736465454\n",
      "step 7118 loss: 2.5067760944366455\n",
      "step 7119 loss: 2.465467691421509\n",
      "step 7120 loss: 2.636017322540283\n",
      "step 7121 loss: 2.5174295902252197\n",
      "step 7122 loss: 2.5208001136779785\n",
      "step 7123 loss: 2.3912291526794434\n",
      "step 7124 loss: 2.504408121109009\n",
      "step 7125 loss: 2.459245443344116\n",
      "step 7126 loss: 2.482578754425049\n",
      "step 7127 loss: 2.466156482696533\n",
      "step 7128 loss: 2.455364465713501\n",
      "step 7129 loss: 2.53509783744812\n",
      "step 7130 loss: 2.473675489425659\n",
      "step 7131 loss: 2.3388307094573975\n",
      "step 7132 loss: 2.4389970302581787\n",
      "step 7133 loss: 2.480726480484009\n",
      "step 7134 loss: 2.4778945446014404\n",
      "step 7135 loss: 2.5107293128967285\n",
      "step 7136 loss: 2.4945859909057617\n",
      "step 7137 loss: 2.409055233001709\n",
      "step 7138 loss: 2.406818389892578\n",
      "step 7139 loss: 2.5647127628326416\n",
      "step 7140 loss: 2.5866754055023193\n",
      "step 7141 loss: 2.4308817386627197\n",
      "step 7142 loss: 2.6041688919067383\n",
      "step 7143 loss: 2.520604133605957\n",
      "step 7144 loss: 2.4279749393463135\n",
      "step 7145 loss: 2.6344969272613525\n",
      "step 7146 loss: 2.4467735290527344\n",
      "step 7147 loss: 2.369964122772217\n",
      "step 7148 loss: 2.525815725326538\n",
      "step 7149 loss: 2.488739490509033\n",
      "step 7150 loss: 2.465487480163574\n",
      "step 7151 loss: 2.504804849624634\n",
      "step 7152 loss: 2.4894697666168213\n",
      "step 7153 loss: 2.35164737701416\n",
      "step 7154 loss: 2.5758302211761475\n",
      "step 7155 loss: 2.5665740966796875\n",
      "step 7156 loss: 2.477248191833496\n",
      "step 7157 loss: 2.5322725772857666\n",
      "step 7158 loss: 2.5219361782073975\n",
      "step 7159 loss: 2.3742458820343018\n",
      "step 7160 loss: 2.550632953643799\n",
      "step 7161 loss: 2.536252975463867\n",
      "step 7162 loss: 2.470378875732422\n",
      "step 7163 loss: 2.511455774307251\n",
      "step 7164 loss: 2.4619953632354736\n",
      "step 7165 loss: 2.6021676063537598\n",
      "step 7166 loss: 2.5715012550354004\n",
      "step 7167 loss: 2.4956307411193848\n",
      "step 7168 loss: 2.5319340229034424\n",
      "step 7169 loss: 2.4663102626800537\n",
      "step 7170 loss: 2.5120959281921387\n",
      "step 7171 loss: 2.504664182662964\n",
      "step 7172 loss: 2.471250295639038\n",
      "step 7173 loss: 2.4811604022979736\n",
      "step 7174 loss: 2.4209766387939453\n",
      "step 7175 loss: 2.4265213012695312\n",
      "step 7176 loss: 2.439992666244507\n",
      "step 7177 loss: 2.446199655532837\n",
      "step 7178 loss: 2.3936126232147217\n",
      "step 7179 loss: 2.602355718612671\n",
      "step 7180 loss: 2.551645040512085\n",
      "step 7181 loss: 2.524017333984375\n",
      "step 7182 loss: 2.5412516593933105\n",
      "step 7183 loss: 2.4921114444732666\n",
      "step 7184 loss: 2.3825416564941406\n",
      "step 7185 loss: 2.4503064155578613\n",
      "step 7186 loss: 2.457592248916626\n",
      "step 7187 loss: 2.522383213043213\n",
      "step 7188 loss: 2.5003156661987305\n",
      "step 7189 loss: 2.4528868198394775\n",
      "step 7190 loss: 2.5145535469055176\n",
      "step 7191 loss: 2.52276349067688\n",
      "step 7192 loss: 2.4977002143859863\n",
      "step 7193 loss: 2.5287606716156006\n",
      "step 7194 loss: 2.4583373069763184\n",
      "step 7195 loss: 2.570765495300293\n",
      "step 7196 loss: 2.4301581382751465\n",
      "step 7197 loss: 2.4465248584747314\n",
      "step 7198 loss: 2.5371110439300537\n",
      "step 7199 loss: 2.540802001953125\n",
      "step 7200 loss: 2.4735515117645264\n",
      "step 7201 loss: 2.4795711040496826\n",
      "step 7202 loss: 2.5615222454071045\n",
      "step 7203 loss: 2.5053372383117676\n",
      "step 7204 loss: 2.5162460803985596\n",
      "step 7205 loss: 2.6169357299804688\n",
      "step 7206 loss: 2.4342734813690186\n",
      "step 7207 loss: 2.453324794769287\n",
      "step 7208 loss: 2.4634978771209717\n",
      "step 7209 loss: 2.5826873779296875\n",
      "step 7210 loss: 2.5628538131713867\n",
      "step 7211 loss: 2.476716995239258\n",
      "step 7212 loss: 2.4270498752593994\n",
      "step 7213 loss: 2.5510778427124023\n",
      "step 7214 loss: 2.606523275375366\n",
      "step 7215 loss: 2.5895745754241943\n",
      "step 7216 loss: 2.4347801208496094\n",
      "step 7217 loss: 2.5079305171966553\n",
      "step 7218 loss: 2.5532479286193848\n",
      "step 7219 loss: 2.430872678756714\n",
      "step 7220 loss: 2.5472028255462646\n",
      "step 7221 loss: 2.4759602546691895\n",
      "step 7222 loss: 2.486494779586792\n",
      "step 7223 loss: 2.42696475982666\n",
      "step 7224 loss: 2.5036849975585938\n",
      "step 7225 loss: 2.4605355262756348\n",
      "step 7226 loss: 2.3612773418426514\n",
      "step 7227 loss: 2.514934778213501\n",
      "step 7228 loss: 2.464033603668213\n",
      "step 7229 loss: 2.594470262527466\n",
      "step 7230 loss: 2.6935336589813232\n",
      "step 7231 loss: 2.565581798553467\n",
      "step 7232 loss: 2.4035134315490723\n",
      "step 7233 loss: 2.3900628089904785\n",
      "step 7234 loss: 2.44406795501709\n",
      "step 7235 loss: 2.513202667236328\n",
      "step 7236 loss: 2.4040422439575195\n",
      "step 7237 loss: 2.6040000915527344\n",
      "step 7238 loss: 2.5492913722991943\n",
      "step 7239 loss: 2.5094528198242188\n",
      "step 7240 loss: 2.4810235500335693\n",
      "step 7241 loss: 2.6217148303985596\n",
      "step 7242 loss: 2.517892837524414\n",
      "step 7243 loss: 2.4629368782043457\n",
      "step 7244 loss: 2.4071428775787354\n",
      "step 7245 loss: 2.443678379058838\n",
      "step 7246 loss: 2.549710512161255\n",
      "step 7247 loss: 2.412203073501587\n",
      "step 7248 loss: 2.4695427417755127\n",
      "step 7249 loss: 2.543569326400757\n",
      "step 7250 loss: 2.566052198410034\n",
      "step 7251 loss: 2.442392110824585\n",
      "step 7252 loss: 2.5381836891174316\n",
      "step 7253 loss: 2.469454765319824\n",
      "step 7254 loss: 2.4850716590881348\n",
      "step 7255 loss: 2.454425573348999\n",
      "step 7256 loss: 2.5433435440063477\n",
      "step 7257 loss: 2.4982638359069824\n",
      "step 7258 loss: 2.4324610233306885\n",
      "step 7259 loss: 2.588301181793213\n",
      "step 7260 loss: 2.4611093997955322\n",
      "step 7261 loss: 2.5073513984680176\n",
      "step 7262 loss: 2.5819380283355713\n",
      "step 7263 loss: 2.449894666671753\n",
      "step 7264 loss: 2.455127716064453\n",
      "step 7265 loss: 2.5781450271606445\n",
      "step 7266 loss: 2.494749069213867\n",
      "step 7267 loss: 2.5366859436035156\n",
      "step 7268 loss: 2.4950366020202637\n",
      "step 7269 loss: 2.5220048427581787\n",
      "step 7270 loss: 2.38466739654541\n",
      "step 7271 loss: 2.546811103820801\n",
      "step 7272 loss: 2.568575143814087\n",
      "step 7273 loss: 2.4901952743530273\n",
      "step 7274 loss: 2.559945583343506\n",
      "step 7275 loss: 2.5299341678619385\n",
      "step 7276 loss: 2.4463768005371094\n",
      "step 7277 loss: 2.5204453468322754\n",
      "step 7278 loss: 2.580422878265381\n",
      "step 7279 loss: 2.4450342655181885\n",
      "step 7280 loss: 2.492917060852051\n",
      "step 7281 loss: 2.4184951782226562\n",
      "step 7282 loss: 2.5534615516662598\n",
      "step 7283 loss: 2.48738169670105\n",
      "step 7284 loss: 2.5056872367858887\n",
      "step 7285 loss: 2.675008535385132\n",
      "step 7286 loss: 2.409731149673462\n",
      "step 7287 loss: 2.4945356845855713\n",
      "step 7288 loss: 2.4140207767486572\n",
      "step 7289 loss: 2.539802312850952\n",
      "step 7290 loss: 2.500373363494873\n",
      "step 7291 loss: 2.585461378097534\n",
      "step 7292 loss: 2.39506459236145\n",
      "step 7293 loss: 2.4192402362823486\n",
      "step 7294 loss: 2.5750277042388916\n",
      "step 7295 loss: 2.537860631942749\n",
      "step 7296 loss: 2.5136327743530273\n",
      "step 7297 loss: 2.4621827602386475\n",
      "step 7298 loss: 2.567289113998413\n",
      "step 7299 loss: 2.533982515335083\n",
      "step 7300 loss: 2.483755588531494\n",
      "step 7301 loss: 2.5683751106262207\n",
      "step 7302 loss: 2.50540828704834\n",
      "step 7303 loss: 2.5601844787597656\n",
      "step 7304 loss: 2.4683589935302734\n",
      "step 7305 loss: 2.4715328216552734\n",
      "step 7306 loss: 2.5937609672546387\n",
      "step 7307 loss: 2.550002098083496\n",
      "step 7308 loss: 2.5318846702575684\n",
      "step 7309 loss: 2.5317625999450684\n",
      "step 7310 loss: 2.48071551322937\n",
      "step 7311 loss: 2.466839075088501\n",
      "step 7312 loss: 2.5009751319885254\n",
      "step 7313 loss: 2.4794321060180664\n",
      "step 7314 loss: 2.549950122833252\n",
      "step 7315 loss: 2.5133326053619385\n",
      "step 7316 loss: 2.5377111434936523\n",
      "step 7317 loss: 2.4259376525878906\n",
      "step 7318 loss: 2.530158281326294\n",
      "step 7319 loss: 2.5160953998565674\n",
      "step 7320 loss: 2.47404408454895\n",
      "step 7321 loss: 2.397270917892456\n",
      "step 7322 loss: 2.4776272773742676\n",
      "step 7323 loss: 2.417800188064575\n",
      "step 7324 loss: 2.5910089015960693\n",
      "step 7325 loss: 2.6685867309570312\n",
      "step 7326 loss: 2.394174575805664\n",
      "step 7327 loss: 2.6096255779266357\n",
      "step 7328 loss: 2.597231864929199\n",
      "step 7329 loss: 2.571702718734741\n",
      "step 7330 loss: 2.534975051879883\n",
      "step 7331 loss: 2.5430877208709717\n",
      "step 7332 loss: 2.4675936698913574\n",
      "step 7333 loss: 2.46024489402771\n",
      "step 7334 loss: 2.5774927139282227\n",
      "step 7335 loss: 2.518008232116699\n",
      "step 7336 loss: 2.5239572525024414\n",
      "step 7337 loss: 2.430450439453125\n",
      "step 7338 loss: 2.442056894302368\n",
      "step 7339 loss: 2.4549248218536377\n",
      "step 7340 loss: 2.4356658458709717\n",
      "step 7341 loss: 2.409141778945923\n",
      "step 7342 loss: 2.615760087966919\n",
      "step 7343 loss: 2.402977705001831\n",
      "step 7344 loss: 2.4466097354888916\n",
      "step 7345 loss: 2.5467989444732666\n",
      "step 7346 loss: 2.472733497619629\n",
      "step 7347 loss: 2.50544810295105\n",
      "step 7348 loss: 2.382507562637329\n",
      "step 7349 loss: 2.4404048919677734\n",
      "step 7350 loss: 2.409541368484497\n",
      "step 7351 loss: 2.3823769092559814\n",
      "step 7352 loss: 2.486461877822876\n",
      "step 7353 loss: 2.534754514694214\n",
      "step 7354 loss: 2.460252046585083\n",
      "step 7355 loss: 2.5135319232940674\n",
      "step 7356 loss: 2.5173845291137695\n",
      "step 7357 loss: 2.5826337337493896\n",
      "step 7358 loss: 2.6239781379699707\n",
      "step 7359 loss: 2.531649112701416\n",
      "step 7360 loss: 2.5638298988342285\n",
      "step 7361 loss: 2.4973554611206055\n",
      "step 7362 loss: 2.4906740188598633\n",
      "step 7363 loss: 2.4959301948547363\n",
      "step 7364 loss: 2.5126683712005615\n",
      "step 7365 loss: 2.5427048206329346\n",
      "step 7366 loss: 2.47601318359375\n",
      "step 7367 loss: 2.568544864654541\n",
      "step 7368 loss: 2.513725996017456\n",
      "step 7369 loss: 2.4402153491973877\n",
      "step 7370 loss: 2.507695198059082\n",
      "step 7371 loss: 2.4857728481292725\n",
      "step 7372 loss: 2.3917510509490967\n",
      "step 7373 loss: 2.4763638973236084\n",
      "step 7374 loss: 2.5565009117126465\n",
      "step 7375 loss: 2.4907751083374023\n",
      "step 7376 loss: 2.4566822052001953\n",
      "step 7377 loss: 2.527798652648926\n",
      "step 7378 loss: 2.358569860458374\n",
      "step 7379 loss: 2.4969093799591064\n",
      "step 7380 loss: 2.562924861907959\n",
      "step 7381 loss: 2.574704885482788\n",
      "step 7382 loss: 2.374481201171875\n",
      "step 7383 loss: 2.52073073387146\n",
      "step 7384 loss: 2.6040890216827393\n",
      "step 7385 loss: 2.4302268028259277\n",
      "step 7386 loss: 2.5894594192504883\n",
      "step 7387 loss: 2.561504602432251\n",
      "step 7388 loss: 2.533766984939575\n",
      "step 7389 loss: 2.472073554992676\n",
      "step 7390 loss: 2.5569283962249756\n",
      "step 7391 loss: 2.3804354667663574\n",
      "step 7392 loss: 2.5075037479400635\n",
      "step 7393 loss: 2.456450939178467\n",
      "step 7394 loss: 2.451892852783203\n",
      "step 7395 loss: 2.5226380825042725\n",
      "step 7396 loss: 2.49157452583313\n",
      "step 7397 loss: 2.480689525604248\n",
      "step 7398 loss: 2.472259044647217\n",
      "step 7399 loss: 2.3563928604125977\n",
      "step 7400 loss: 2.444798707962036\n",
      "step 7401 loss: 2.505891799926758\n",
      "step 7402 loss: 2.4880080223083496\n",
      "step 7403 loss: 2.4626569747924805\n",
      "step 7404 loss: 2.6162068843841553\n",
      "step 7405 loss: 2.562716245651245\n",
      "step 7406 loss: 2.466695785522461\n",
      "step 7407 loss: 2.62503719329834\n",
      "step 7408 loss: 2.3857815265655518\n",
      "step 7409 loss: 2.4853012561798096\n",
      "step 7410 loss: 2.496997594833374\n",
      "step 7411 loss: 2.557609796524048\n",
      "step 7412 loss: 2.50584077835083\n",
      "step 7413 loss: 2.4791533946990967\n",
      "step 7414 loss: 2.494831085205078\n",
      "step 7415 loss: 2.5449252128601074\n",
      "step 7416 loss: 2.432568311691284\n",
      "step 7417 loss: 2.5285134315490723\n",
      "step 7418 loss: 2.5366053581237793\n",
      "step 7419 loss: 2.5222384929656982\n",
      "step 7420 loss: 2.588888168334961\n",
      "step 7421 loss: 2.3820104598999023\n",
      "step 7422 loss: 2.494547128677368\n",
      "step 7423 loss: 2.5998964309692383\n",
      "step 7424 loss: 2.45997953414917\n",
      "step 7425 loss: 2.4328696727752686\n",
      "step 7426 loss: 2.4231314659118652\n",
      "step 7427 loss: 2.503143072128296\n",
      "step 7428 loss: 2.621572494506836\n",
      "step 7429 loss: 2.4133691787719727\n",
      "step 7430 loss: 2.575084924697876\n",
      "step 7431 loss: 2.5645759105682373\n",
      "step 7432 loss: 2.497485876083374\n",
      "step 7433 loss: 2.579996347427368\n",
      "step 7434 loss: 2.381385326385498\n",
      "step 7435 loss: 2.467008352279663\n",
      "step 7436 loss: 2.532071352005005\n",
      "step 7437 loss: 2.556288003921509\n",
      "step 7438 loss: 2.4306821823120117\n",
      "step 7439 loss: 2.438398838043213\n",
      "step 7440 loss: 2.393807888031006\n",
      "step 7441 loss: 2.3721868991851807\n",
      "step 7442 loss: 2.418290615081787\n",
      "step 7443 loss: 2.4225215911865234\n",
      "step 7444 loss: 2.442089557647705\n",
      "step 7445 loss: 2.5795512199401855\n",
      "step 7446 loss: 2.4672865867614746\n",
      "step 7447 loss: 2.494626760482788\n",
      "step 7448 loss: 2.5038440227508545\n",
      "step 7449 loss: 2.5022857189178467\n",
      "step 7450 loss: 2.559168815612793\n",
      "step 7451 loss: 2.5580382347106934\n",
      "step 7452 loss: 2.4847733974456787\n",
      "step 7453 loss: 2.588055372238159\n",
      "step 7454 loss: 2.544395685195923\n",
      "step 7455 loss: 2.4329495429992676\n",
      "step 7456 loss: 2.5013060569763184\n",
      "step 7457 loss: 2.5014710426330566\n",
      "step 7458 loss: 2.4842796325683594\n",
      "step 7459 loss: 2.5858428478240967\n",
      "step 7460 loss: 2.5531809329986572\n",
      "step 7461 loss: 2.5043087005615234\n",
      "step 7462 loss: 2.4491281509399414\n",
      "step 7463 loss: 2.413100004196167\n",
      "step 7464 loss: 2.587275505065918\n",
      "step 7465 loss: 2.384018898010254\n",
      "step 7466 loss: 2.5409669876098633\n",
      "step 7467 loss: 2.4761340618133545\n",
      "step 7468 loss: 2.4906325340270996\n",
      "step 7469 loss: 2.505110263824463\n",
      "step 7470 loss: 2.4672350883483887\n",
      "step 7471 loss: 2.5661087036132812\n",
      "step 7472 loss: 2.4217543601989746\n",
      "step 7473 loss: 2.498331308364868\n",
      "step 7474 loss: 2.5239617824554443\n",
      "step 7475 loss: 2.4743666648864746\n",
      "step 7476 loss: 2.611433744430542\n",
      "step 7477 loss: 2.4793076515197754\n",
      "step 7478 loss: 2.515302896499634\n",
      "step 7479 loss: 2.548506498336792\n",
      "step 7480 loss: 2.4630813598632812\n",
      "step 7481 loss: 2.4174041748046875\n",
      "step 7482 loss: 2.4251937866210938\n",
      "step 7483 loss: 2.4268808364868164\n",
      "step 7484 loss: 2.5238990783691406\n",
      "step 7485 loss: 2.462766647338867\n",
      "step 7486 loss: 2.58955979347229\n",
      "step 7487 loss: 2.4495458602905273\n",
      "step 7488 loss: 2.5320186614990234\n",
      "step 7489 loss: 2.4286952018737793\n",
      "step 7490 loss: 2.4793918132781982\n",
      "step 7491 loss: 2.443232536315918\n",
      "step 7492 loss: 2.487459659576416\n",
      "step 7493 loss: 2.5101754665374756\n",
      "step 7494 loss: 2.5056443214416504\n",
      "step 7495 loss: 2.3496410846710205\n",
      "step 7496 loss: 2.4752655029296875\n",
      "step 7497 loss: 2.4691765308380127\n",
      "step 7498 loss: 2.553086519241333\n",
      "step 7499 loss: 2.373178482055664\n",
      "step 7500 loss: 2.5974466800689697\n",
      "step 7501 loss: 2.546588897705078\n",
      "step 7502 loss: 2.5443131923675537\n",
      "step 7503 loss: 2.506989002227783\n",
      "step 7504 loss: 2.5776946544647217\n",
      "step 7505 loss: 2.458624839782715\n",
      "step 7506 loss: 2.618917465209961\n",
      "step 7507 loss: 2.7120919227600098\n",
      "step 7508 loss: 2.4352517127990723\n",
      "step 7509 loss: 2.549962043762207\n",
      "step 7510 loss: 2.5122509002685547\n",
      "step 7511 loss: 2.6256442070007324\n",
      "step 7512 loss: 2.464024066925049\n",
      "step 7513 loss: 2.525566816329956\n",
      "step 7514 loss: 2.530848503112793\n",
      "step 7515 loss: 2.5088918209075928\n",
      "step 7516 loss: 2.471708059310913\n",
      "step 7517 loss: 2.5561954975128174\n",
      "step 7518 loss: 2.5206966400146484\n",
      "step 7519 loss: 2.5648865699768066\n",
      "step 7520 loss: 2.657843589782715\n",
      "step 7521 loss: 2.4987618923187256\n",
      "step 7522 loss: 2.391265869140625\n",
      "step 7523 loss: 2.504563093185425\n",
      "step 7524 loss: 2.546658515930176\n",
      "step 7525 loss: 2.5286920070648193\n",
      "step 7526 loss: 2.3257131576538086\n",
      "step 7527 loss: 2.4623732566833496\n",
      "step 7528 loss: 2.4265005588531494\n",
      "step 7529 loss: 2.5822465419769287\n",
      "step 7530 loss: 2.447136402130127\n",
      "step 7531 loss: 2.3384206295013428\n",
      "step 7532 loss: 2.5864713191986084\n",
      "step 7533 loss: 2.5846564769744873\n",
      "step 7534 loss: 2.5123367309570312\n",
      "step 7535 loss: 2.5563433170318604\n",
      "step 7536 loss: 2.4881389141082764\n",
      "step 7537 loss: 2.4401655197143555\n",
      "step 7538 loss: 2.472262144088745\n",
      "step 7539 loss: 2.4818308353424072\n",
      "step 7540 loss: 2.5319442749023438\n",
      "step 7541 loss: 2.5053417682647705\n",
      "step 7542 loss: 2.4677042961120605\n",
      "step 7543 loss: 2.5755724906921387\n",
      "step 7544 loss: 2.4159679412841797\n",
      "step 7545 loss: 2.357943296432495\n",
      "step 7546 loss: 2.5067265033721924\n",
      "step 7547 loss: 2.380708932876587\n",
      "step 7548 loss: 2.4788691997528076\n",
      "step 7549 loss: 2.481416702270508\n",
      "step 7550 loss: 2.510488510131836\n",
      "step 7551 loss: 2.5215165615081787\n",
      "step 7552 loss: 2.590409755706787\n",
      "step 7553 loss: 2.466870069503784\n",
      "step 7554 loss: 2.4678292274475098\n",
      "step 7555 loss: 2.527355194091797\n",
      "step 7556 loss: 2.4417126178741455\n",
      "step 7557 loss: 2.531871795654297\n",
      "step 7558 loss: 2.5417404174804688\n",
      "step 7559 loss: 2.4792046546936035\n",
      "step 7560 loss: 2.4555912017822266\n",
      "step 7561 loss: 2.4929018020629883\n",
      "step 7562 loss: 2.5447816848754883\n",
      "step 7563 loss: 2.5344290733337402\n",
      "step 7564 loss: 2.307913064956665\n",
      "step 7565 loss: 2.5729639530181885\n",
      "step 7566 loss: 2.434873580932617\n",
      "step 7567 loss: 2.466961622238159\n",
      "step 7568 loss: 2.4225687980651855\n",
      "step 7569 loss: 2.397935390472412\n",
      "step 7570 loss: 2.4029381275177\n",
      "step 7571 loss: 2.5153985023498535\n",
      "step 7572 loss: 2.3881194591522217\n",
      "step 7573 loss: 2.4441606998443604\n",
      "step 7574 loss: 2.444678544998169\n",
      "step 7575 loss: 2.5284364223480225\n",
      "step 7576 loss: 2.4479968547821045\n",
      "step 7577 loss: 2.495164155960083\n",
      "step 7578 loss: 2.510878562927246\n",
      "step 7579 loss: 2.4792168140411377\n",
      "step 7580 loss: 2.5386037826538086\n",
      "step 7581 loss: 2.4546310901641846\n",
      "step 7582 loss: 2.560872793197632\n",
      "step 7583 loss: 2.421161651611328\n",
      "step 7584 loss: 2.5190770626068115\n",
      "step 7585 loss: 2.4933438301086426\n",
      "step 7586 loss: 2.5863893032073975\n",
      "step 7587 loss: 2.5301737785339355\n",
      "step 7588 loss: 2.5678603649139404\n",
      "step 7589 loss: 2.4067606925964355\n",
      "step 7590 loss: 2.585430383682251\n",
      "step 7591 loss: 2.5605931282043457\n",
      "step 7592 loss: 2.3953161239624023\n",
      "step 7593 loss: 2.389035701751709\n",
      "step 7594 loss: 2.499075412750244\n",
      "step 7595 loss: 2.498595714569092\n",
      "step 7596 loss: 2.5304102897644043\n",
      "step 7597 loss: 2.425426959991455\n",
      "step 7598 loss: 2.5852575302124023\n",
      "step 7599 loss: 2.5835468769073486\n",
      "step 7600 loss: 2.4312326908111572\n",
      "step 7601 loss: 2.4413020610809326\n",
      "step 7602 loss: 2.680497169494629\n",
      "step 7603 loss: 2.6679751873016357\n",
      "step 7604 loss: 2.478710651397705\n",
      "step 7605 loss: 2.4352521896362305\n",
      "step 7606 loss: 2.4484524726867676\n",
      "step 7607 loss: 2.6592376232147217\n",
      "step 7608 loss: 2.4266304969787598\n",
      "step 7609 loss: 2.4307901859283447\n",
      "step 7610 loss: 2.5285773277282715\n",
      "step 7611 loss: 2.600393056869507\n",
      "step 7612 loss: 2.475541830062866\n",
      "step 7613 loss: 2.5233054161071777\n",
      "step 7614 loss: 2.420257329940796\n",
      "step 7615 loss: 2.5685300827026367\n",
      "step 7616 loss: 2.4382400512695312\n",
      "step 7617 loss: 2.5610384941101074\n",
      "step 7618 loss: 2.4468905925750732\n",
      "step 7619 loss: 2.477647304534912\n",
      "step 7620 loss: 2.48750901222229\n",
      "step 7621 loss: 2.4350481033325195\n",
      "step 7622 loss: 2.4955201148986816\n",
      "step 7623 loss: 2.4986789226531982\n",
      "step 7624 loss: 2.326359987258911\n",
      "step 7625 loss: 2.4203391075134277\n",
      "step 7626 loss: 2.5567307472229004\n",
      "step 7627 loss: 2.530602216720581\n",
      "step 7628 loss: 2.5002596378326416\n",
      "step 7629 loss: 2.4929006099700928\n",
      "step 7630 loss: 2.534510374069214\n",
      "step 7631 loss: 2.534153699874878\n",
      "step 7632 loss: 2.709547996520996\n",
      "step 7633 loss: 2.5123491287231445\n",
      "step 7634 loss: 2.4983885288238525\n",
      "step 7635 loss: 2.4435653686523438\n",
      "step 7636 loss: 2.5614984035491943\n",
      "step 7637 loss: 2.5648927688598633\n",
      "step 7638 loss: 2.4523231983184814\n",
      "step 7639 loss: 2.491140604019165\n",
      "step 7640 loss: 2.522733211517334\n",
      "step 7641 loss: 2.5797526836395264\n",
      "step 7642 loss: 2.5127110481262207\n",
      "step 7643 loss: 2.5972342491149902\n",
      "step 7644 loss: 2.5251147747039795\n",
      "step 7645 loss: 2.6369636058807373\n",
      "step 7646 loss: 2.58557391166687\n",
      "step 7647 loss: 2.5132017135620117\n",
      "step 7648 loss: 2.5036227703094482\n",
      "step 7649 loss: 2.461341381072998\n",
      "step 7650 loss: 2.4649274349212646\n",
      "step 7651 loss: 2.4094643592834473\n",
      "step 7652 loss: 2.5005977153778076\n",
      "step 7653 loss: 2.4838545322418213\n",
      "step 7654 loss: 2.541163444519043\n",
      "step 7655 loss: 2.487528085708618\n",
      "step 7656 loss: 2.461916446685791\n",
      "step 7657 loss: 2.6053011417388916\n",
      "step 7658 loss: 2.503220796585083\n",
      "step 7659 loss: 2.6166203022003174\n",
      "step 7660 loss: 2.4247031211853027\n",
      "step 7661 loss: 2.4977245330810547\n",
      "step 7662 loss: 2.4455859661102295\n",
      "step 7663 loss: 2.698072671890259\n",
      "step 7664 loss: 2.523341417312622\n",
      "step 7665 loss: 2.410565137863159\n",
      "step 7666 loss: 2.5641772747039795\n",
      "step 7667 loss: 2.4850456714630127\n",
      "step 7668 loss: 2.408250331878662\n",
      "step 7669 loss: 2.5519204139709473\n",
      "step 7670 loss: 2.374112367630005\n",
      "step 7671 loss: 2.486788511276245\n",
      "step 7672 loss: 2.4737234115600586\n",
      "step 7673 loss: 2.4378037452697754\n",
      "step 7674 loss: 2.5736918449401855\n",
      "step 7675 loss: 2.398322343826294\n",
      "step 7676 loss: 2.5317091941833496\n",
      "step 7677 loss: 2.53253173828125\n",
      "step 7678 loss: 2.559170961380005\n",
      "step 7679 loss: 2.402172327041626\n",
      "step 7680 loss: 2.4371533393859863\n",
      "step 7681 loss: 2.514509916305542\n",
      "step 7682 loss: 2.4278981685638428\n",
      "step 7683 loss: 2.5624196529388428\n",
      "step 7684 loss: 2.5901384353637695\n",
      "step 7685 loss: 2.4581353664398193\n",
      "step 7686 loss: 2.478499412536621\n",
      "step 7687 loss: 2.4415347576141357\n",
      "step 7688 loss: 2.468738555908203\n",
      "step 7689 loss: 2.342158555984497\n",
      "step 7690 loss: 2.4974656105041504\n",
      "step 7691 loss: 2.4191455841064453\n",
      "step 7692 loss: 2.5096466541290283\n",
      "step 7693 loss: 2.4937973022460938\n",
      "step 7694 loss: 2.4475297927856445\n",
      "step 7695 loss: 2.427408218383789\n",
      "step 7696 loss: 2.3467514514923096\n",
      "step 7697 loss: 2.522934675216675\n",
      "step 7698 loss: 2.3822531700134277\n",
      "step 7699 loss: 2.545351028442383\n",
      "step 7700 loss: 2.465223550796509\n",
      "step 7701 loss: 2.51056170463562\n",
      "step 7702 loss: 2.649146556854248\n",
      "step 7703 loss: 2.381103038787842\n",
      "step 7704 loss: 2.5407187938690186\n",
      "step 7705 loss: 2.635899543762207\n",
      "step 7706 loss: 2.491987705230713\n",
      "step 7707 loss: 2.453206777572632\n",
      "step 7708 loss: 2.526926279067993\n",
      "step 7709 loss: 2.385345458984375\n",
      "step 7710 loss: 2.591113567352295\n",
      "step 7711 loss: 2.515749931335449\n",
      "step 7712 loss: 2.3960378170013428\n",
      "step 7713 loss: 2.58257794380188\n",
      "step 7714 loss: 2.5117902755737305\n",
      "step 7715 loss: 2.52359938621521\n",
      "step 7716 loss: 2.4011213779449463\n",
      "step 7717 loss: 2.464289903640747\n",
      "step 7718 loss: 2.477088689804077\n",
      "step 7719 loss: 2.3678369522094727\n",
      "step 7720 loss: 2.594252586364746\n",
      "step 7721 loss: 2.5300018787384033\n",
      "step 7722 loss: 2.502045154571533\n",
      "step 7723 loss: 2.5821948051452637\n",
      "step 7724 loss: 2.6022098064422607\n",
      "step 7725 loss: 2.5146031379699707\n",
      "step 7726 loss: 2.567286729812622\n",
      "step 7727 loss: 2.480640172958374\n",
      "step 7728 loss: 2.4617607593536377\n",
      "step 7729 loss: 2.547830581665039\n",
      "step 7730 loss: 2.4209301471710205\n",
      "step 7731 loss: 2.4929940700531006\n",
      "step 7732 loss: 2.541416645050049\n",
      "step 7733 loss: 2.5138142108917236\n",
      "step 7734 loss: 2.478862762451172\n",
      "step 7735 loss: 2.5849809646606445\n",
      "step 7736 loss: 2.5711488723754883\n",
      "step 7737 loss: 2.5542855262756348\n",
      "step 7738 loss: 2.45571231842041\n",
      "step 7739 loss: 2.4021852016448975\n",
      "step 7740 loss: 2.4233016967773438\n",
      "step 7741 loss: 2.539442777633667\n",
      "step 7742 loss: 2.6350677013397217\n",
      "step 7743 loss: 2.437965154647827\n",
      "step 7744 loss: 2.3957836627960205\n",
      "step 7745 loss: 2.5591037273406982\n",
      "step 7746 loss: 2.5423498153686523\n",
      "step 7747 loss: 2.4605510234832764\n",
      "step 7748 loss: 2.646251678466797\n",
      "step 7749 loss: 2.447197675704956\n",
      "step 7750 loss: 2.4590141773223877\n",
      "step 7751 loss: 2.4370408058166504\n",
      "step 7752 loss: 2.5522143840789795\n",
      "step 7753 loss: 2.5548996925354004\n",
      "step 7754 loss: 2.4616193771362305\n",
      "step 7755 loss: 2.4470558166503906\n",
      "step 7756 loss: 2.4467933177948\n",
      "step 7757 loss: 2.5363337993621826\n",
      "step 7758 loss: 2.4629104137420654\n",
      "step 7759 loss: 2.4577767848968506\n",
      "step 7760 loss: 2.525792121887207\n",
      "step 7761 loss: 2.4420864582061768\n",
      "step 7762 loss: 2.4371914863586426\n",
      "step 7763 loss: 2.4613590240478516\n",
      "step 7764 loss: 2.5009334087371826\n",
      "step 7765 loss: 2.389481544494629\n",
      "step 7766 loss: 2.4495022296905518\n",
      "step 7767 loss: 2.4580423831939697\n",
      "step 7768 loss: 2.4857077598571777\n",
      "step 7769 loss: 2.4850664138793945\n",
      "step 7770 loss: 2.52156400680542\n",
      "step 7771 loss: 2.4787750244140625\n",
      "step 7772 loss: 2.4755825996398926\n",
      "step 7773 loss: 2.485759973526001\n",
      "step 7774 loss: 2.615664005279541\n",
      "step 7775 loss: 2.4959006309509277\n",
      "step 7776 loss: 2.5123589038848877\n",
      "step 7777 loss: 2.5724189281463623\n",
      "step 7778 loss: 2.505187511444092\n",
      "step 7779 loss: 2.502563238143921\n",
      "step 7780 loss: 2.4194228649139404\n",
      "step 7781 loss: 2.5761795043945312\n",
      "step 7782 loss: 2.403697967529297\n",
      "step 7783 loss: 2.508439540863037\n",
      "step 7784 loss: 2.520905017852783\n",
      "step 7785 loss: 2.624926805496216\n",
      "step 7786 loss: 2.4148948192596436\n",
      "step 7787 loss: 2.545450448989868\n",
      "step 7788 loss: 2.4421701431274414\n",
      "step 7789 loss: 2.5045385360717773\n",
      "step 7790 loss: 2.481339693069458\n",
      "step 7791 loss: 2.4952709674835205\n",
      "step 7792 loss: 2.428286075592041\n",
      "step 7793 loss: 2.4475908279418945\n",
      "step 7794 loss: 2.459735870361328\n",
      "step 7795 loss: 2.3924031257629395\n",
      "step 7796 loss: 2.532135486602783\n",
      "step 7797 loss: 2.3936142921447754\n",
      "step 7798 loss: 2.3629837036132812\n",
      "step 7799 loss: 2.5737247467041016\n",
      "step 7800 loss: 2.4884138107299805\n",
      "step 7801 loss: 2.4235596656799316\n",
      "step 7802 loss: 2.5992259979248047\n",
      "step 7803 loss: 2.5236244201660156\n",
      "step 7804 loss: 2.515552043914795\n",
      "step 7805 loss: 2.5532870292663574\n",
      "step 7806 loss: 2.5010673999786377\n",
      "step 7807 loss: 2.441537857055664\n",
      "step 7808 loss: 2.541081428527832\n",
      "step 7809 loss: 2.496119499206543\n",
      "step 7810 loss: 2.5568010807037354\n",
      "step 7811 loss: 2.4067108631134033\n",
      "step 7812 loss: 2.5129618644714355\n",
      "step 7813 loss: 2.493732452392578\n",
      "step 7814 loss: 2.407693862915039\n",
      "step 7815 loss: 2.489036798477173\n",
      "step 7816 loss: 2.567260503768921\n",
      "step 7817 loss: 2.629375457763672\n",
      "step 7818 loss: 2.469848394393921\n",
      "step 7819 loss: 2.4108989238739014\n",
      "step 7820 loss: 2.3930273056030273\n",
      "step 7821 loss: 2.5120959281921387\n",
      "step 7822 loss: 2.6988422870635986\n",
      "step 7823 loss: 2.6078336238861084\n",
      "step 7824 loss: 2.447593927383423\n",
      "step 7825 loss: 2.3815996646881104\n",
      "step 7826 loss: 2.593149185180664\n",
      "step 7827 loss: 2.4599015712738037\n",
      "step 7828 loss: 2.452277898788452\n",
      "step 7829 loss: 2.494281530380249\n",
      "step 7830 loss: 2.4009971618652344\n",
      "step 7831 loss: 2.510368824005127\n",
      "step 7832 loss: 2.473341703414917\n",
      "step 7833 loss: 2.500932216644287\n",
      "step 7834 loss: 2.5332837104797363\n",
      "step 7835 loss: 2.569793462753296\n",
      "step 7836 loss: 2.4865458011627197\n",
      "step 7837 loss: 2.5245211124420166\n",
      "step 7838 loss: 2.5988543033599854\n",
      "step 7839 loss: 2.5213634967803955\n",
      "step 7840 loss: 2.4376120567321777\n",
      "step 7841 loss: 2.3442699909210205\n",
      "step 7842 loss: 2.2933201789855957\n",
      "step 7843 loss: 2.476865530014038\n",
      "step 7844 loss: 2.5090978145599365\n",
      "step 7845 loss: 2.546643018722534\n",
      "step 7846 loss: 2.5377864837646484\n",
      "step 7847 loss: 2.4249675273895264\n",
      "step 7848 loss: 2.4092986583709717\n",
      "step 7849 loss: 2.364736318588257\n",
      "step 7850 loss: 2.523871660232544\n",
      "step 7851 loss: 2.440566301345825\n",
      "step 7852 loss: 2.5178353786468506\n",
      "step 7853 loss: 2.3947813510894775\n",
      "step 7854 loss: 2.412010669708252\n",
      "step 7855 loss: 2.4596669673919678\n",
      "step 7856 loss: 2.429029941558838\n",
      "step 7857 loss: 2.51883602142334\n",
      "step 7858 loss: 2.50319766998291\n",
      "step 7859 loss: 2.5373849868774414\n",
      "step 7860 loss: 2.6260135173797607\n",
      "step 7861 loss: 2.580026865005493\n",
      "step 7862 loss: 2.429581642150879\n",
      "step 7863 loss: 2.5732879638671875\n",
      "step 7864 loss: 2.4495840072631836\n",
      "step 7865 loss: 2.471053123474121\n",
      "step 7866 loss: 2.5624992847442627\n",
      "step 7867 loss: 2.4128708839416504\n",
      "step 7868 loss: 2.3569281101226807\n",
      "step 7869 loss: 2.547924041748047\n",
      "step 7870 loss: 2.3849425315856934\n",
      "step 7871 loss: 2.550611734390259\n",
      "step 7872 loss: 2.5154194831848145\n",
      "step 7873 loss: 2.495448350906372\n",
      "step 7874 loss: 2.363402843475342\n",
      "step 7875 loss: 2.4385664463043213\n",
      "step 7876 loss: 2.4816153049468994\n",
      "step 7877 loss: 2.612079381942749\n",
      "step 7878 loss: 2.49190092086792\n",
      "step 7879 loss: 2.644887924194336\n",
      "step 7880 loss: 2.515695810317993\n",
      "step 7881 loss: 2.4182772636413574\n",
      "step 7882 loss: 2.5088276863098145\n",
      "step 7883 loss: 2.518268585205078\n",
      "step 7884 loss: 2.496795892715454\n",
      "step 7885 loss: 2.418055772781372\n",
      "step 7886 loss: 2.474757432937622\n",
      "step 7887 loss: 2.4444527626037598\n",
      "step 7888 loss: 2.48691463470459\n",
      "step 7889 loss: 2.3643224239349365\n",
      "step 7890 loss: 2.539226531982422\n",
      "step 7891 loss: 2.4975714683532715\n",
      "step 7892 loss: 2.50577712059021\n",
      "step 7893 loss: 2.6172685623168945\n",
      "step 7894 loss: 2.256110429763794\n",
      "step 7895 loss: 2.524143695831299\n",
      "step 7896 loss: 2.528164863586426\n",
      "step 7897 loss: 2.5169849395751953\n",
      "step 7898 loss: 2.4757308959960938\n",
      "step 7899 loss: 2.4945461750030518\n",
      "step 7900 loss: 2.5662143230438232\n",
      "step 7901 loss: 2.4961676597595215\n",
      "step 7902 loss: 2.5523829460144043\n",
      "step 7903 loss: 2.5045294761657715\n",
      "step 7904 loss: 2.6505351066589355\n",
      "step 7905 loss: 2.4760916233062744\n",
      "step 7906 loss: 2.464202642440796\n",
      "step 7907 loss: 2.4003846645355225\n",
      "step 7908 loss: 2.4186182022094727\n",
      "step 7909 loss: 2.4894814491271973\n",
      "step 7910 loss: 2.5298731327056885\n",
      "step 7911 loss: 2.527448892593384\n",
      "step 7912 loss: 2.4299092292785645\n",
      "step 7913 loss: 2.5642266273498535\n",
      "step 7914 loss: 2.3354930877685547\n",
      "step 7915 loss: 2.5079073905944824\n",
      "step 7916 loss: 2.499526023864746\n",
      "step 7917 loss: 2.487460136413574\n",
      "step 7918 loss: 2.451557159423828\n",
      "step 7919 loss: 2.4342119693756104\n",
      "step 7920 loss: 2.48576283454895\n",
      "step 7921 loss: 2.353245735168457\n",
      "step 7922 loss: 2.5053255558013916\n",
      "step 7923 loss: 2.559267997741699\n",
      "step 7924 loss: 2.481908082962036\n",
      "step 7925 loss: 2.4538848400115967\n",
      "step 7926 loss: 2.5597429275512695\n",
      "step 7927 loss: 2.4633309841156006\n",
      "step 7928 loss: 2.2967097759246826\n",
      "step 7929 loss: 2.5614945888519287\n",
      "step 7930 loss: 2.524446964263916\n",
      "step 7931 loss: 2.399151563644409\n",
      "step 7932 loss: 2.4165499210357666\n",
      "step 7933 loss: 2.5487887859344482\n",
      "step 7934 loss: 2.4741218090057373\n",
      "step 7935 loss: 2.5003862380981445\n",
      "step 7936 loss: 2.4375951290130615\n",
      "step 7937 loss: 2.4888017177581787\n",
      "step 7938 loss: 2.490485906600952\n",
      "step 7939 loss: 2.3759067058563232\n",
      "step 7940 loss: 2.4606828689575195\n",
      "step 7941 loss: 2.517800807952881\n",
      "step 7942 loss: 2.432680368423462\n",
      "step 7943 loss: 2.4898650646209717\n",
      "step 7944 loss: 2.4231886863708496\n",
      "step 7945 loss: 2.5757830142974854\n",
      "step 7946 loss: 2.5247042179107666\n",
      "step 7947 loss: 2.5188076496124268\n",
      "step 7948 loss: 2.490723133087158\n",
      "step 7949 loss: 2.6263320446014404\n",
      "step 7950 loss: 2.4170405864715576\n",
      "step 7951 loss: 2.460294246673584\n",
      "step 7952 loss: 2.448293447494507\n",
      "step 7953 loss: 2.531770944595337\n",
      "step 7954 loss: 2.4699254035949707\n",
      "step 7955 loss: 2.625601053237915\n",
      "step 7956 loss: 2.450072765350342\n",
      "step 7957 loss: 2.5288422107696533\n",
      "step 7958 loss: 2.4700448513031006\n",
      "step 7959 loss: 2.512021541595459\n",
      "step 7960 loss: 2.559253692626953\n",
      "step 7961 loss: 2.4016895294189453\n",
      "step 7962 loss: 2.517178535461426\n",
      "step 7963 loss: 2.4588544368743896\n",
      "step 7964 loss: 2.446758508682251\n",
      "step 7965 loss: 2.534027576446533\n",
      "step 7966 loss: 2.577871561050415\n",
      "step 7967 loss: 2.3628015518188477\n",
      "step 7968 loss: 2.566995143890381\n",
      "step 7969 loss: 2.4517953395843506\n",
      "step 7970 loss: 2.5400969982147217\n",
      "step 7971 loss: 2.4752211570739746\n",
      "step 7972 loss: 2.548830270767212\n",
      "step 7973 loss: 2.561269521713257\n",
      "step 7974 loss: 2.4593141078948975\n",
      "step 7975 loss: 2.514922857284546\n",
      "step 7976 loss: 2.6149280071258545\n",
      "step 7977 loss: 2.591419219970703\n",
      "step 7978 loss: 2.463090658187866\n",
      "step 7979 loss: 2.4695522785186768\n",
      "step 7980 loss: 2.384849786758423\n",
      "step 7981 loss: 2.485978841781616\n",
      "step 7982 loss: 2.5952749252319336\n",
      "step 7983 loss: 2.4002528190612793\n",
      "step 7984 loss: 2.4376583099365234\n",
      "step 7985 loss: 2.5909786224365234\n",
      "step 7986 loss: 2.559978485107422\n",
      "step 7987 loss: 2.3816380500793457\n",
      "step 7988 loss: 2.442338705062866\n",
      "step 7989 loss: 2.4839351177215576\n",
      "step 7990 loss: 2.489537000656128\n",
      "step 7991 loss: 2.5588629245758057\n",
      "step 7992 loss: 2.5797693729400635\n",
      "step 7993 loss: 2.4319169521331787\n",
      "step 7994 loss: 2.4986581802368164\n",
      "step 7995 loss: 2.5390970706939697\n",
      "step 7996 loss: 2.5753612518310547\n",
      "step 7997 loss: 2.4706485271453857\n",
      "step 7998 loss: 2.5322084426879883\n",
      "step 7999 loss: 2.4253439903259277\n",
      "step 8000 loss: 2.44449782371521\n",
      "step 8001 loss: 2.435389518737793\n",
      "step 8002 loss: 2.543555498123169\n",
      "step 8003 loss: 2.3574154376983643\n",
      "step 8004 loss: 2.4523186683654785\n",
      "step 8005 loss: 2.4324498176574707\n",
      "step 8006 loss: 2.5176103115081787\n",
      "step 8007 loss: 2.455324649810791\n",
      "step 8008 loss: 2.563446044921875\n",
      "step 8009 loss: 2.5560104846954346\n",
      "step 8010 loss: 2.5979557037353516\n",
      "step 8011 loss: 2.4791226387023926\n",
      "step 8012 loss: 2.5404245853424072\n",
      "step 8013 loss: 2.4255127906799316\n",
      "step 8014 loss: 2.5161125659942627\n",
      "step 8015 loss: 2.4931323528289795\n",
      "step 8016 loss: 2.4549784660339355\n",
      "step 8017 loss: 2.3813254833221436\n",
      "step 8018 loss: 2.4293112754821777\n",
      "step 8019 loss: 2.4575729370117188\n",
      "step 8020 loss: 2.481008529663086\n",
      "step 8021 loss: 2.51898455619812\n",
      "step 8022 loss: 2.526494264602661\n",
      "step 8023 loss: 2.402090311050415\n",
      "step 8024 loss: 2.517416000366211\n",
      "step 8025 loss: 2.361527919769287\n",
      "step 8026 loss: 2.5349011421203613\n",
      "step 8027 loss: 2.3702549934387207\n",
      "step 8028 loss: 2.474586248397827\n",
      "step 8029 loss: 2.5314998626708984\n",
      "step 8030 loss: 2.420836925506592\n",
      "step 8031 loss: 2.4265501499176025\n",
      "step 8032 loss: 2.334144353866577\n",
      "step 8033 loss: 2.3702945709228516\n",
      "step 8034 loss: 2.4050304889678955\n",
      "step 8035 loss: 2.4722824096679688\n",
      "step 8036 loss: 2.4071953296661377\n",
      "step 8037 loss: 2.5468482971191406\n",
      "step 8038 loss: 2.5776491165161133\n",
      "step 8039 loss: 2.502270460128784\n",
      "step 8040 loss: 2.408355951309204\n",
      "step 8041 loss: 2.4769694805145264\n",
      "step 8042 loss: 2.4322314262390137\n",
      "step 8043 loss: 2.546987295150757\n",
      "step 8044 loss: 2.375291109085083\n",
      "step 8045 loss: 2.457197666168213\n",
      "step 8046 loss: 2.56186842918396\n",
      "step 8047 loss: 2.3882386684417725\n",
      "step 8048 loss: 2.3655917644500732\n",
      "step 8049 loss: 2.4873228073120117\n",
      "step 8050 loss: 2.474860191345215\n",
      "step 8051 loss: 2.4546024799346924\n",
      "step 8052 loss: 2.4694371223449707\n",
      "step 8053 loss: 2.463092088699341\n",
      "step 8054 loss: 2.5065548419952393\n",
      "step 8055 loss: 2.508523941040039\n",
      "step 8056 loss: 2.409538745880127\n",
      "step 8057 loss: 2.590924024581909\n",
      "step 8058 loss: 2.39029860496521\n",
      "step 8059 loss: 2.5592286586761475\n",
      "step 8060 loss: 2.380514144897461\n",
      "step 8061 loss: 2.4887537956237793\n",
      "step 8062 loss: 2.265862226486206\n",
      "step 8063 loss: 2.4298739433288574\n",
      "step 8064 loss: 2.4477968215942383\n",
      "step 8065 loss: 2.5545105934143066\n",
      "step 8066 loss: 2.4727931022644043\n",
      "step 8067 loss: 2.526085615158081\n",
      "step 8068 loss: 2.3544974327087402\n",
      "step 8069 loss: 2.4216020107269287\n",
      "step 8070 loss: 2.4489986896514893\n",
      "step 8071 loss: 2.458690643310547\n",
      "step 8072 loss: 2.5785653591156006\n",
      "step 8073 loss: 2.465848684310913\n",
      "step 8074 loss: 2.3851194381713867\n",
      "step 8075 loss: 2.3516440391540527\n",
      "step 8076 loss: 2.5981128215789795\n",
      "step 8077 loss: 2.4984493255615234\n",
      "step 8078 loss: 2.4776132106781006\n",
      "step 8079 loss: 2.4105241298675537\n",
      "step 8080 loss: 2.4956390857696533\n",
      "step 8081 loss: 2.6631882190704346\n",
      "step 8082 loss: 2.455728054046631\n",
      "step 8083 loss: 2.537144422531128\n",
      "step 8084 loss: 2.5640037059783936\n",
      "step 8085 loss: 2.5154573917388916\n",
      "step 8086 loss: 2.586721420288086\n",
      "step 8087 loss: 2.3156635761260986\n",
      "step 8088 loss: 2.4177796840667725\n",
      "step 8089 loss: 2.4995312690734863\n",
      "step 8090 loss: 2.451992988586426\n",
      "step 8091 loss: 2.5627825260162354\n",
      "step 8092 loss: 2.436718702316284\n",
      "step 8093 loss: 2.512023448944092\n",
      "step 8094 loss: 2.4523231983184814\n",
      "step 8095 loss: 2.543973922729492\n",
      "step 8096 loss: 2.4698286056518555\n",
      "step 8097 loss: 2.430589437484741\n",
      "step 8098 loss: 2.527833938598633\n",
      "step 8099 loss: 2.415501832962036\n",
      "step 8100 loss: 2.5472488403320312\n",
      "step 8101 loss: 2.5590975284576416\n",
      "step 8102 loss: 2.5755021572113037\n",
      "step 8103 loss: 2.488985776901245\n",
      "step 8104 loss: 2.4526734352111816\n",
      "step 8105 loss: 2.4619083404541016\n",
      "step 8106 loss: 2.5432591438293457\n",
      "step 8107 loss: 2.515410900115967\n",
      "step 8108 loss: 2.409696102142334\n",
      "step 8109 loss: 2.492253303527832\n",
      "step 8110 loss: 2.4694855213165283\n",
      "step 8111 loss: 2.519416332244873\n",
      "step 8112 loss: 2.551140069961548\n",
      "step 8113 loss: 2.6388792991638184\n",
      "step 8114 loss: 2.5001707077026367\n",
      "step 8115 loss: 2.4532744884490967\n",
      "step 8116 loss: 2.430929660797119\n",
      "step 8117 loss: 2.5389113426208496\n",
      "step 8118 loss: 2.4129559993743896\n",
      "step 8119 loss: 2.499206781387329\n",
      "step 8120 loss: 2.455786943435669\n",
      "step 8121 loss: 2.437987804412842\n",
      "step 8122 loss: 2.482884645462036\n",
      "step 8123 loss: 2.4660418033599854\n",
      "step 8124 loss: 2.301964282989502\n",
      "step 8125 loss: 2.5436744689941406\n",
      "step 8126 loss: 2.490565776824951\n",
      "step 8127 loss: 2.4313759803771973\n",
      "step 8128 loss: 2.5599324703216553\n",
      "step 8129 loss: 2.6579322814941406\n",
      "step 8130 loss: 2.5901994705200195\n",
      "step 8131 loss: 2.496307611465454\n",
      "step 8132 loss: 2.394923686981201\n",
      "step 8133 loss: 2.3907055854797363\n",
      "step 8134 loss: 2.488377571105957\n",
      "step 8135 loss: 2.6122097969055176\n",
      "step 8136 loss: 2.518118381500244\n",
      "step 8137 loss: 2.5303709506988525\n",
      "step 8138 loss: 2.4923975467681885\n",
      "step 8139 loss: 2.5096514225006104\n",
      "step 8140 loss: 2.412315607070923\n",
      "step 8141 loss: 2.4484026432037354\n",
      "step 8142 loss: 2.499439239501953\n",
      "step 8143 loss: 2.4373490810394287\n",
      "step 8144 loss: 2.4243969917297363\n",
      "step 8145 loss: 2.540884494781494\n",
      "step 8146 loss: 2.456446409225464\n",
      "step 8147 loss: 2.409531831741333\n",
      "step 8148 loss: 2.463449001312256\n",
      "step 8149 loss: 2.4079248905181885\n",
      "step 8150 loss: 2.502323627471924\n",
      "step 8151 loss: 2.4907162189483643\n",
      "step 8152 loss: 2.476346015930176\n",
      "step 8153 loss: 2.613553047180176\n",
      "step 8154 loss: 2.5161819458007812\n",
      "step 8155 loss: 2.43900990486145\n",
      "step 8156 loss: 2.4900739192962646\n",
      "step 8157 loss: 2.4034337997436523\n",
      "step 8158 loss: 2.5680463314056396\n",
      "step 8159 loss: 2.365424394607544\n",
      "step 8160 loss: 2.409951686859131\n",
      "step 8161 loss: 2.5000343322753906\n",
      "step 8162 loss: 2.5975630283355713\n",
      "step 8163 loss: 2.4835450649261475\n",
      "step 8164 loss: 2.3302197456359863\n",
      "step 8165 loss: 2.4478256702423096\n",
      "step 8166 loss: 2.354257822036743\n",
      "step 8167 loss: 2.4645135402679443\n",
      "step 8168 loss: 2.518961191177368\n",
      "step 8169 loss: 2.4821701049804688\n",
      "step 8170 loss: 2.5498712062835693\n",
      "step 8171 loss: 2.3822832107543945\n",
      "step 8172 loss: 2.530116558074951\n",
      "step 8173 loss: 2.4404399394989014\n",
      "step 8174 loss: 2.4755704402923584\n",
      "step 8175 loss: 2.557454824447632\n",
      "step 8176 loss: 2.505572557449341\n",
      "step 8177 loss: 2.41418719291687\n",
      "step 8178 loss: 2.520454168319702\n",
      "step 8179 loss: 2.453732967376709\n",
      "step 8180 loss: 2.4622886180877686\n",
      "step 8181 loss: 2.5579330921173096\n",
      "step 8182 loss: 2.4549593925476074\n",
      "step 8183 loss: 2.481891393661499\n",
      "step 8184 loss: 2.4431142807006836\n",
      "step 8185 loss: 2.357970714569092\n",
      "step 8186 loss: 2.471017360687256\n",
      "step 8187 loss: 2.463460683822632\n",
      "step 8188 loss: 2.441298007965088\n",
      "step 8189 loss: 2.5702710151672363\n",
      "step 8190 loss: 2.438692569732666\n",
      "step 8191 loss: 2.4586329460144043\n",
      "step 8192 loss: 2.469306468963623\n",
      "step 8193 loss: 2.4525058269500732\n",
      "step 8194 loss: 2.573021173477173\n",
      "step 8195 loss: 2.4605631828308105\n",
      "step 8196 loss: 2.457803726196289\n",
      "step 8197 loss: 2.4408535957336426\n",
      "step 8198 loss: 2.5220415592193604\n",
      "step 8199 loss: 2.4564836025238037\n",
      "step 8200 loss: 2.4675867557525635\n",
      "step 8201 loss: 2.5339956283569336\n",
      "step 8202 loss: 2.621551036834717\n",
      "step 8203 loss: 2.448259115219116\n",
      "step 8204 loss: 2.480936288833618\n",
      "step 8205 loss: 2.5009267330169678\n",
      "step 8206 loss: 2.380303144454956\n",
      "step 8207 loss: 2.499534845352173\n",
      "step 8208 loss: 2.4553537368774414\n",
      "step 8209 loss: 2.5483171939849854\n",
      "step 8210 loss: 2.500459671020508\n",
      "step 8211 loss: 2.3634114265441895\n",
      "step 8212 loss: 2.443803071975708\n",
      "step 8213 loss: 2.4735116958618164\n",
      "step 8214 loss: 2.4223384857177734\n",
      "step 8215 loss: 2.626405715942383\n",
      "step 8216 loss: 2.559792995452881\n",
      "step 8217 loss: 2.4779586791992188\n",
      "step 8218 loss: 2.5188212394714355\n",
      "step 8219 loss: 2.6077866554260254\n",
      "step 8220 loss: 2.630239248275757\n",
      "step 8221 loss: 2.3523497581481934\n",
      "step 8222 loss: 2.591978073120117\n",
      "step 8223 loss: 2.458550214767456\n",
      "step 8224 loss: 2.4922313690185547\n",
      "step 8225 loss: 2.4848382472991943\n",
      "step 8226 loss: 2.5086498260498047\n",
      "step 8227 loss: 2.428760290145874\n",
      "step 8228 loss: 2.476320266723633\n",
      "step 8229 loss: 2.4554929733276367\n",
      "step 8230 loss: 2.4172627925872803\n",
      "step 8231 loss: 2.418762445449829\n",
      "step 8232 loss: 2.527923822402954\n",
      "step 8233 loss: 2.546017646789551\n",
      "step 8234 loss: 2.3937313556671143\n",
      "step 8235 loss: 2.458036422729492\n",
      "step 8236 loss: 2.3937485218048096\n",
      "step 8237 loss: 2.3499715328216553\n",
      "step 8238 loss: 2.396587371826172\n",
      "step 8239 loss: 2.3999276161193848\n",
      "step 8240 loss: 2.511756181716919\n",
      "step 8241 loss: 2.6544482707977295\n",
      "step 8242 loss: 2.3614652156829834\n",
      "step 8243 loss: 2.519726514816284\n",
      "step 8244 loss: 2.5252490043640137\n",
      "step 8245 loss: 2.417133092880249\n",
      "step 8246 loss: 2.530493974685669\n",
      "step 8247 loss: 2.519987106323242\n",
      "step 8248 loss: 2.6098134517669678\n",
      "step 8249 loss: 2.360199451446533\n",
      "step 8250 loss: 2.366025686264038\n",
      "step 8251 loss: 2.510911226272583\n",
      "step 8252 loss: 2.6730215549468994\n",
      "step 8253 loss: 2.4747111797332764\n",
      "step 8254 loss: 2.5153961181640625\n",
      "step 8255 loss: 2.490746021270752\n",
      "step 8256 loss: 2.5325112342834473\n",
      "step 8257 loss: 2.6179797649383545\n",
      "step 8258 loss: 2.3639883995056152\n",
      "step 8259 loss: 2.487668037414551\n",
      "step 8260 loss: 2.4752297401428223\n",
      "step 8261 loss: 2.585014581680298\n",
      "step 8262 loss: 2.3850622177124023\n",
      "step 8263 loss: 2.408672571182251\n",
      "step 8264 loss: 2.5122482776641846\n",
      "step 8265 loss: 2.4974074363708496\n",
      "step 8266 loss: 2.331256628036499\n",
      "step 8267 loss: 2.4960196018218994\n",
      "step 8268 loss: 2.394940137863159\n",
      "step 8269 loss: 2.503499746322632\n",
      "step 8270 loss: 2.494401216506958\n",
      "step 8271 loss: 2.371856451034546\n",
      "step 8272 loss: 2.5335605144500732\n",
      "step 8273 loss: 2.438337802886963\n",
      "step 8274 loss: 2.5413942337036133\n",
      "step 8275 loss: 2.633147954940796\n",
      "step 8276 loss: 2.4502735137939453\n",
      "step 8277 loss: 2.5188755989074707\n",
      "step 8278 loss: 2.477382183074951\n",
      "step 8279 loss: 2.459779739379883\n",
      "step 8280 loss: 2.46933913230896\n",
      "step 8281 loss: 2.595761299133301\n",
      "step 8282 loss: 2.457348108291626\n",
      "step 8283 loss: 2.4076128005981445\n",
      "step 8284 loss: 2.5224668979644775\n",
      "step 8285 loss: 2.417092800140381\n",
      "step 8286 loss: 2.404066801071167\n",
      "step 8287 loss: 2.4984402656555176\n",
      "step 8288 loss: 2.582709550857544\n",
      "step 8289 loss: 2.3176374435424805\n",
      "step 8290 loss: 2.5126497745513916\n",
      "step 8291 loss: 2.4605419635772705\n",
      "step 8292 loss: 2.479367733001709\n",
      "step 8293 loss: 2.466919183731079\n",
      "step 8294 loss: 2.545577049255371\n",
      "step 8295 loss: 2.4008708000183105\n",
      "step 8296 loss: 2.5046188831329346\n",
      "step 8297 loss: 2.4642133712768555\n",
      "step 8298 loss: 2.449965476989746\n",
      "step 8299 loss: 2.471540927886963\n",
      "step 8300 loss: 2.5351293087005615\n",
      "step 8301 loss: 2.44116473197937\n",
      "step 8302 loss: 2.5016534328460693\n",
      "step 8303 loss: 2.3580574989318848\n",
      "step 8304 loss: 2.463747262954712\n",
      "step 8305 loss: 2.4462978839874268\n",
      "step 8306 loss: 2.4429945945739746\n",
      "step 8307 loss: 2.509068012237549\n",
      "step 8308 loss: 2.555732488632202\n",
      "step 8309 loss: 2.4790709018707275\n",
      "step 8310 loss: 2.458698272705078\n",
      "step 8311 loss: 2.4865095615386963\n",
      "step 8312 loss: 2.468716859817505\n",
      "step 8313 loss: 2.4895260334014893\n",
      "step 8314 loss: 2.4834935665130615\n",
      "step 8315 loss: 2.4693799018859863\n",
      "step 8316 loss: 2.32470703125\n",
      "step 8317 loss: 2.411943197250366\n",
      "step 8318 loss: 2.580749273300171\n",
      "step 8319 loss: 2.5895345211029053\n",
      "step 8320 loss: 2.406343936920166\n",
      "step 8321 loss: 2.5714480876922607\n",
      "step 8322 loss: 2.3873088359832764\n",
      "step 8323 loss: 2.5395121574401855\n",
      "step 8324 loss: 2.455479145050049\n",
      "step 8325 loss: 2.420659303665161\n",
      "step 8326 loss: 2.4726123809814453\n",
      "step 8327 loss: 2.448488235473633\n",
      "step 8328 loss: 2.4609405994415283\n",
      "step 8329 loss: 2.5264673233032227\n",
      "step 8330 loss: 2.591094493865967\n",
      "step 8331 loss: 2.453354835510254\n",
      "step 8332 loss: 2.570374011993408\n",
      "step 8333 loss: 2.4901247024536133\n",
      "step 8334 loss: 2.4340152740478516\n",
      "step 8335 loss: 2.4501633644104004\n",
      "step 8336 loss: 2.54483699798584\n",
      "step 8337 loss: 2.3856823444366455\n",
      "step 8338 loss: 2.4685299396514893\n",
      "step 8339 loss: 2.51346755027771\n",
      "step 8340 loss: 2.473517656326294\n",
      "step 8341 loss: 2.478560209274292\n",
      "step 8342 loss: 2.53212571144104\n",
      "step 8343 loss: 2.473471164703369\n",
      "step 8344 loss: 2.4277355670928955\n",
      "step 8345 loss: 2.4689536094665527\n",
      "step 8346 loss: 2.5207042694091797\n",
      "step 8347 loss: 2.4918978214263916\n",
      "step 8348 loss: 2.4116673469543457\n",
      "step 8349 loss: 2.563317060470581\n",
      "step 8350 loss: 2.491577625274658\n",
      "step 8351 loss: 2.48856520652771\n",
      "step 8352 loss: 2.4675371646881104\n",
      "step 8353 loss: 2.4272656440734863\n",
      "step 8354 loss: 2.5249102115631104\n",
      "step 8355 loss: 2.5030910968780518\n",
      "step 8356 loss: 2.414552688598633\n",
      "step 8357 loss: 2.5236330032348633\n",
      "step 8358 loss: 2.5202882289886475\n",
      "step 8359 loss: 2.4117681980133057\n",
      "step 8360 loss: 2.4104771614074707\n",
      "step 8361 loss: 2.586681604385376\n",
      "step 8362 loss: 2.5156259536743164\n",
      "step 8363 loss: 2.4060168266296387\n",
      "step 8364 loss: 2.510063886642456\n",
      "step 8365 loss: 2.434356927871704\n",
      "step 8366 loss: 2.5878491401672363\n",
      "step 8367 loss: 2.6080355644226074\n",
      "step 8368 loss: 2.4971811771392822\n",
      "step 8369 loss: 2.5193169116973877\n",
      "step 8370 loss: 2.435171365737915\n",
      "step 8371 loss: 2.471757173538208\n",
      "step 8372 loss: 2.3838164806365967\n",
      "step 8373 loss: 2.509707450866699\n",
      "step 8374 loss: 2.4190614223480225\n",
      "step 8375 loss: 2.435420036315918\n",
      "step 8376 loss: 2.4717330932617188\n",
      "step 8377 loss: 2.427441120147705\n",
      "step 8378 loss: 2.5584068298339844\n",
      "step 8379 loss: 2.683612585067749\n",
      "step 8380 loss: 2.4284303188323975\n",
      "step 8381 loss: 2.644346237182617\n",
      "step 8382 loss: 2.4633381366729736\n",
      "step 8383 loss: 2.447174072265625\n",
      "step 8384 loss: 2.580044746398926\n",
      "step 8385 loss: 2.597346544265747\n",
      "step 8386 loss: 2.547969102859497\n",
      "step 8387 loss: 2.4776787757873535\n",
      "step 8388 loss: 2.4660325050354004\n",
      "step 8389 loss: 2.5162277221679688\n",
      "step 8390 loss: 2.4335193634033203\n",
      "step 8391 loss: 2.3911192417144775\n",
      "step 8392 loss: 2.5094387531280518\n",
      "step 8393 loss: 2.5424249172210693\n",
      "step 8394 loss: 2.4790334701538086\n",
      "step 8395 loss: 2.523806571960449\n",
      "step 8396 loss: 2.5121421813964844\n",
      "step 8397 loss: 2.6674931049346924\n",
      "step 8398 loss: 2.4999403953552246\n",
      "step 8399 loss: 2.5712170600891113\n",
      "step 8400 loss: 2.3684194087982178\n",
      "step 8401 loss: 2.542600631713867\n",
      "step 8402 loss: 2.5410311222076416\n",
      "step 8403 loss: 2.41271710395813\n",
      "step 8404 loss: 2.4253947734832764\n",
      "step 8405 loss: 2.5647149085998535\n",
      "step 8406 loss: 2.4746294021606445\n",
      "step 8407 loss: 2.4119925498962402\n",
      "step 8408 loss: 2.5357258319854736\n",
      "step 8409 loss: 2.4362688064575195\n",
      "step 8410 loss: 2.356633424758911\n",
      "step 8411 loss: 2.5066020488739014\n",
      "step 8412 loss: 2.5165047645568848\n",
      "step 8413 loss: 2.4630465507507324\n",
      "step 8414 loss: 2.419555425643921\n",
      "step 8415 loss: 2.4727132320404053\n",
      "step 8416 loss: 2.543076753616333\n",
      "step 8417 loss: 2.4122936725616455\n",
      "step 8418 loss: 2.428180456161499\n",
      "step 8419 loss: 2.600395917892456\n",
      "step 8420 loss: 2.5611703395843506\n",
      "step 8421 loss: 2.553095817565918\n",
      "step 8422 loss: 2.4070487022399902\n",
      "step 8423 loss: 2.398831844329834\n",
      "step 8424 loss: 2.6601920127868652\n",
      "step 8425 loss: 2.42627215385437\n",
      "step 8426 loss: 2.494736433029175\n",
      "step 8427 loss: 2.456986427307129\n",
      "step 8428 loss: 2.5159475803375244\n",
      "step 8429 loss: 2.6145477294921875\n",
      "step 8430 loss: 2.395592212677002\n",
      "step 8431 loss: 2.5223095417022705\n",
      "step 8432 loss: 2.4714102745056152\n",
      "step 8433 loss: 2.442601203918457\n",
      "step 8434 loss: 2.4034717082977295\n",
      "step 8435 loss: 2.515429735183716\n",
      "step 8436 loss: 2.3908138275146484\n",
      "step 8437 loss: 2.598971366882324\n",
      "step 8438 loss: 2.427753210067749\n",
      "step 8439 loss: 2.619443893432617\n",
      "step 8440 loss: 2.512791395187378\n",
      "step 8441 loss: 2.4430155754089355\n",
      "step 8442 loss: 2.51289963722229\n",
      "step 8443 loss: 2.4254167079925537\n",
      "step 8444 loss: 2.531282663345337\n",
      "step 8445 loss: 2.3700852394104004\n",
      "step 8446 loss: 2.4888651371002197\n",
      "step 8447 loss: 2.509913921356201\n",
      "step 8448 loss: 2.466052293777466\n",
      "step 8449 loss: 2.485593557357788\n",
      "step 8450 loss: 2.3720219135284424\n",
      "step 8451 loss: 2.4779129028320312\n",
      "step 8452 loss: 2.3782501220703125\n",
      "step 8453 loss: 2.4900195598602295\n",
      "step 8454 loss: 2.3358521461486816\n",
      "step 8455 loss: 2.338641405105591\n",
      "step 8456 loss: 2.4436237812042236\n",
      "step 8457 loss: 2.5055155754089355\n",
      "step 8458 loss: 2.481157064437866\n",
      "step 8459 loss: 2.588047981262207\n",
      "step 8460 loss: 2.419830560684204\n",
      "step 8461 loss: 2.431288242340088\n",
      "step 8462 loss: 2.3949925899505615\n",
      "step 8463 loss: 2.365865707397461\n",
      "step 8464 loss: 2.4405221939086914\n",
      "step 8465 loss: 2.5435538291931152\n",
      "step 8466 loss: 2.5115480422973633\n",
      "step 8467 loss: 2.4424335956573486\n",
      "step 8468 loss: 2.52152156829834\n",
      "step 8469 loss: 2.4648892879486084\n",
      "step 8470 loss: 2.468266725540161\n",
      "step 8471 loss: 2.471745252609253\n",
      "step 8472 loss: 2.521832227706909\n",
      "step 8473 loss: 2.466585159301758\n",
      "step 8474 loss: 2.370098352432251\n",
      "step 8475 loss: 2.443079948425293\n",
      "step 8476 loss: 2.572190046310425\n",
      "step 8477 loss: 2.6317646503448486\n",
      "step 8478 loss: 2.4331960678100586\n",
      "step 8479 loss: 2.5257041454315186\n",
      "step 8480 loss: 2.419975519180298\n",
      "step 8481 loss: 2.469604969024658\n",
      "step 8482 loss: 2.4811668395996094\n",
      "step 8483 loss: 2.5287587642669678\n",
      "step 8484 loss: 2.5131425857543945\n",
      "step 8485 loss: 2.4982011318206787\n",
      "step 8486 loss: 2.5191376209259033\n",
      "step 8487 loss: 2.536137580871582\n",
      "step 8488 loss: 2.379037380218506\n",
      "step 8489 loss: 2.4823648929595947\n",
      "step 8490 loss: 2.5925850868225098\n",
      "step 8491 loss: 2.6680264472961426\n",
      "step 8492 loss: 2.3796064853668213\n",
      "step 8493 loss: 2.5507678985595703\n",
      "step 8494 loss: 2.4381134510040283\n",
      "step 8495 loss: 2.587791919708252\n",
      "step 8496 loss: 2.398097038269043\n",
      "step 8497 loss: 2.358022689819336\n",
      "step 8498 loss: 2.4946393966674805\n",
      "step 8499 loss: 2.3889193534851074\n",
      "step 8500 loss: 2.417296886444092\n",
      "step 8501 loss: 2.4726831912994385\n",
      "step 8502 loss: 2.5757689476013184\n",
      "step 8503 loss: 2.5483016967773438\n",
      "step 8504 loss: 2.5442116260528564\n",
      "step 8505 loss: 2.5525848865509033\n",
      "step 8506 loss: 2.521582841873169\n",
      "step 8507 loss: 2.2656028270721436\n",
      "step 8508 loss: 2.5032763481140137\n",
      "step 8509 loss: 2.4144744873046875\n",
      "step 8510 loss: 2.3896396160125732\n",
      "step 8511 loss: 2.422125816345215\n",
      "step 8512 loss: 2.53019642829895\n",
      "step 8513 loss: 2.558957576751709\n",
      "step 8514 loss: 2.5431063175201416\n",
      "step 8515 loss: 2.4873173236846924\n",
      "step 8516 loss: 2.5606517791748047\n",
      "step 8517 loss: 2.5002708435058594\n",
      "step 8518 loss: 2.439767599105835\n",
      "step 8519 loss: 2.471461772918701\n",
      "step 8520 loss: 2.4644479751586914\n",
      "step 8521 loss: 2.468672752380371\n",
      "step 8522 loss: 2.4884772300720215\n",
      "step 8523 loss: 2.4533674716949463\n",
      "step 8524 loss: 2.497678279876709\n",
      "step 8525 loss: 2.4480812549591064\n",
      "step 8526 loss: 2.526803731918335\n",
      "step 8527 loss: 2.478020429611206\n",
      "step 8528 loss: 2.5548818111419678\n",
      "step 8529 loss: 2.495037794113159\n",
      "step 8530 loss: 2.4476747512817383\n",
      "step 8531 loss: 2.4911630153656006\n",
      "step 8532 loss: 2.404463052749634\n",
      "step 8533 loss: 2.3985469341278076\n",
      "step 8534 loss: 2.4784200191497803\n",
      "step 8535 loss: 2.510697603225708\n",
      "step 8536 loss: 2.4128217697143555\n",
      "step 8537 loss: 2.432899236679077\n",
      "step 8538 loss: 2.581648349761963\n",
      "step 8539 loss: 2.413015604019165\n",
      "step 8540 loss: 2.527941942214966\n",
      "step 8541 loss: 2.397165536880493\n",
      "step 8542 loss: 2.442230701446533\n",
      "step 8543 loss: 2.420074224472046\n",
      "step 8544 loss: 2.520782947540283\n",
      "step 8545 loss: 2.5264852046966553\n",
      "step 8546 loss: 2.6799957752227783\n",
      "step 8547 loss: 2.6154215335845947\n",
      "step 8548 loss: 2.457921028137207\n",
      "step 8549 loss: 2.4703009128570557\n",
      "step 8550 loss: 2.384593963623047\n",
      "step 8551 loss: 2.5447998046875\n",
      "step 8552 loss: 2.500096321105957\n",
      "step 8553 loss: 2.4632840156555176\n",
      "step 8554 loss: 2.4048445224761963\n",
      "step 8555 loss: 2.531710147857666\n",
      "step 8556 loss: 2.5102505683898926\n",
      "step 8557 loss: 2.4488365650177\n",
      "step 8558 loss: 2.4307243824005127\n",
      "step 8559 loss: 2.50053334236145\n",
      "step 8560 loss: 2.636493444442749\n",
      "step 8561 loss: 2.4022953510284424\n",
      "step 8562 loss: 2.4009206295013428\n",
      "step 8563 loss: 2.4644124507904053\n",
      "step 8564 loss: 2.4969849586486816\n",
      "step 8565 loss: 2.4499664306640625\n",
      "step 8566 loss: 2.402545928955078\n",
      "step 8567 loss: 2.4566781520843506\n",
      "step 8568 loss: 2.4329309463500977\n",
      "step 8569 loss: 2.4263837337493896\n",
      "step 8570 loss: 2.3912405967712402\n",
      "step 8571 loss: 2.4455692768096924\n",
      "step 8572 loss: 2.4241085052490234\n",
      "step 8573 loss: 2.551518201828003\n",
      "step 8574 loss: 2.512552261352539\n",
      "step 8575 loss: 2.4878759384155273\n",
      "step 8576 loss: 2.397948980331421\n",
      "step 8577 loss: 2.3903799057006836\n",
      "step 8578 loss: 2.407289743423462\n",
      "step 8579 loss: 2.3729660511016846\n",
      "step 8580 loss: 2.4717092514038086\n",
      "step 8581 loss: 2.6281139850616455\n",
      "step 8582 loss: 2.3908770084381104\n",
      "step 8583 loss: 2.4578375816345215\n",
      "step 8584 loss: 2.4924066066741943\n",
      "step 8585 loss: 2.5656256675720215\n",
      "step 8586 loss: 2.5396628379821777\n",
      "step 8587 loss: 2.669271230697632\n",
      "step 8588 loss: 2.497262477874756\n",
      "step 8589 loss: 2.4594063758850098\n",
      "step 8590 loss: 2.4610962867736816\n",
      "step 8591 loss: 2.5159354209899902\n",
      "step 8592 loss: 2.4802424907684326\n",
      "step 8593 loss: 2.467424154281616\n",
      "step 8594 loss: 2.424487352371216\n",
      "step 8595 loss: 2.3080289363861084\n",
      "step 8596 loss: 2.40659761428833\n",
      "step 8597 loss: 2.5032122135162354\n",
      "step 8598 loss: 2.3718152046203613\n",
      "step 8599 loss: 2.5462377071380615\n",
      "step 8600 loss: 2.5745158195495605\n",
      "step 8601 loss: 2.399885892868042\n",
      "step 8602 loss: 2.4647250175476074\n",
      "step 8603 loss: 2.532038927078247\n",
      "step 8604 loss: 2.4966890811920166\n",
      "step 8605 loss: 2.5609655380249023\n",
      "step 8606 loss: 2.379638195037842\n",
      "step 8607 loss: 2.4267232418060303\n",
      "step 8608 loss: 2.439467668533325\n",
      "step 8609 loss: 2.379018545150757\n",
      "step 8610 loss: 2.5722861289978027\n",
      "step 8611 loss: 2.4479079246520996\n",
      "step 8612 loss: 2.4012181758880615\n",
      "step 8613 loss: 2.4783482551574707\n",
      "step 8614 loss: 2.302882432937622\n",
      "step 8615 loss: 2.411499261856079\n",
      "step 8616 loss: 2.425609827041626\n",
      "step 8617 loss: 2.4522132873535156\n",
      "step 8618 loss: 2.4568734169006348\n",
      "step 8619 loss: 2.479945421218872\n",
      "step 8620 loss: 2.4822418689727783\n",
      "step 8621 loss: 2.5105934143066406\n",
      "step 8622 loss: 2.4145023822784424\n",
      "step 8623 loss: 2.4852206707000732\n",
      "step 8624 loss: 2.3990318775177\n",
      "step 8625 loss: 2.5252225399017334\n",
      "step 8626 loss: 2.5224337577819824\n",
      "step 8627 loss: 2.3164467811584473\n",
      "step 8628 loss: 2.445484161376953\n",
      "step 8629 loss: 2.4023730754852295\n",
      "step 8630 loss: 2.4741053581237793\n",
      "step 8631 loss: 2.4533956050872803\n",
      "step 8632 loss: 2.474010467529297\n",
      "step 8633 loss: 2.5075278282165527\n",
      "step 8634 loss: 2.5547120571136475\n",
      "step 8635 loss: 2.4486992359161377\n",
      "step 8636 loss: 2.5019149780273438\n",
      "step 8637 loss: 2.462885618209839\n",
      "step 8638 loss: 2.5207014083862305\n",
      "step 8639 loss: 2.4549105167388916\n",
      "step 8640 loss: 2.583989381790161\n",
      "step 8641 loss: 2.476829767227173\n",
      "step 8642 loss: 2.4947924613952637\n",
      "step 8643 loss: 2.435086488723755\n",
      "step 8644 loss: 2.4995274543762207\n",
      "step 8645 loss: 2.436785936355591\n",
      "step 8646 loss: 2.3493738174438477\n",
      "step 8647 loss: 2.4329020977020264\n",
      "step 8648 loss: 2.510753870010376\n",
      "step 8649 loss: 2.503959894180298\n",
      "step 8650 loss: 2.49194598197937\n",
      "step 8651 loss: 2.4976212978363037\n",
      "step 8652 loss: 2.489272117614746\n",
      "step 8653 loss: 2.4642059803009033\n",
      "step 8654 loss: 2.4748165607452393\n",
      "step 8655 loss: 2.512650489807129\n",
      "step 8656 loss: 2.518733501434326\n",
      "step 8657 loss: 2.398024320602417\n",
      "step 8658 loss: 2.4345204830169678\n",
      "step 8659 loss: 2.488675117492676\n",
      "step 8660 loss: 2.4042587280273438\n",
      "step 8661 loss: 2.4750630855560303\n",
      "step 8662 loss: 2.6756834983825684\n",
      "step 8663 loss: 2.519571542739868\n",
      "step 8664 loss: 2.516010284423828\n",
      "step 8665 loss: 2.54364275932312\n",
      "step 8666 loss: 2.410580635070801\n",
      "step 8667 loss: 2.359703779220581\n",
      "step 8668 loss: 2.4587795734405518\n",
      "step 8669 loss: 2.4714462757110596\n",
      "step 8670 loss: 2.396340847015381\n",
      "step 8671 loss: 2.5067923069000244\n",
      "step 8672 loss: 2.4797472953796387\n",
      "step 8673 loss: 2.4730923175811768\n",
      "step 8674 loss: 2.500046730041504\n",
      "step 8675 loss: 2.5339372158050537\n",
      "step 8676 loss: 2.4133801460266113\n",
      "step 8677 loss: 2.575453758239746\n",
      "step 8678 loss: 2.4805397987365723\n",
      "step 8679 loss: 2.4933416843414307\n",
      "step 8680 loss: 2.4236114025115967\n",
      "step 8681 loss: 2.436948537826538\n",
      "step 8682 loss: 2.505286931991577\n",
      "step 8683 loss: 2.4238953590393066\n",
      "step 8684 loss: 2.453012466430664\n",
      "step 8685 loss: 2.4325177669525146\n",
      "step 8686 loss: 2.434840679168701\n",
      "step 8687 loss: 2.521383047103882\n",
      "step 8688 loss: 2.413461685180664\n",
      "step 8689 loss: 2.4013917446136475\n",
      "step 8690 loss: 2.57568359375\n",
      "step 8691 loss: 2.474764823913574\n",
      "step 8692 loss: 2.487487316131592\n",
      "step 8693 loss: 2.5536351203918457\n",
      "step 8694 loss: 2.56201171875\n",
      "step 8695 loss: 2.436187267303467\n",
      "step 8696 loss: 2.579876184463501\n",
      "step 8697 loss: 2.482959747314453\n",
      "step 8698 loss: 2.503183126449585\n",
      "step 8699 loss: 2.488495349884033\n",
      "step 8700 loss: 2.4143502712249756\n",
      "step 8701 loss: 2.4612479209899902\n",
      "step 8702 loss: 2.462507486343384\n",
      "step 8703 loss: 2.5941741466522217\n",
      "step 8704 loss: 2.4588024616241455\n",
      "step 8705 loss: 2.6131696701049805\n",
      "step 8706 loss: 2.465508222579956\n",
      "step 8707 loss: 2.5417184829711914\n",
      "step 8708 loss: 2.415006160736084\n",
      "step 8709 loss: 2.5208795070648193\n",
      "step 8710 loss: 2.6128621101379395\n",
      "step 8711 loss: 2.46639084815979\n",
      "step 8712 loss: 2.4284379482269287\n",
      "step 8713 loss: 2.5287418365478516\n",
      "step 8714 loss: 2.4768431186676025\n",
      "step 8715 loss: 2.5689914226531982\n",
      "step 8716 loss: 2.5009653568267822\n",
      "step 8717 loss: 2.479578971862793\n",
      "step 8718 loss: 2.4659314155578613\n",
      "step 8719 loss: 2.5270884037017822\n",
      "step 8720 loss: 2.5803751945495605\n",
      "step 8721 loss: 2.486781597137451\n",
      "step 8722 loss: 2.509165048599243\n",
      "step 8723 loss: 2.457266330718994\n",
      "step 8724 loss: 2.4272983074188232\n",
      "step 8725 loss: 2.4888997077941895\n",
      "step 8726 loss: 2.55899715423584\n",
      "step 8727 loss: 2.5674386024475098\n",
      "step 8728 loss: 2.461390733718872\n",
      "step 8729 loss: 2.3919596672058105\n",
      "step 8730 loss: 2.388383626937866\n",
      "step 8731 loss: 2.355180263519287\n",
      "step 8732 loss: 2.392047166824341\n",
      "step 8733 loss: 2.482254981994629\n",
      "step 8734 loss: 2.5415749549865723\n",
      "step 8735 loss: 2.5068507194519043\n",
      "step 8736 loss: 2.5482418537139893\n",
      "step 8737 loss: 2.4125449657440186\n",
      "step 8738 loss: 2.4914145469665527\n",
      "step 8739 loss: 2.485811471939087\n",
      "step 8740 loss: 2.61803936958313\n",
      "step 8741 loss: 2.4196815490722656\n",
      "step 8742 loss: 2.5181267261505127\n",
      "step 8743 loss: 2.44049072265625\n",
      "step 8744 loss: 2.555510997772217\n",
      "step 8745 loss: 2.454808235168457\n",
      "step 8746 loss: 2.460054874420166\n",
      "step 8747 loss: 2.44478702545166\n",
      "step 8748 loss: 2.5784523487091064\n",
      "step 8749 loss: 2.4487760066986084\n",
      "step 8750 loss: 2.4365322589874268\n",
      "step 8751 loss: 2.522066831588745\n",
      "step 8752 loss: 2.494002103805542\n",
      "step 8753 loss: 2.38811993598938\n",
      "step 8754 loss: 2.450815200805664\n",
      "step 8755 loss: 2.394298791885376\n",
      "step 8756 loss: 2.421967029571533\n",
      "step 8757 loss: 2.5273313522338867\n",
      "step 8758 loss: 2.5642828941345215\n",
      "step 8759 loss: 2.3584110736846924\n",
      "step 8760 loss: 2.53715443611145\n",
      "step 8761 loss: 2.4842045307159424\n",
      "step 8762 loss: 2.4106671810150146\n",
      "step 8763 loss: 2.5015923976898193\n",
      "step 8764 loss: 2.4840381145477295\n",
      "step 8765 loss: 2.429910659790039\n",
      "step 8766 loss: 2.470341205596924\n",
      "step 8767 loss: 2.6063661575317383\n",
      "step 8768 loss: 2.4313766956329346\n",
      "step 8769 loss: 2.539597988128662\n",
      "step 8770 loss: 2.388134479522705\n",
      "step 8771 loss: 2.4402427673339844\n",
      "step 8772 loss: 2.421006441116333\n",
      "step 8773 loss: 2.443411350250244\n",
      "step 8774 loss: 2.471255302429199\n",
      "step 8775 loss: 2.513577461242676\n",
      "step 8776 loss: 2.4593071937561035\n",
      "step 8777 loss: 2.4300177097320557\n",
      "step 8778 loss: 2.3945229053497314\n",
      "step 8779 loss: 2.542294502258301\n",
      "step 8780 loss: 2.4739601612091064\n",
      "step 8781 loss: 2.560438394546509\n",
      "step 8782 loss: 2.4929447174072266\n",
      "step 8783 loss: 2.5055696964263916\n",
      "step 8784 loss: 2.429938554763794\n",
      "step 8785 loss: 2.375824451446533\n",
      "step 8786 loss: 2.4609315395355225\n",
      "step 8787 loss: 2.5064027309417725\n",
      "step 8788 loss: 2.597003936767578\n",
      "step 8789 loss: 2.5117993354797363\n",
      "step 8790 loss: 2.526451587677002\n",
      "step 8791 loss: 2.440774440765381\n",
      "step 8792 loss: 2.5253801345825195\n",
      "step 8793 loss: 2.459230422973633\n",
      "step 8794 loss: 2.3519437313079834\n",
      "step 8795 loss: 2.499939441680908\n",
      "step 8796 loss: 2.4966788291931152\n",
      "step 8797 loss: 2.5006422996520996\n",
      "step 8798 loss: 2.4126551151275635\n",
      "step 8799 loss: 2.4092628955841064\n",
      "step 8800 loss: 2.418973445892334\n",
      "step 8801 loss: 2.5607333183288574\n",
      "step 8802 loss: 2.492365598678589\n",
      "step 8803 loss: 2.4370598793029785\n",
      "step 8804 loss: 2.4748411178588867\n",
      "step 8805 loss: 2.5170161724090576\n",
      "step 8806 loss: 2.48781418800354\n",
      "step 8807 loss: 2.421841859817505\n",
      "step 8808 loss: 2.609240770339966\n",
      "step 8809 loss: 2.429319381713867\n",
      "step 8810 loss: 2.443197250366211\n",
      "step 8811 loss: 2.55924654006958\n",
      "step 8812 loss: 2.4047012329101562\n",
      "step 8813 loss: 2.521148204803467\n",
      "step 8814 loss: 2.4221489429473877\n",
      "step 8815 loss: 2.5146262645721436\n",
      "step 8816 loss: 2.563718557357788\n",
      "step 8817 loss: 2.5914828777313232\n",
      "step 8818 loss: 2.5018346309661865\n",
      "step 8819 loss: 2.496772050857544\n",
      "step 8820 loss: 2.493626356124878\n",
      "step 8821 loss: 2.3170435428619385\n",
      "step 8822 loss: 2.504290819168091\n",
      "step 8823 loss: 2.5413990020751953\n",
      "step 8824 loss: 2.528041362762451\n",
      "step 8825 loss: 2.4006617069244385\n",
      "step 8826 loss: 2.543757438659668\n",
      "step 8827 loss: 2.465458393096924\n",
      "step 8828 loss: 2.5257933139801025\n",
      "step 8829 loss: 2.5050017833709717\n",
      "step 8830 loss: 2.485452651977539\n",
      "step 8831 loss: 2.4209654331207275\n",
      "step 8832 loss: 2.4700093269348145\n",
      "step 8833 loss: 2.4231314659118652\n",
      "step 8834 loss: 2.4311251640319824\n",
      "step 8835 loss: 2.4259209632873535\n",
      "step 8836 loss: 2.3621551990509033\n",
      "step 8837 loss: 2.590826988220215\n",
      "step 8838 loss: 2.4419116973876953\n",
      "step 8839 loss: 2.3635332584381104\n",
      "step 8840 loss: 2.4922375679016113\n",
      "step 8841 loss: 2.443519353866577\n",
      "step 8842 loss: 2.490410804748535\n",
      "step 8843 loss: 2.4327571392059326\n",
      "step 8844 loss: 2.4915711879730225\n",
      "step 8845 loss: 2.550546407699585\n",
      "step 8846 loss: 2.4721925258636475\n",
      "step 8847 loss: 2.4913628101348877\n",
      "step 8848 loss: 2.4598655700683594\n",
      "step 8849 loss: 2.412783145904541\n",
      "step 8850 loss: 2.512331962585449\n",
      "step 8851 loss: 2.474550247192383\n",
      "step 8852 loss: 2.4984991550445557\n",
      "step 8853 loss: 2.4810667037963867\n",
      "step 8854 loss: 2.4081082344055176\n",
      "step 8855 loss: 2.4468696117401123\n",
      "step 8856 loss: 2.410844326019287\n",
      "step 8857 loss: 2.5783534049987793\n",
      "step 8858 loss: 2.5093295574188232\n",
      "step 8859 loss: 2.5221822261810303\n",
      "step 8860 loss: 2.530325412750244\n",
      "step 8861 loss: 2.5817453861236572\n",
      "step 8862 loss: 2.561152935028076\n",
      "step 8863 loss: 2.4589180946350098\n",
      "step 8864 loss: 2.4297428131103516\n",
      "step 8865 loss: 2.6085503101348877\n",
      "step 8866 loss: 2.529353618621826\n",
      "step 8867 loss: 2.4332261085510254\n",
      "step 8868 loss: 2.452805519104004\n",
      "step 8869 loss: 2.6139824390411377\n",
      "step 8870 loss: 2.4669463634490967\n",
      "step 8871 loss: 2.4888761043548584\n",
      "step 8872 loss: 2.542206048965454\n",
      "step 8873 loss: 2.5563771724700928\n",
      "step 8874 loss: 2.423124074935913\n",
      "step 8875 loss: 2.5352423191070557\n",
      "step 8876 loss: 2.4821321964263916\n",
      "step 8877 loss: 2.4698612689971924\n",
      "step 8878 loss: 2.408418893814087\n",
      "step 8879 loss: 2.5637052059173584\n",
      "step 8880 loss: 2.4901437759399414\n",
      "step 8881 loss: 2.4751691818237305\n",
      "step 8882 loss: 2.524358034133911\n",
      "step 8883 loss: 2.4764161109924316\n",
      "step 8884 loss: 2.538834571838379\n",
      "step 8885 loss: 2.4383137226104736\n",
      "step 8886 loss: 2.5217554569244385\n",
      "step 8887 loss: 2.5670058727264404\n",
      "step 8888 loss: 2.5197627544403076\n",
      "step 8889 loss: 2.397561550140381\n",
      "step 8890 loss: 2.4195940494537354\n",
      "step 8891 loss: 2.4412002563476562\n",
      "step 8892 loss: 2.4073877334594727\n",
      "step 8893 loss: 2.4265284538269043\n",
      "step 8894 loss: 2.5232350826263428\n",
      "step 8895 loss: 2.5107834339141846\n",
      "step 8896 loss: 2.422112464904785\n",
      "step 8897 loss: 2.4390854835510254\n",
      "step 8898 loss: 2.461962938308716\n",
      "step 8899 loss: 2.404740571975708\n",
      "step 8900 loss: 2.494617462158203\n",
      "step 8901 loss: 2.4737160205841064\n",
      "step 8902 loss: 2.36653208732605\n",
      "step 8903 loss: 2.496049404144287\n",
      "step 8904 loss: 2.4014742374420166\n",
      "step 8905 loss: 2.400343894958496\n",
      "step 8906 loss: 2.4029126167297363\n",
      "step 8907 loss: 2.571044921875\n",
      "step 8908 loss: 2.5735561847686768\n",
      "step 8909 loss: 2.556488275527954\n",
      "step 8910 loss: 2.4591028690338135\n",
      "step 8911 loss: 2.486283779144287\n",
      "step 8912 loss: 2.511460542678833\n",
      "step 8913 loss: 2.3628621101379395\n",
      "step 8914 loss: 2.5731542110443115\n",
      "step 8915 loss: 2.518097400665283\n",
      "step 8916 loss: 2.396867036819458\n",
      "step 8917 loss: 2.451587677001953\n",
      "step 8918 loss: 2.431039571762085\n",
      "step 8919 loss: 2.283282995223999\n",
      "step 8920 loss: 2.4730896949768066\n",
      "step 8921 loss: 2.4638521671295166\n",
      "step 8922 loss: 2.461097240447998\n",
      "step 8923 loss: 2.539921760559082\n",
      "step 8924 loss: 2.581125020980835\n",
      "step 8925 loss: 2.515484094619751\n",
      "step 8926 loss: 2.369546413421631\n",
      "step 8927 loss: 2.5257089138031006\n",
      "step 8928 loss: 2.5836663246154785\n",
      "step 8929 loss: 2.5093326568603516\n",
      "step 8930 loss: 2.6087470054626465\n",
      "step 8931 loss: 2.5937116146087646\n",
      "step 8932 loss: 2.34964919090271\n",
      "step 8933 loss: 2.5611138343811035\n",
      "step 8934 loss: 2.482560157775879\n",
      "step 8935 loss: 2.3675148487091064\n",
      "step 8936 loss: 2.5069918632507324\n",
      "step 8937 loss: 2.359363317489624\n",
      "step 8938 loss: 2.5274481773376465\n",
      "step 8939 loss: 2.5081663131713867\n",
      "step 8940 loss: 2.588137626647949\n",
      "step 8941 loss: 2.543250799179077\n",
      "step 8942 loss: 2.4769256114959717\n",
      "step 8943 loss: 2.563261032104492\n",
      "step 8944 loss: 2.356827974319458\n",
      "step 8945 loss: 2.378756284713745\n",
      "step 8946 loss: 2.4204726219177246\n",
      "step 8947 loss: 2.4030508995056152\n",
      "step 8948 loss: 2.44905424118042\n",
      "step 8949 loss: 2.382445812225342\n",
      "step 8950 loss: 2.321463108062744\n",
      "step 8951 loss: 2.5636532306671143\n",
      "step 8952 loss: 2.5833542346954346\n",
      "step 8953 loss: 2.592986583709717\n",
      "step 8954 loss: 2.4232523441314697\n",
      "step 8955 loss: 2.529085159301758\n",
      "step 8956 loss: 2.5334866046905518\n",
      "step 8957 loss: 2.4216959476470947\n",
      "step 8958 loss: 2.5205562114715576\n",
      "step 8959 loss: 2.555913209915161\n",
      "step 8960 loss: 2.39412784576416\n",
      "step 8961 loss: 2.538374662399292\n",
      "step 8962 loss: 2.4916300773620605\n",
      "step 8963 loss: 2.5290286540985107\n",
      "step 8964 loss: 2.4948365688323975\n",
      "step 8965 loss: 2.575960874557495\n",
      "step 8966 loss: 2.4002671241760254\n",
      "step 8967 loss: 2.46081805229187\n",
      "step 8968 loss: 2.5730559825897217\n",
      "step 8969 loss: 2.3797061443328857\n",
      "step 8970 loss: 2.5088212490081787\n",
      "step 8971 loss: 2.442864418029785\n",
      "step 8972 loss: 2.4538168907165527\n",
      "step 8973 loss: 2.45654296875\n",
      "step 8974 loss: 2.402510166168213\n",
      "step 8975 loss: 2.522968053817749\n",
      "step 8976 loss: 2.4678852558135986\n",
      "step 8977 loss: 2.449773073196411\n",
      "step 8978 loss: 2.438955068588257\n",
      "step 8979 loss: 2.424147605895996\n",
      "step 8980 loss: 2.487438917160034\n",
      "step 8981 loss: 2.5280797481536865\n",
      "step 8982 loss: 2.5124237537384033\n",
      "step 8983 loss: 2.4130728244781494\n",
      "step 8984 loss: 2.551583766937256\n",
      "step 8985 loss: 2.3381781578063965\n",
      "step 8986 loss: 2.4478979110717773\n",
      "step 8987 loss: 2.42960262298584\n",
      "step 8988 loss: 2.409168243408203\n",
      "step 8989 loss: 2.4121909141540527\n",
      "step 8990 loss: 2.3875434398651123\n",
      "step 8991 loss: 2.4861159324645996\n",
      "step 8992 loss: 2.364194631576538\n",
      "step 8993 loss: 2.3813157081604004\n",
      "step 8994 loss: 2.398996353149414\n",
      "step 8995 loss: 2.569213628768921\n",
      "step 8996 loss: 2.499877452850342\n",
      "step 8997 loss: 2.456491231918335\n",
      "step 8998 loss: 2.4699254035949707\n",
      "step 8999 loss: 2.2895452976226807\n",
      "step 9000 loss: 2.3975775241851807\n",
      "step 9001 loss: 2.483323812484741\n",
      "step 9002 loss: 2.44674015045166\n",
      "step 9003 loss: 2.5971858501434326\n",
      "step 9004 loss: 2.4627020359039307\n",
      "step 9005 loss: 2.4528703689575195\n",
      "step 9006 loss: 2.5323140621185303\n",
      "step 9007 loss: 2.3829519748687744\n",
      "step 9008 loss: 2.606724739074707\n",
      "step 9009 loss: 2.398130416870117\n",
      "step 9010 loss: 2.4609320163726807\n",
      "step 9011 loss: 2.515044927597046\n",
      "step 9012 loss: 2.4141552448272705\n",
      "step 9013 loss: 2.442014694213867\n",
      "step 9014 loss: 2.6014437675476074\n",
      "step 9015 loss: 2.3686373233795166\n",
      "step 9016 loss: 2.426743268966675\n",
      "step 9017 loss: 2.510484218597412\n",
      "step 9018 loss: 2.4876644611358643\n",
      "step 9019 loss: 2.484863519668579\n",
      "step 9020 loss: 2.3862295150756836\n",
      "step 9021 loss: 2.448190689086914\n",
      "step 9022 loss: 2.4278125762939453\n",
      "step 9023 loss: 2.4906463623046875\n",
      "step 9024 loss: 2.485372304916382\n",
      "step 9025 loss: 2.3962767124176025\n",
      "step 9026 loss: 2.442270278930664\n",
      "step 9027 loss: 2.563405752182007\n",
      "step 9028 loss: 2.5989582538604736\n",
      "step 9029 loss: 2.4455344676971436\n",
      "step 9030 loss: 2.5017828941345215\n",
      "step 9031 loss: 2.4877216815948486\n",
      "step 9032 loss: 2.4258289337158203\n",
      "step 9033 loss: 2.5258872509002686\n",
      "step 9034 loss: 2.488497495651245\n",
      "step 9035 loss: 2.489920139312744\n",
      "step 9036 loss: 2.4710628986358643\n",
      "step 9037 loss: 2.4765961170196533\n",
      "step 9038 loss: 2.5875515937805176\n",
      "step 9039 loss: 2.574127674102783\n",
      "step 9040 loss: 2.5511815547943115\n",
      "step 9041 loss: 2.4514598846435547\n",
      "step 9042 loss: 2.352537155151367\n",
      "step 9043 loss: 2.4176900386810303\n",
      "step 9044 loss: 2.5299689769744873\n",
      "step 9045 loss: 2.4928696155548096\n",
      "step 9046 loss: 2.510970115661621\n",
      "step 9047 loss: 2.4163198471069336\n",
      "step 9048 loss: 2.4353160858154297\n",
      "step 9049 loss: 2.5755767822265625\n",
      "step 9050 loss: 2.485201835632324\n",
      "step 9051 loss: 2.4468297958374023\n",
      "step 9052 loss: 2.322232246398926\n",
      "step 9053 loss: 2.5042576789855957\n",
      "step 9054 loss: 2.5039751529693604\n",
      "step 9055 loss: 2.3377740383148193\n",
      "step 9056 loss: 2.640186309814453\n",
      "step 9057 loss: 2.5499143600463867\n",
      "step 9058 loss: 2.497596502304077\n",
      "step 9059 loss: 2.3536860942840576\n",
      "step 9060 loss: 2.498687744140625\n",
      "step 9061 loss: 2.3782131671905518\n",
      "step 9062 loss: 2.5669782161712646\n",
      "step 9063 loss: 2.46101450920105\n",
      "step 9064 loss: 2.433506965637207\n",
      "step 9065 loss: 2.352660655975342\n",
      "step 9066 loss: 2.4349124431610107\n",
      "step 9067 loss: 2.4870553016662598\n",
      "step 9068 loss: 2.542691230773926\n",
      "step 9069 loss: 2.4053354263305664\n",
      "step 9070 loss: 2.45278263092041\n",
      "step 9071 loss: 2.4263834953308105\n",
      "step 9072 loss: 2.4762957096099854\n",
      "step 9073 loss: 2.5182321071624756\n",
      "step 9074 loss: 2.41556978225708\n",
      "step 9075 loss: 2.532475233078003\n",
      "step 9076 loss: 2.427083730697632\n",
      "step 9077 loss: 2.3638055324554443\n",
      "step 9078 loss: 2.468785524368286\n",
      "step 9079 loss: 2.555522918701172\n",
      "step 9080 loss: 2.631460666656494\n",
      "step 9081 loss: 2.415022134780884\n",
      "step 9082 loss: 2.647230386734009\n",
      "step 9083 loss: 2.499904155731201\n",
      "step 9084 loss: 2.5232889652252197\n",
      "step 9085 loss: 2.443549871444702\n",
      "step 9086 loss: 2.434736490249634\n",
      "step 9087 loss: 2.3822994232177734\n",
      "step 9088 loss: 2.538386583328247\n",
      "step 9089 loss: 2.6024112701416016\n",
      "step 9090 loss: 2.5798251628875732\n",
      "step 9091 loss: 2.522724151611328\n",
      "step 9092 loss: 2.4747207164764404\n",
      "step 9093 loss: 2.467341423034668\n",
      "step 9094 loss: 2.355286121368408\n",
      "step 9095 loss: 2.620466947555542\n",
      "step 9096 loss: 2.4081978797912598\n",
      "step 9097 loss: 2.364476203918457\n",
      "step 9098 loss: 2.3923816680908203\n",
      "step 9099 loss: 2.531818151473999\n",
      "step 9100 loss: 2.5384695529937744\n",
      "step 9101 loss: 2.4317193031311035\n",
      "step 9102 loss: 2.4492990970611572\n",
      "step 9103 loss: 2.6017332077026367\n",
      "step 9104 loss: 2.465294361114502\n",
      "step 9105 loss: 2.3274710178375244\n",
      "step 9106 loss: 2.2659096717834473\n",
      "step 9107 loss: 2.4122564792633057\n",
      "step 9108 loss: 2.4780683517456055\n",
      "step 9109 loss: 2.403698205947876\n",
      "step 9110 loss: 2.427889585494995\n",
      "step 9111 loss: 2.425917625427246\n",
      "step 9112 loss: 2.4358887672424316\n",
      "step 9113 loss: 2.4421894550323486\n",
      "step 9114 loss: 2.5719237327575684\n",
      "step 9115 loss: 2.5224385261535645\n",
      "step 9116 loss: 2.466265916824341\n",
      "step 9117 loss: 2.590773820877075\n",
      "step 9118 loss: 2.397172451019287\n",
      "step 9119 loss: 2.420236110687256\n",
      "step 9120 loss: 2.5630908012390137\n",
      "step 9121 loss: 2.539987325668335\n",
      "step 9122 loss: 2.292915105819702\n",
      "step 9123 loss: 2.4970743656158447\n",
      "step 9124 loss: 2.459927558898926\n",
      "step 9125 loss: 2.3993496894836426\n",
      "step 9126 loss: 2.477168560028076\n",
      "step 9127 loss: 2.4279072284698486\n",
      "step 9128 loss: 2.4111745357513428\n",
      "step 9129 loss: 2.5763182640075684\n",
      "step 9130 loss: 2.4750730991363525\n",
      "step 9131 loss: 2.4654059410095215\n",
      "step 9132 loss: 2.5612337589263916\n",
      "step 9133 loss: 2.424238920211792\n",
      "step 9134 loss: 2.4010658264160156\n",
      "step 9135 loss: 2.519576072692871\n",
      "step 9136 loss: 2.540395736694336\n",
      "step 9137 loss: 2.383122444152832\n",
      "step 9138 loss: 2.3824596405029297\n",
      "step 9139 loss: 2.4304888248443604\n",
      "step 9140 loss: 2.5050384998321533\n",
      "step 9141 loss: 2.5135316848754883\n",
      "step 9142 loss: 2.5692825317382812\n",
      "step 9143 loss: 2.367753267288208\n",
      "step 9144 loss: 2.438685894012451\n",
      "step 9145 loss: 2.4390523433685303\n",
      "step 9146 loss: 2.6184756755828857\n",
      "step 9147 loss: 2.4686927795410156\n",
      "step 9148 loss: 2.3825454711914062\n",
      "step 9149 loss: 2.5142874717712402\n",
      "step 9150 loss: 2.4495675563812256\n",
      "step 9151 loss: 2.4947094917297363\n",
      "step 9152 loss: 2.493347406387329\n",
      "step 9153 loss: 2.3599729537963867\n",
      "step 9154 loss: 2.539435863494873\n",
      "step 9155 loss: 2.373210906982422\n",
      "step 9156 loss: 2.5646257400512695\n",
      "step 9157 loss: 2.563040018081665\n",
      "step 9158 loss: 2.454435110092163\n",
      "step 9159 loss: 2.434678316116333\n",
      "step 9160 loss: 2.4253499507904053\n",
      "step 9161 loss: 2.5049216747283936\n",
      "step 9162 loss: 2.555194139480591\n",
      "step 9163 loss: 2.5163755416870117\n",
      "step 9164 loss: 2.5479636192321777\n",
      "step 9165 loss: 2.536900520324707\n",
      "step 9166 loss: 2.4698657989501953\n",
      "step 9167 loss: 2.4987189769744873\n",
      "step 9168 loss: 2.6242001056671143\n",
      "step 9169 loss: 2.5164194107055664\n",
      "step 9170 loss: 2.4671077728271484\n",
      "step 9171 loss: 2.4272472858428955\n",
      "step 9172 loss: 2.448728084564209\n",
      "step 9173 loss: 2.3398396968841553\n",
      "step 9174 loss: 2.5375261306762695\n",
      "step 9175 loss: 2.5701725482940674\n",
      "step 9176 loss: 2.5579705238342285\n",
      "step 9177 loss: 2.417790412902832\n",
      "step 9178 loss: 2.5135254859924316\n",
      "step 9179 loss: 2.4467597007751465\n",
      "step 9180 loss: 2.5186469554901123\n",
      "step 9181 loss: 2.3701722621917725\n",
      "step 9182 loss: 2.552830219268799\n",
      "step 9183 loss: 2.5148913860321045\n",
      "step 9184 loss: 2.383654832839966\n",
      "step 9185 loss: 2.455533981323242\n",
      "step 9186 loss: 2.5329933166503906\n",
      "step 9187 loss: 2.3961358070373535\n",
      "step 9188 loss: 2.524442672729492\n",
      "step 9189 loss: 2.488722801208496\n",
      "step 9190 loss: 2.5458643436431885\n",
      "step 9191 loss: 2.4630160331726074\n",
      "step 9192 loss: 2.6300740242004395\n",
      "step 9193 loss: 2.49564528465271\n",
      "step 9194 loss: 2.386892795562744\n",
      "step 9195 loss: 2.526182174682617\n",
      "step 9196 loss: 2.4000706672668457\n",
      "step 9197 loss: 2.546250581741333\n",
      "step 9198 loss: 2.441757917404175\n",
      "step 9199 loss: 2.3487656116485596\n",
      "step 9200 loss: 2.5396728515625\n",
      "step 9201 loss: 2.4242424964904785\n",
      "step 9202 loss: 2.5399510860443115\n",
      "step 9203 loss: 2.419130802154541\n",
      "step 9204 loss: 2.59082293510437\n",
      "step 9205 loss: 2.4252219200134277\n",
      "step 9206 loss: 2.483217716217041\n",
      "step 9207 loss: 2.421652317047119\n",
      "step 9208 loss: 2.463690996170044\n",
      "step 9209 loss: 2.3622488975524902\n",
      "step 9210 loss: 2.4951963424682617\n",
      "step 9211 loss: 2.621427297592163\n",
      "step 9212 loss: 2.366990327835083\n",
      "step 9213 loss: 2.4199676513671875\n",
      "step 9214 loss: 2.5030009746551514\n",
      "step 9215 loss: 2.5502145290374756\n",
      "step 9216 loss: 2.472679615020752\n",
      "step 9217 loss: 2.522878646850586\n",
      "step 9218 loss: 2.435357093811035\n",
      "step 9219 loss: 2.4603688716888428\n",
      "step 9220 loss: 2.5430045127868652\n",
      "step 9221 loss: 2.528799057006836\n",
      "step 9222 loss: 2.441513776779175\n",
      "step 9223 loss: 2.5102672576904297\n",
      "step 9224 loss: 2.530062675476074\n",
      "step 9225 loss: 2.413300037384033\n",
      "step 9226 loss: 2.5468387603759766\n",
      "step 9227 loss: 2.497030019760132\n",
      "step 9228 loss: 2.379413604736328\n",
      "step 9229 loss: 2.337843894958496\n",
      "step 9230 loss: 2.451672315597534\n",
      "step 9231 loss: 2.410130739212036\n",
      "step 9232 loss: 2.404031991958618\n",
      "step 9233 loss: 2.3237810134887695\n",
      "step 9234 loss: 2.401634454727173\n",
      "step 9235 loss: 2.4096884727478027\n",
      "step 9236 loss: 2.618837356567383\n",
      "step 9237 loss: 2.5739922523498535\n",
      "step 9238 loss: 2.469682216644287\n",
      "step 9239 loss: 2.3381268978118896\n",
      "step 9240 loss: 2.4630093574523926\n",
      "step 9241 loss: 2.460271120071411\n",
      "step 9242 loss: 2.4466030597686768\n",
      "step 9243 loss: 2.5186357498168945\n",
      "step 9244 loss: 2.5217740535736084\n",
      "step 9245 loss: 2.507944107055664\n",
      "step 9246 loss: 2.432377338409424\n",
      "step 9247 loss: 2.483166217803955\n",
      "step 9248 loss: 2.4073472023010254\n",
      "step 9249 loss: 2.470913887023926\n",
      "step 9250 loss: 2.5736162662506104\n",
      "step 9251 loss: 2.498119354248047\n",
      "step 9252 loss: 2.450576066970825\n",
      "step 9253 loss: 2.459848403930664\n",
      "step 9254 loss: 2.4420785903930664\n",
      "step 9255 loss: 2.5131912231445312\n",
      "step 9256 loss: 2.5286169052124023\n",
      "step 9257 loss: 2.431304693222046\n",
      "step 9258 loss: 2.403592586517334\n",
      "step 9259 loss: 2.3330447673797607\n",
      "step 9260 loss: 2.369166851043701\n",
      "step 9261 loss: 2.573962688446045\n",
      "step 9262 loss: 2.4556710720062256\n",
      "step 9263 loss: 2.458306074142456\n",
      "step 9264 loss: 2.532397985458374\n",
      "step 9265 loss: 2.3554110527038574\n",
      "step 9266 loss: 2.508920192718506\n",
      "step 9267 loss: 2.486635208129883\n",
      "step 9268 loss: 2.3735663890838623\n",
      "step 9269 loss: 2.4073541164398193\n",
      "step 9270 loss: 2.5225987434387207\n",
      "step 9271 loss: 2.5686750411987305\n",
      "step 9272 loss: 2.461385488510132\n",
      "step 9273 loss: 2.4456233978271484\n",
      "step 9274 loss: 2.3708267211914062\n",
      "step 9275 loss: 2.527862310409546\n",
      "step 9276 loss: 2.559831380844116\n",
      "step 9277 loss: 2.5399277210235596\n",
      "step 9278 loss: 2.498730421066284\n",
      "step 9279 loss: 2.492809295654297\n",
      "step 9280 loss: 2.6272833347320557\n",
      "step 9281 loss: 2.4830234050750732\n",
      "step 9282 loss: 2.5095407962799072\n",
      "step 9283 loss: 2.4562926292419434\n",
      "step 9284 loss: 2.4918465614318848\n",
      "step 9285 loss: 2.398984909057617\n",
      "step 9286 loss: 2.5371334552764893\n",
      "step 9287 loss: 2.445401906967163\n",
      "step 9288 loss: 2.4812426567077637\n",
      "step 9289 loss: 2.3954694271087646\n",
      "step 9290 loss: 2.601468324661255\n",
      "step 9291 loss: 2.42767596244812\n",
      "step 9292 loss: 2.4342503547668457\n",
      "step 9293 loss: 2.535280704498291\n",
      "step 9294 loss: 2.408212423324585\n",
      "step 9295 loss: 2.535853624343872\n",
      "step 9296 loss: 2.5276272296905518\n",
      "step 9297 loss: 2.5533459186553955\n",
      "step 9298 loss: 2.5836308002471924\n",
      "step 9299 loss: 2.6264450550079346\n",
      "step 9300 loss: 2.563826322555542\n",
      "step 9301 loss: 2.5080673694610596\n",
      "step 9302 loss: 2.5592775344848633\n",
      "step 9303 loss: 2.4302713871002197\n",
      "step 9304 loss: 2.518867254257202\n",
      "step 9305 loss: 2.4403955936431885\n",
      "step 9306 loss: 2.447916269302368\n",
      "step 9307 loss: 2.5249006748199463\n",
      "step 9308 loss: 2.575779914855957\n",
      "step 9309 loss: 2.492194652557373\n",
      "step 9310 loss: 2.570878267288208\n",
      "step 9311 loss: 2.4767000675201416\n",
      "step 9312 loss: 2.5649101734161377\n",
      "step 9313 loss: 2.364701986312866\n",
      "step 9314 loss: 2.4551498889923096\n",
      "step 9315 loss: 2.665205478668213\n",
      "step 9316 loss: 2.4394023418426514\n",
      "step 9317 loss: 2.4207799434661865\n",
      "step 9318 loss: 2.3762030601501465\n",
      "step 9319 loss: 2.5891127586364746\n",
      "step 9320 loss: 2.4480223655700684\n",
      "step 9321 loss: 2.5128095149993896\n",
      "step 9322 loss: 2.424053430557251\n",
      "step 9323 loss: 2.479867696762085\n",
      "step 9324 loss: 2.4232592582702637\n",
      "step 9325 loss: 2.576064109802246\n",
      "step 9326 loss: 2.6231868267059326\n",
      "step 9327 loss: 2.7372093200683594\n",
      "step 9328 loss: 2.4266207218170166\n",
      "step 9329 loss: 2.393143653869629\n",
      "step 9330 loss: 2.4854495525360107\n",
      "step 9331 loss: 2.4798450469970703\n",
      "step 9332 loss: 2.4331753253936768\n",
      "step 9333 loss: 2.447068691253662\n",
      "step 9334 loss: 2.4399075508117676\n",
      "step 9335 loss: 2.5630011558532715\n",
      "step 9336 loss: 2.338495969772339\n",
      "step 9337 loss: 2.6074273586273193\n",
      "step 9338 loss: 2.414330244064331\n",
      "step 9339 loss: 2.44728946685791\n",
      "step 9340 loss: 2.446324110031128\n",
      "step 9341 loss: 2.514678955078125\n",
      "step 9342 loss: 2.398298740386963\n",
      "step 9343 loss: 2.4961724281311035\n",
      "step 9344 loss: 2.5866219997406006\n",
      "step 9345 loss: 2.522091865539551\n",
      "step 9346 loss: 2.4197330474853516\n",
      "step 9347 loss: 2.5683956146240234\n",
      "step 9348 loss: 2.4931790828704834\n",
      "step 9349 loss: 2.5064635276794434\n",
      "step 9350 loss: 2.4904372692108154\n",
      "step 9351 loss: 2.4411275386810303\n",
      "step 9352 loss: 2.5726675987243652\n",
      "step 9353 loss: 2.368450880050659\n",
      "step 9354 loss: 2.346463203430176\n",
      "step 9355 loss: 2.5160224437713623\n",
      "step 9356 loss: 2.5120956897735596\n",
      "step 9357 loss: 2.5115201473236084\n",
      "step 9358 loss: 2.4686195850372314\n",
      "step 9359 loss: 2.537374496459961\n",
      "step 9360 loss: 2.3762245178222656\n",
      "step 9361 loss: 2.453899621963501\n",
      "step 9362 loss: 2.531867265701294\n",
      "step 9363 loss: 2.515247106552124\n",
      "step 9364 loss: 2.4355316162109375\n",
      "step 9365 loss: 2.564767837524414\n",
      "step 9366 loss: 2.417213201522827\n",
      "step 9367 loss: 2.4074532985687256\n",
      "step 9368 loss: 2.451580047607422\n",
      "step 9369 loss: 2.469053030014038\n",
      "step 9370 loss: 2.5309741497039795\n",
      "step 9371 loss: 2.4931578636169434\n",
      "step 9372 loss: 2.633600950241089\n",
      "step 9373 loss: 2.455057382583618\n",
      "step 9374 loss: 2.38623046875\n",
      "step 9375 loss: 2.4465978145599365\n",
      "step 9376 loss: 2.5743062496185303\n",
      "step 9377 loss: 2.477220058441162\n",
      "step 9378 loss: 2.4957916736602783\n",
      "step 9379 loss: 2.363736152648926\n",
      "step 9380 loss: 2.5259811878204346\n",
      "step 9381 loss: 2.3896517753601074\n",
      "step 9382 loss: 2.450446367263794\n",
      "step 9383 loss: 2.6068308353424072\n",
      "step 9384 loss: 2.523470401763916\n",
      "step 9385 loss: 2.393270254135132\n",
      "step 9386 loss: 2.644319772720337\n",
      "step 9387 loss: 2.4494895935058594\n",
      "step 9388 loss: 2.439443826675415\n",
      "step 9389 loss: 2.321779727935791\n",
      "step 9390 loss: 2.575303792953491\n",
      "step 9391 loss: 2.3955888748168945\n",
      "step 9392 loss: 2.496241331100464\n",
      "step 9393 loss: 2.4833390712738037\n",
      "step 9394 loss: 2.5579559803009033\n",
      "step 9395 loss: 2.5335490703582764\n",
      "step 9396 loss: 2.5256989002227783\n",
      "step 9397 loss: 2.4167490005493164\n",
      "step 9398 loss: 2.5522594451904297\n",
      "step 9399 loss: 2.539850950241089\n",
      "step 9400 loss: 2.363532304763794\n",
      "step 9401 loss: 2.502851963043213\n",
      "step 9402 loss: 2.3443198204040527\n",
      "step 9403 loss: 2.4988789558410645\n",
      "step 9404 loss: 2.454113483428955\n",
      "step 9405 loss: 2.4153101444244385\n",
      "step 9406 loss: 2.3782172203063965\n",
      "step 9407 loss: 2.414954662322998\n",
      "step 9408 loss: 2.351318836212158\n",
      "step 9409 loss: 2.5428333282470703\n",
      "step 9410 loss: 2.431521415710449\n",
      "step 9411 loss: 2.520062208175659\n",
      "step 9412 loss: 2.468810796737671\n",
      "step 9413 loss: 2.5240118503570557\n",
      "step 9414 loss: 2.483811378479004\n",
      "step 9415 loss: 2.513941526412964\n",
      "step 9416 loss: 2.4977097511291504\n",
      "step 9417 loss: 2.51969051361084\n",
      "step 9418 loss: 2.395052909851074\n",
      "step 9419 loss: 2.675631284713745\n",
      "step 9420 loss: 2.421278476715088\n",
      "step 9421 loss: 2.493115186691284\n",
      "step 9422 loss: 2.4751040935516357\n",
      "step 9423 loss: 2.447129011154175\n",
      "step 9424 loss: 2.472736358642578\n",
      "step 9425 loss: 2.543778657913208\n",
      "step 9426 loss: 2.5608572959899902\n",
      "step 9427 loss: 2.446363687515259\n",
      "step 9428 loss: 2.420337438583374\n",
      "step 9429 loss: 2.459763526916504\n",
      "step 9430 loss: 2.5061910152435303\n",
      "step 9431 loss: 2.621844530105591\n",
      "step 9432 loss: 2.421304225921631\n",
      "step 9433 loss: 2.409482955932617\n",
      "step 9434 loss: 2.439704656600952\n",
      "step 9435 loss: 2.49664568901062\n",
      "step 9436 loss: 2.4069128036499023\n",
      "step 9437 loss: 2.5451643466949463\n",
      "step 9438 loss: 2.40140700340271\n",
      "step 9439 loss: 2.4643664360046387\n",
      "step 9440 loss: 2.526934862136841\n",
      "step 9441 loss: 2.4719669818878174\n",
      "step 9442 loss: 2.4475245475769043\n",
      "step 9443 loss: 2.5618176460266113\n",
      "step 9444 loss: 2.4249327182769775\n",
      "step 9445 loss: 2.5137314796447754\n",
      "step 9446 loss: 2.5174176692962646\n",
      "step 9447 loss: 2.3803694248199463\n",
      "step 9448 loss: 2.515942335128784\n",
      "step 9449 loss: 2.637990951538086\n",
      "step 9450 loss: 2.3500263690948486\n",
      "step 9451 loss: 2.468048095703125\n",
      "step 9452 loss: 2.4738292694091797\n",
      "step 9453 loss: 2.494016647338867\n",
      "step 9454 loss: 2.4614908695220947\n",
      "step 9455 loss: 2.5828018188476562\n",
      "step 9456 loss: 2.515270709991455\n",
      "step 9457 loss: 2.466073751449585\n",
      "step 9458 loss: 2.3830089569091797\n",
      "step 9459 loss: 2.3692402839660645\n",
      "step 9460 loss: 2.4940242767333984\n",
      "step 9461 loss: 2.540031909942627\n",
      "step 9462 loss: 2.521259307861328\n",
      "step 9463 loss: 2.53633189201355\n",
      "step 9464 loss: 2.45802903175354\n",
      "step 9465 loss: 2.457542657852173\n",
      "step 9466 loss: 2.401506185531616\n",
      "step 9467 loss: 2.4010961055755615\n",
      "step 9468 loss: 2.48594069480896\n",
      "step 9469 loss: 2.528106212615967\n",
      "step 9470 loss: 2.447950601577759\n",
      "step 9471 loss: 2.478167772293091\n",
      "step 9472 loss: 2.5886402130126953\n",
      "step 9473 loss: 2.5502851009368896\n",
      "step 9474 loss: 2.4413743019104004\n",
      "step 9475 loss: 2.4525861740112305\n",
      "step 9476 loss: 2.337587356567383\n",
      "step 9477 loss: 2.4381892681121826\n",
      "step 9478 loss: 2.4769880771636963\n",
      "step 9479 loss: 2.4525904655456543\n",
      "step 9480 loss: 2.3977301120758057\n",
      "step 9481 loss: 2.3878672122955322\n",
      "step 9482 loss: 2.5434839725494385\n",
      "step 9483 loss: 2.4520652294158936\n",
      "step 9484 loss: 2.5122668743133545\n",
      "step 9485 loss: 2.604598045349121\n",
      "step 9486 loss: 2.4823966026306152\n",
      "step 9487 loss: 2.494288682937622\n",
      "step 9488 loss: 2.5030624866485596\n",
      "step 9489 loss: 2.3447763919830322\n",
      "step 9490 loss: 2.6093671321868896\n",
      "step 9491 loss: 2.5139963626861572\n",
      "step 9492 loss: 2.4742543697357178\n",
      "step 9493 loss: 2.4289190769195557\n",
      "step 9494 loss: 2.4995405673980713\n",
      "step 9495 loss: 2.4300649166107178\n",
      "step 9496 loss: 2.4937920570373535\n",
      "step 9497 loss: 2.4308135509490967\n",
      "step 9498 loss: 2.4846343994140625\n",
      "step 9499 loss: 2.4984896183013916\n",
      "step 9500 loss: 2.384711742401123\n",
      "step 9501 loss: 2.537051200866699\n",
      "step 9502 loss: 2.4323408603668213\n",
      "step 9503 loss: 2.4650957584381104\n",
      "step 9504 loss: 2.547213554382324\n",
      "step 9505 loss: 2.548408269882202\n",
      "step 9506 loss: 2.4075307846069336\n",
      "step 9507 loss: 2.438661575317383\n",
      "step 9508 loss: 2.4123642444610596\n",
      "step 9509 loss: 2.5839695930480957\n",
      "step 9510 loss: 2.5265657901763916\n",
      "step 9511 loss: 2.453390121459961\n",
      "step 9512 loss: 2.451287031173706\n",
      "step 9513 loss: 2.3856449127197266\n",
      "step 9514 loss: 2.480674982070923\n",
      "step 9515 loss: 2.3948559761047363\n",
      "step 9516 loss: 2.6184885501861572\n",
      "step 9517 loss: 2.332076072692871\n",
      "step 9518 loss: 2.5307576656341553\n",
      "step 9519 loss: 2.499727487564087\n",
      "step 9520 loss: 2.4560043811798096\n",
      "step 9521 loss: 2.46305251121521\n",
      "step 9522 loss: 2.3927905559539795\n",
      "step 9523 loss: 2.349282741546631\n",
      "step 9524 loss: 2.5618081092834473\n",
      "step 9525 loss: 2.5720016956329346\n",
      "step 9526 loss: 2.5664634704589844\n",
      "step 9527 loss: 2.516552448272705\n",
      "step 9528 loss: 2.4719061851501465\n",
      "step 9529 loss: 2.3962154388427734\n",
      "step 9530 loss: 2.4389140605926514\n",
      "step 9531 loss: 2.591689109802246\n",
      "step 9532 loss: 2.3965091705322266\n",
      "step 9533 loss: 2.5263900756835938\n",
      "step 9534 loss: 2.3935387134552\n",
      "step 9535 loss: 2.4611871242523193\n",
      "step 9536 loss: 2.520732879638672\n",
      "step 9537 loss: 2.429029941558838\n",
      "step 9538 loss: 2.568831205368042\n",
      "step 9539 loss: 2.51324462890625\n",
      "step 9540 loss: 2.420682907104492\n",
      "step 9541 loss: 2.465860605239868\n",
      "step 9542 loss: 2.3993749618530273\n",
      "step 9543 loss: 2.4571406841278076\n",
      "step 9544 loss: 2.5122435092926025\n",
      "step 9545 loss: 2.558265447616577\n",
      "step 9546 loss: 2.367738962173462\n",
      "step 9547 loss: 2.450150966644287\n",
      "step 9548 loss: 2.313477039337158\n",
      "step 9549 loss: 2.417133092880249\n",
      "step 9550 loss: 2.3173534870147705\n",
      "step 9551 loss: 2.3073174953460693\n",
      "step 9552 loss: 2.4458582401275635\n",
      "step 9553 loss: 2.6073572635650635\n",
      "step 9554 loss: 2.446052312850952\n",
      "step 9555 loss: 2.575762987136841\n",
      "step 9556 loss: 2.4760031700134277\n",
      "step 9557 loss: 2.4620723724365234\n",
      "step 9558 loss: 2.533182382583618\n",
      "step 9559 loss: 2.473015308380127\n",
      "step 9560 loss: 2.472222089767456\n",
      "step 9561 loss: 2.4474339485168457\n",
      "step 9562 loss: 2.4527792930603027\n",
      "step 9563 loss: 2.5443785190582275\n",
      "step 9564 loss: 2.479649782180786\n",
      "step 9565 loss: 2.5441205501556396\n",
      "step 9566 loss: 2.3725008964538574\n",
      "step 9567 loss: 2.3745956420898438\n",
      "step 9568 loss: 2.4902820587158203\n",
      "step 9569 loss: 2.3266921043395996\n",
      "step 9570 loss: 2.5740137100219727\n",
      "step 9571 loss: 2.4506611824035645\n",
      "step 9572 loss: 2.347762107849121\n",
      "step 9573 loss: 2.392268180847168\n",
      "step 9574 loss: 2.5046749114990234\n",
      "step 9575 loss: 2.4581422805786133\n",
      "step 9576 loss: 2.3691818714141846\n",
      "step 9577 loss: 2.551600456237793\n",
      "step 9578 loss: 2.4070866107940674\n",
      "step 9579 loss: 2.4200658798217773\n",
      "step 9580 loss: 2.467529296875\n",
      "step 9581 loss: 2.4456050395965576\n",
      "step 9582 loss: 2.593243360519409\n",
      "step 9583 loss: 2.603923797607422\n",
      "step 9584 loss: 2.56058669090271\n",
      "step 9585 loss: 2.5034573078155518\n",
      "step 9586 loss: 2.4939165115356445\n",
      "step 9587 loss: 2.6002697944641113\n",
      "step 9588 loss: 2.5191023349761963\n",
      "step 9589 loss: 2.4752042293548584\n",
      "step 9590 loss: 2.5962140560150146\n",
      "step 9591 loss: 2.62233304977417\n",
      "step 9592 loss: 2.3806402683258057\n",
      "step 9593 loss: 2.4935507774353027\n",
      "step 9594 loss: 2.3679990768432617\n",
      "step 9595 loss: 2.3781228065490723\n",
      "step 9596 loss: 2.557973861694336\n",
      "step 9597 loss: 2.439084768295288\n",
      "step 9598 loss: 2.5238430500030518\n",
      "step 9599 loss: 2.3977131843566895\n",
      "step 9600 loss: 2.497910499572754\n",
      "step 9601 loss: 2.4555349349975586\n",
      "step 9602 loss: 2.481046438217163\n",
      "step 9603 loss: 2.5024948120117188\n",
      "step 9604 loss: 2.596101999282837\n",
      "step 9605 loss: 2.3567304611206055\n",
      "step 9606 loss: 2.4781394004821777\n",
      "step 9607 loss: 2.4873454570770264\n",
      "step 9608 loss: 2.3884215354919434\n",
      "step 9609 loss: 2.4661405086517334\n",
      "step 9610 loss: 2.3391101360321045\n",
      "step 9611 loss: 2.388604164123535\n",
      "step 9612 loss: 2.381298303604126\n",
      "step 9613 loss: 2.3297555446624756\n",
      "step 9614 loss: 2.455408811569214\n",
      "step 9615 loss: 2.5558876991271973\n",
      "step 9616 loss: 2.5153450965881348\n",
      "step 9617 loss: 2.5070948600769043\n",
      "step 9618 loss: 2.552337884902954\n",
      "step 9619 loss: 2.5103659629821777\n",
      "step 9620 loss: 2.4718101024627686\n",
      "step 9621 loss: 2.4956748485565186\n",
      "step 9622 loss: 2.385829210281372\n",
      "step 9623 loss: 2.4726080894470215\n",
      "step 9624 loss: 2.479968309402466\n",
      "step 9625 loss: 2.521476984024048\n",
      "step 9626 loss: 2.3253893852233887\n",
      "step 9627 loss: 2.600895643234253\n",
      "step 9628 loss: 2.4713876247406006\n",
      "step 9629 loss: 2.516205310821533\n",
      "step 9630 loss: 2.4376418590545654\n",
      "step 9631 loss: 2.5385565757751465\n",
      "step 9632 loss: 2.475332498550415\n",
      "step 9633 loss: 2.514827013015747\n",
      "step 9634 loss: 2.3089678287506104\n",
      "step 9635 loss: 2.4491069316864014\n",
      "step 9636 loss: 2.453361749649048\n",
      "step 9637 loss: 2.542121410369873\n",
      "step 9638 loss: 2.6167209148406982\n",
      "step 9639 loss: 2.5213804244995117\n",
      "step 9640 loss: 2.4644553661346436\n",
      "step 9641 loss: 2.4493069648742676\n",
      "step 9642 loss: 2.531299352645874\n",
      "step 9643 loss: 2.449089288711548\n",
      "step 9644 loss: 2.5609147548675537\n",
      "step 9645 loss: 2.417229652404785\n",
      "step 9646 loss: 2.4756269454956055\n",
      "step 9647 loss: 2.492204189300537\n",
      "step 9648 loss: 2.4081358909606934\n",
      "step 9649 loss: 2.2487106323242188\n",
      "step 9650 loss: 2.4956252574920654\n",
      "step 9651 loss: 2.4565231800079346\n",
      "step 9652 loss: 2.4748446941375732\n",
      "step 9653 loss: 2.6063716411590576\n",
      "step 9654 loss: 2.459493398666382\n",
      "step 9655 loss: 2.408210277557373\n",
      "step 9656 loss: 2.4477992057800293\n",
      "step 9657 loss: 2.430941581726074\n",
      "step 9658 loss: 2.4950625896453857\n",
      "step 9659 loss: 2.4337735176086426\n",
      "step 9660 loss: 2.423013687133789\n",
      "step 9661 loss: 2.5557448863983154\n",
      "step 9662 loss: 2.508347511291504\n",
      "step 9663 loss: 2.4562461376190186\n",
      "step 9664 loss: 2.466294288635254\n",
      "step 9665 loss: 2.4583353996276855\n",
      "step 9666 loss: 2.419771671295166\n",
      "step 9667 loss: 2.5030229091644287\n",
      "step 9668 loss: 2.421894073486328\n",
      "step 9669 loss: 2.5197255611419678\n",
      "step 9670 loss: 2.417754888534546\n",
      "step 9671 loss: 2.3386874198913574\n",
      "step 9672 loss: 2.5107808113098145\n",
      "step 9673 loss: 2.4835216999053955\n",
      "step 9674 loss: 2.494752883911133\n",
      "step 9675 loss: 2.489633798599243\n",
      "step 9676 loss: 2.4212558269500732\n",
      "step 9677 loss: 2.515836238861084\n",
      "step 9678 loss: 2.4743645191192627\n",
      "step 9679 loss: 2.4972801208496094\n",
      "step 9680 loss: 2.514051675796509\n",
      "step 9681 loss: 2.4763169288635254\n",
      "step 9682 loss: 2.528338670730591\n",
      "step 9683 loss: 2.4528439044952393\n",
      "step 9684 loss: 2.4836909770965576\n",
      "step 9685 loss: 2.355478286743164\n",
      "step 9686 loss: 2.4300570487976074\n",
      "step 9687 loss: 2.483015775680542\n",
      "step 9688 loss: 2.4372901916503906\n",
      "step 9689 loss: 2.610947608947754\n",
      "step 9690 loss: 2.370269775390625\n",
      "step 9691 loss: 2.3598458766937256\n",
      "step 9692 loss: 2.4479269981384277\n",
      "step 9693 loss: 2.5232467651367188\n",
      "step 9694 loss: 2.388731002807617\n",
      "step 9695 loss: 2.507600784301758\n",
      "step 9696 loss: 2.434417486190796\n",
      "step 9697 loss: 2.413339853286743\n",
      "step 9698 loss: 2.446476936340332\n",
      "step 9699 loss: 2.4695398807525635\n",
      "step 9700 loss: 2.4720048904418945\n",
      "step 9701 loss: 2.3779220581054688\n",
      "step 9702 loss: 2.506624221801758\n",
      "step 9703 loss: 2.4615883827209473\n",
      "step 9704 loss: 2.4116923809051514\n",
      "step 9705 loss: 2.4260525703430176\n",
      "step 9706 loss: 2.4601893424987793\n",
      "step 9707 loss: 2.4476699829101562\n",
      "step 9708 loss: 2.417527198791504\n",
      "step 9709 loss: 2.5897910594940186\n",
      "step 9710 loss: 2.5939626693725586\n",
      "step 9711 loss: 2.5056979656219482\n",
      "step 9712 loss: 2.5486319065093994\n",
      "step 9713 loss: 2.460718870162964\n",
      "step 9714 loss: 2.4369215965270996\n",
      "step 9715 loss: 2.4094960689544678\n",
      "step 9716 loss: 2.5610709190368652\n",
      "step 9717 loss: 2.533195972442627\n",
      "step 9718 loss: 2.505218505859375\n",
      "step 9719 loss: 2.534518241882324\n",
      "step 9720 loss: 2.432922840118408\n",
      "step 9721 loss: 2.52060604095459\n",
      "step 9722 loss: 2.404271364212036\n",
      "step 9723 loss: 2.4976212978363037\n",
      "step 9724 loss: 2.4995858669281006\n",
      "step 9725 loss: 2.3997726440429688\n",
      "step 9726 loss: 2.4954471588134766\n",
      "step 9727 loss: 2.50077223777771\n",
      "step 9728 loss: 2.5090742111206055\n",
      "step 9729 loss: 2.4044530391693115\n",
      "step 9730 loss: 2.4955008029937744\n",
      "step 9731 loss: 2.5062785148620605\n",
      "step 9732 loss: 2.4858291149139404\n",
      "step 9733 loss: 2.4342775344848633\n",
      "step 9734 loss: 2.501108407974243\n",
      "step 9735 loss: 2.351827621459961\n",
      "step 9736 loss: 2.381268262863159\n",
      "step 9737 loss: 2.4988560676574707\n",
      "step 9738 loss: 2.4339852333068848\n",
      "step 9739 loss: 2.453998565673828\n",
      "step 9740 loss: 2.4828786849975586\n",
      "step 9741 loss: 2.5123298168182373\n",
      "step 9742 loss: 2.490067720413208\n",
      "step 9743 loss: 2.4227020740509033\n",
      "step 9744 loss: 2.463697910308838\n",
      "step 9745 loss: 2.463458776473999\n",
      "step 9746 loss: 2.3859610557556152\n",
      "step 9747 loss: 2.5194571018218994\n",
      "step 9748 loss: 2.5754573345184326\n",
      "step 9749 loss: 2.551453113555908\n",
      "step 9750 loss: 2.3861465454101562\n",
      "step 9751 loss: 2.478090524673462\n",
      "step 9752 loss: 2.521387815475464\n",
      "step 9753 loss: 2.3381242752075195\n",
      "step 9754 loss: 2.445857286453247\n",
      "step 9755 loss: 2.324367046356201\n",
      "step 9756 loss: 2.432751178741455\n",
      "step 9757 loss: 2.5143117904663086\n",
      "step 9758 loss: 2.639272451400757\n",
      "step 9759 loss: 2.4602396488189697\n",
      "step 9760 loss: 2.312073230743408\n",
      "step 9761 loss: 2.3010001182556152\n",
      "step 9762 loss: 2.3779385089874268\n",
      "step 9763 loss: 2.4108262062072754\n",
      "step 9764 loss: 2.4909186363220215\n",
      "step 9765 loss: 2.5769777297973633\n",
      "step 9766 loss: 2.363298177719116\n",
      "step 9767 loss: 2.5164382457733154\n",
      "step 9768 loss: 2.4665207862854004\n",
      "step 9769 loss: 2.517768144607544\n",
      "step 9770 loss: 2.5182950496673584\n",
      "step 9771 loss: 2.519958734512329\n",
      "step 9772 loss: 2.420828342437744\n",
      "step 9773 loss: 2.4645607471466064\n",
      "step 9774 loss: 2.526911735534668\n",
      "step 9775 loss: 2.3184759616851807\n",
      "step 9776 loss: 2.397735118865967\n",
      "step 9777 loss: 2.4211037158966064\n",
      "step 9778 loss: 2.515918254852295\n",
      "step 9779 loss: 2.4666271209716797\n",
      "step 9780 loss: 2.3709716796875\n",
      "step 9781 loss: 2.4905991554260254\n",
      "step 9782 loss: 2.2961368560791016\n",
      "step 9783 loss: 2.4720304012298584\n",
      "step 9784 loss: 2.4373421669006348\n",
      "step 9785 loss: 2.492039203643799\n",
      "step 9786 loss: 2.4918928146362305\n",
      "step 9787 loss: 2.5371694564819336\n",
      "step 9788 loss: 2.4996426105499268\n",
      "step 9789 loss: 2.4854509830474854\n",
      "step 9790 loss: 2.3792850971221924\n",
      "step 9791 loss: 2.4616689682006836\n",
      "step 9792 loss: 2.398437023162842\n",
      "step 9793 loss: 2.4700217247009277\n",
      "step 9794 loss: 2.449453115463257\n",
      "step 9795 loss: 2.5489375591278076\n",
      "step 9796 loss: 2.3728787899017334\n",
      "step 9797 loss: 2.4511847496032715\n",
      "step 9798 loss: 2.5101752281188965\n",
      "step 9799 loss: 2.508436441421509\n",
      "step 9800 loss: 2.4446794986724854\n",
      "step 9801 loss: 2.4842002391815186\n",
      "step 9802 loss: 2.4438138008117676\n",
      "step 9803 loss: 2.4489128589630127\n",
      "step 9804 loss: 2.427851915359497\n",
      "step 9805 loss: 2.392531156539917\n",
      "step 9806 loss: 2.413916826248169\n",
      "step 9807 loss: 2.417226791381836\n",
      "step 9808 loss: 2.4984209537506104\n",
      "step 9809 loss: 2.415787696838379\n",
      "step 9810 loss: 2.473599433898926\n",
      "step 9811 loss: 2.540032148361206\n",
      "step 9812 loss: 2.4563236236572266\n",
      "step 9813 loss: 2.5159225463867188\n",
      "step 9814 loss: 2.5833704471588135\n",
      "step 9815 loss: 2.3807783126831055\n",
      "step 9816 loss: 2.5063822269439697\n",
      "step 9817 loss: 2.4883251190185547\n",
      "step 9818 loss: 2.4558229446411133\n",
      "step 9819 loss: 2.5515758991241455\n",
      "step 9820 loss: 2.525071859359741\n",
      "step 9821 loss: 2.446263551712036\n",
      "step 9822 loss: 2.5287673473358154\n",
      "step 9823 loss: 2.560460329055786\n",
      "step 9824 loss: 2.370404005050659\n",
      "step 9825 loss: 2.4355416297912598\n",
      "step 9826 loss: 2.4288575649261475\n",
      "step 9827 loss: 2.49631929397583\n",
      "step 9828 loss: 2.446685791015625\n",
      "step 9829 loss: 2.533095359802246\n",
      "step 9830 loss: 2.4484241008758545\n",
      "step 9831 loss: 2.4425461292266846\n",
      "step 9832 loss: 2.3850200176239014\n",
      "step 9833 loss: 2.476732015609741\n",
      "step 9834 loss: 2.4562630653381348\n",
      "step 9835 loss: 2.4793357849121094\n",
      "step 9836 loss: 2.4182841777801514\n",
      "step 9837 loss: 2.438615560531616\n",
      "step 9838 loss: 2.4760618209838867\n",
      "step 9839 loss: 2.507847309112549\n",
      "step 9840 loss: 2.6219935417175293\n",
      "step 9841 loss: 2.4084699153900146\n",
      "step 9842 loss: 2.4349405765533447\n",
      "step 9843 loss: 2.391183853149414\n",
      "step 9844 loss: 2.4173219203948975\n",
      "step 9845 loss: 2.513969659805298\n",
      "step 9846 loss: 2.4976699352264404\n",
      "step 9847 loss: 2.5746726989746094\n",
      "step 9848 loss: 2.4825470447540283\n",
      "step 9849 loss: 2.528085231781006\n",
      "step 9850 loss: 2.4376492500305176\n",
      "step 9851 loss: 2.5036237239837646\n",
      "step 9852 loss: 2.521000385284424\n",
      "step 9853 loss: 2.658416986465454\n",
      "step 9854 loss: 2.441348075866699\n",
      "step 9855 loss: 2.447283983230591\n",
      "step 9856 loss: 2.433462142944336\n",
      "step 9857 loss: 2.5190327167510986\n",
      "step 9858 loss: 2.5379691123962402\n",
      "step 9859 loss: 2.4294981956481934\n",
      "step 9860 loss: 2.4687130451202393\n",
      "step 9861 loss: 2.5004515647888184\n",
      "step 9862 loss: 2.3521475791931152\n",
      "step 9863 loss: 2.50349497795105\n",
      "step 9864 loss: 2.6273183822631836\n",
      "step 9865 loss: 2.483315944671631\n",
      "step 9866 loss: 2.4510674476623535\n",
      "step 9867 loss: 2.4319539070129395\n",
      "step 9868 loss: 2.4290881156921387\n",
      "step 9869 loss: 2.366288185119629\n",
      "step 9870 loss: 2.4465181827545166\n",
      "step 9871 loss: 2.2952728271484375\n",
      "step 9872 loss: 2.621263027191162\n",
      "step 9873 loss: 2.397495746612549\n",
      "step 9874 loss: 2.5312764644622803\n",
      "step 9875 loss: 2.466494560241699\n",
      "step 9876 loss: 2.429795742034912\n",
      "step 9877 loss: 2.4671030044555664\n",
      "step 9878 loss: 2.427431106567383\n",
      "step 9879 loss: 2.358677387237549\n",
      "step 9880 loss: 2.4482483863830566\n",
      "step 9881 loss: 2.512381076812744\n",
      "step 9882 loss: 2.4742748737335205\n",
      "step 9883 loss: 2.4737348556518555\n",
      "step 9884 loss: 2.3844006061553955\n",
      "step 9885 loss: 2.5208823680877686\n",
      "step 9886 loss: 2.4669747352600098\n",
      "step 9887 loss: 2.3884201049804688\n",
      "step 9888 loss: 2.3458611965179443\n",
      "step 9889 loss: 2.5350120067596436\n",
      "step 9890 loss: 2.3940837383270264\n",
      "step 9891 loss: 2.5582494735717773\n",
      "step 9892 loss: 2.4279098510742188\n",
      "step 9893 loss: 2.599599599838257\n",
      "step 9894 loss: 2.3750879764556885\n",
      "step 9895 loss: 2.4374101161956787\n",
      "step 9896 loss: 2.46138596534729\n",
      "step 9897 loss: 2.5240654945373535\n",
      "step 9898 loss: 2.4739577770233154\n",
      "step 9899 loss: 2.5837459564208984\n",
      "step 9900 loss: 2.555168867111206\n",
      "step 9901 loss: 2.456563949584961\n",
      "step 9902 loss: 2.612368106842041\n",
      "step 9903 loss: 2.553384304046631\n",
      "step 9904 loss: 2.5857293605804443\n",
      "step 9905 loss: 2.495192527770996\n",
      "step 9906 loss: 2.4510836601257324\n",
      "step 9907 loss: 2.4496588706970215\n",
      "step 9908 loss: 2.5487864017486572\n",
      "step 9909 loss: 2.4871573448181152\n",
      "step 9910 loss: 2.5820460319519043\n",
      "step 9911 loss: 2.557509422302246\n",
      "step 9912 loss: 2.5172083377838135\n",
      "step 9913 loss: 2.4309346675872803\n",
      "step 9914 loss: 2.5425915718078613\n",
      "step 9915 loss: 2.522026300430298\n",
      "step 9916 loss: 2.4413514137268066\n",
      "step 9917 loss: 2.451286792755127\n",
      "step 9918 loss: 2.377112627029419\n",
      "step 9919 loss: 2.4610707759857178\n",
      "step 9920 loss: 2.4533839225769043\n",
      "step 9921 loss: 2.4334704875946045\n",
      "step 9922 loss: 2.586845636367798\n",
      "step 9923 loss: 2.4831702709198\n",
      "step 9924 loss: 2.3876168727874756\n",
      "step 9925 loss: 2.4065494537353516\n",
      "step 9926 loss: 2.442577362060547\n",
      "step 9927 loss: 2.626410484313965\n",
      "step 9928 loss: 2.4292707443237305\n",
      "step 9929 loss: 2.3795156478881836\n",
      "step 9930 loss: 2.461519718170166\n",
      "step 9931 loss: 2.3681468963623047\n",
      "step 9932 loss: 2.3508059978485107\n",
      "step 9933 loss: 2.4261844158172607\n",
      "step 9934 loss: 2.4640161991119385\n",
      "step 9935 loss: 2.4459471702575684\n",
      "step 9936 loss: 2.5588748455047607\n",
      "step 9937 loss: 2.4455864429473877\n",
      "step 9938 loss: 2.410200357437134\n",
      "step 9939 loss: 2.5142674446105957\n",
      "step 9940 loss: 2.5332629680633545\n",
      "step 9941 loss: 2.4482462406158447\n",
      "step 9942 loss: 2.5358848571777344\n",
      "step 9943 loss: 2.435288667678833\n",
      "step 9944 loss: 2.4530296325683594\n",
      "step 9945 loss: 2.426421642303467\n",
      "step 9946 loss: 2.408393621444702\n",
      "step 9947 loss: 2.4768614768981934\n",
      "step 9948 loss: 2.423658847808838\n",
      "step 9949 loss: 2.4604978561401367\n",
      "step 9950 loss: 2.352022171020508\n",
      "step 9951 loss: 2.616729736328125\n",
      "step 9952 loss: 2.438844680786133\n",
      "step 9953 loss: 2.3838815689086914\n",
      "step 9954 loss: 2.4250171184539795\n",
      "step 9955 loss: 2.527254104614258\n",
      "step 9956 loss: 2.4126899242401123\n",
      "step 9957 loss: 2.531003952026367\n",
      "step 9958 loss: 2.457232713699341\n",
      "step 9959 loss: 2.3739736080169678\n",
      "step 9960 loss: 2.6009209156036377\n",
      "step 9961 loss: 2.4741735458374023\n",
      "step 9962 loss: 2.5035433769226074\n",
      "step 9963 loss: 2.5980031490325928\n",
      "step 9964 loss: 2.4459853172302246\n",
      "step 9965 loss: 2.416569948196411\n",
      "step 9966 loss: 2.446298837661743\n",
      "step 9967 loss: 2.4373574256896973\n",
      "step 9968 loss: 2.4491865634918213\n",
      "step 9969 loss: 2.3583202362060547\n",
      "step 9970 loss: 2.5272154808044434\n",
      "step 9971 loss: 2.5690624713897705\n",
      "step 9972 loss: 2.4425108432769775\n",
      "step 9973 loss: 2.3575470447540283\n",
      "step 9974 loss: 2.3525846004486084\n",
      "step 9975 loss: 2.4572999477386475\n",
      "step 9976 loss: 2.5518462657928467\n",
      "step 9977 loss: 2.4869492053985596\n",
      "step 9978 loss: 2.4332406520843506\n",
      "step 9979 loss: 2.4381191730499268\n",
      "step 9980 loss: 2.4858953952789307\n",
      "step 9981 loss: 2.3604414463043213\n",
      "step 9982 loss: 2.4909651279449463\n",
      "step 9983 loss: 2.5129499435424805\n",
      "step 9984 loss: 2.3157684803009033\n",
      "step 9985 loss: 2.4188594818115234\n",
      "step 9986 loss: 2.470501184463501\n",
      "step 9987 loss: 2.5485379695892334\n",
      "step 9988 loss: 2.491631507873535\n",
      "step 9989 loss: 2.348879814147949\n",
      "step 9990 loss: 2.5741419792175293\n",
      "step 9991 loss: 2.424443483352661\n",
      "step 9992 loss: 2.3543453216552734\n",
      "step 9993 loss: 2.4115967750549316\n",
      "step 9994 loss: 2.4428446292877197\n",
      "step 9995 loss: 2.407186985015869\n",
      "step 9996 loss: 2.407933473587036\n",
      "step 9997 loss: 2.512389659881592\n",
      "step 9998 loss: 2.4176244735717773\n",
      "step 9999 loss: 2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"step {steps} loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecor\n"
     ]
    }
   ],
   "source": [
    "# after training, we have a much better result from the model\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), \n",
    "                                          dtype=torch.long), \n",
    "                                          max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing the self attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for self attention, we want to get the current token and all past tokens (not the future tokens)\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        # take average of all previous tokens up to the current token\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0) # (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei/wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x  # (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "\n",
    "print(xbow[0])\n",
    "# first row is same\n",
    "# second row is average of first two tokens\n",
    "# third row is average of first three tokens\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c= tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# this method would be very inefficient for large contexts\n",
    "# lets use matrix multiplication to do this more efficiently\n",
    "torch.manual_seed(42)\n",
    "a= torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a, dim=1, keepdim=True)\n",
    "b= torch.randint(0,10,(3,2)).float()\n",
    "c= a @ b\n",
    "\n",
    "print(\"a=\",a)\n",
    "print(\"---\")\n",
    "print(\"b=\",b)\n",
    "print(\"---\")\n",
    "print(\"c=\",c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "# scale dot product attention\n",
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn((B, T, C))\n",
    "\n",
    "head_size = 16 \n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = value(x)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count of notes about the attention mechanism\n",
    "- the implementation above is a \"self-attention\"decoder block where the past and current tokens are fused to get the value \n",
    "- for an encoder block, we can simply remove the masked_fill so the entire context is used to get the value \n",
    "- self attention just means that the query, key, and value come from the same source i.e. x\n",
    "- for cross attention, the query and key comes from a seperate encoder block (which is the gpt3 encoder decoder architecture)\n",
    "- scaled attention divides wei by the sqrt of the head size \n",
    "- why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0273)\n",
      "tensor(0.3190, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(wei.var())\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "print(wei.var())2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the head size is large, the variance of the wei is too large and the softmax will saturate\n",
    "# so we divide by the sqrt of the head size to normalize the variance\n",
    "# this is a hack to make the training more stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two hacks for training deep neural networks: \n",
    "- residual connections: add a seperate pathway for the gradients to flow through, this allows the residual blocks to learn only the residual (difference) between input x and output H(x)\n",
    "- layer normalization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we focus on layer normalization\n",
    "# lets start from the batch normalization code\n",
    "import torch\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.9):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True # different behavior based on training or validation \n",
    "        self.gamma = torch.ones((dim))\n",
    "        self.beta = torch.zeros((dim))\n",
    "        self.mean_running = torch.zeros((dim))\n",
    "        self.var_running = torch.ones((dim))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True) # batch variance\n",
    "        else:\n",
    "            xmean = self.mean_running\n",
    "            xvar = self.var_running\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # update the running mean and var (i.e. buffers in pytorch language)\n",
    "        with torch.no_grad(): # not using gradient descent for running mean and var\n",
    "            self.mean_running = self.momentum * xmean + (1 - self.momentum) * self.mean_running\n",
    "            self.var_running = self.momentum * xvar + (1 - self.momentum) * self.var_running\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn((32, 100))\n",
    "x = module(x)\n",
    "x.shape # batch size 32, 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4901e-08), tensor(1.0000))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0].mean(), x[:, 0].std()\n",
    "# so here each batch is normalized which means individual columns are normalized to 0 mean and 1 std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization is similar but instead of normalizing each column, it normalizes each row\n",
    "# also remove the momentum and the running mean and var \n",
    "# also remove the training flag since we dont need to distinguish between training and inference \n",
    "import torch\n",
    "class LayerNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones((dim))\n",
    "        self.beta = torch.zeros((dim))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim=True) # instead of taking the mean of the entire batch, take the mean of each row\n",
    "        xvar = x.var(1, keepdim=True) # instead of taking the variance of the entire batch, take the variance of each row\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1469) tensor(0.8803)\n",
      "tensor(2.3842e-09) tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# after change, the columns are not normalized, but rows are\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn((32, 100))\n",
    "x = module(x)\n",
    "\n",
    "print(x[:, 0].mean(), x[:, 0].std())\n",
    "print(x[0].mean(), x[0].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note: the transforer architecture has had one change in recent years\n",
    "- the add+norm layer is done before the attention (transformation) layer (this is called pre-norm formulation)\n",
    "- add dropout layer in various places to prevent overfitting, randoly drop the nodes at each training to regularize the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
