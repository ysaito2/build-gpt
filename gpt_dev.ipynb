{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-20 17:09:57--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M  3.39MB/s    in 0.3s    \n",
      "\n",
      "2025-02-20 17:09:58 (3.39 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters: 1115394\n",
      "\n",
      "First 1000 characters:\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset in characters: {len(text)}\")\n",
    "print(\"\\nFirst 1000 characters:\\n\")\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(' '.join(chars))\n",
    "print(f\"\\nVocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. tokenize the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and val\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement the bigram language model\n",
    "start from the simpliest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(loss) \n",
    "print(logits.shape)\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss: 4.704006195068359\n",
      "step 1 loss: 4.721118927001953\n",
      "step 2 loss: 4.653193473815918\n",
      "step 3 loss: 4.706261157989502\n",
      "step 4 loss: 4.780904293060303\n",
      "step 5 loss: 4.751267910003662\n",
      "step 6 loss: 4.8395490646362305\n",
      "step 7 loss: 4.667973041534424\n",
      "step 8 loss: 4.743716716766357\n",
      "step 9 loss: 4.774043083190918\n",
      "step 10 loss: 4.6908278465271\n",
      "step 11 loss: 4.789142608642578\n",
      "step 12 loss: 4.61777925491333\n",
      "step 13 loss: 4.650947093963623\n",
      "step 14 loss: 4.886447429656982\n",
      "step 15 loss: 4.703796863555908\n",
      "step 16 loss: 4.757591724395752\n",
      "step 17 loss: 4.655108451843262\n",
      "step 18 loss: 4.709283828735352\n",
      "step 19 loss: 4.6745147705078125\n",
      "step 20 loss: 4.760501384735107\n",
      "step 21 loss: 4.7892632484436035\n",
      "step 22 loss: 4.653748512268066\n",
      "step 23 loss: 4.6619181632995605\n",
      "step 24 loss: 4.673007488250732\n",
      "step 25 loss: 4.66577672958374\n",
      "step 26 loss: 4.7301106452941895\n",
      "step 27 loss: 4.755304336547852\n",
      "step 28 loss: 4.712186813354492\n",
      "step 29 loss: 4.745501518249512\n",
      "step 30 loss: 4.726755619049072\n",
      "step 31 loss: 4.735108375549316\n",
      "step 32 loss: 4.777461051940918\n",
      "step 33 loss: 4.643350601196289\n",
      "step 34 loss: 4.6651835441589355\n",
      "step 35 loss: 4.79764461517334\n",
      "step 36 loss: 4.717412948608398\n",
      "step 37 loss: 4.683647155761719\n",
      "step 38 loss: 4.81886100769043\n",
      "step 39 loss: 4.613771915435791\n",
      "step 40 loss: 4.573785781860352\n",
      "step 41 loss: 4.560741901397705\n",
      "step 42 loss: 4.81563138961792\n",
      "step 43 loss: 4.6061553955078125\n",
      "step 44 loss: 4.619696140289307\n",
      "step 45 loss: 4.725419521331787\n",
      "step 46 loss: 4.650487899780273\n",
      "step 47 loss: 4.5941481590271\n",
      "step 48 loss: 4.7202863693237305\n",
      "step 49 loss: 4.699342250823975\n",
      "step 50 loss: 4.6724138259887695\n",
      "step 51 loss: 4.727972984313965\n",
      "step 52 loss: 4.66152286529541\n",
      "step 53 loss: 4.616766929626465\n",
      "step 54 loss: 4.599857807159424\n",
      "step 55 loss: 4.6533403396606445\n",
      "step 56 loss: 4.716132164001465\n",
      "step 57 loss: 4.692666053771973\n",
      "step 58 loss: 4.6675333976745605\n",
      "step 59 loss: 4.655758857727051\n",
      "step 60 loss: 4.655789375305176\n",
      "step 61 loss: 4.754217624664307\n",
      "step 62 loss: 4.723147869110107\n",
      "step 63 loss: 4.617090702056885\n",
      "step 64 loss: 4.704502582550049\n",
      "step 65 loss: 4.752079486846924\n",
      "step 66 loss: 4.569591999053955\n",
      "step 67 loss: 4.547887802124023\n",
      "step 68 loss: 4.571099281311035\n",
      "step 69 loss: 4.569430828094482\n",
      "step 70 loss: 4.598389148712158\n",
      "step 71 loss: 4.547847747802734\n",
      "step 72 loss: 4.591439247131348\n",
      "step 73 loss: 4.6599297523498535\n",
      "step 74 loss: 4.623749732971191\n",
      "step 75 loss: 4.742475509643555\n",
      "step 76 loss: 4.644272804260254\n",
      "step 77 loss: 4.642965316772461\n",
      "step 78 loss: 4.579803943634033\n",
      "step 79 loss: 4.583383560180664\n",
      "step 80 loss: 4.721978187561035\n",
      "step 81 loss: 4.669403076171875\n",
      "step 82 loss: 4.630046367645264\n",
      "step 83 loss: 4.55496883392334\n",
      "step 84 loss: 4.681591987609863\n",
      "step 85 loss: 4.6749467849731445\n",
      "step 86 loss: 4.65585994720459\n",
      "step 87 loss: 4.669306755065918\n",
      "step 88 loss: 4.64952278137207\n",
      "step 89 loss: 4.674875736236572\n",
      "step 90 loss: 4.6534528732299805\n",
      "step 91 loss: 4.785573959350586\n",
      "step 92 loss: 4.731050968170166\n",
      "step 93 loss: 4.596864700317383\n",
      "step 94 loss: 4.586680889129639\n",
      "step 95 loss: 4.762486934661865\n",
      "step 96 loss: 4.741381645202637\n",
      "step 97 loss: 4.602567672729492\n",
      "step 98 loss: 4.662181854248047\n",
      "step 99 loss: 4.587916374206543\n",
      "step 100 loss: 4.658433437347412\n",
      "step 101 loss: 4.679844856262207\n",
      "step 102 loss: 4.538045883178711\n",
      "step 103 loss: 4.507107257843018\n",
      "step 104 loss: 4.527500629425049\n",
      "step 105 loss: 4.667482852935791\n",
      "step 106 loss: 4.598316192626953\n",
      "step 107 loss: 4.6995110511779785\n",
      "step 108 loss: 4.581157207489014\n",
      "step 109 loss: 4.587600231170654\n",
      "step 110 loss: 4.610328674316406\n",
      "step 111 loss: 4.557375907897949\n",
      "step 112 loss: 4.600196838378906\n",
      "step 113 loss: 4.6857194900512695\n",
      "step 114 loss: 4.5854692459106445\n",
      "step 115 loss: 4.586297512054443\n",
      "step 116 loss: 4.621298313140869\n",
      "step 117 loss: 4.562487602233887\n",
      "step 118 loss: 4.607450485229492\n",
      "step 119 loss: 4.634469509124756\n",
      "step 120 loss: 4.556134223937988\n",
      "step 121 loss: 4.522068500518799\n",
      "step 122 loss: 4.546227931976318\n",
      "step 123 loss: 4.494812488555908\n",
      "step 124 loss: 4.551519870758057\n",
      "step 125 loss: 4.553579330444336\n",
      "step 126 loss: 4.537591934204102\n",
      "step 127 loss: 4.570073127746582\n",
      "step 128 loss: 4.541749954223633\n",
      "step 129 loss: 4.5774383544921875\n",
      "step 130 loss: 4.4986491203308105\n",
      "step 131 loss: 4.58778715133667\n",
      "step 132 loss: 4.624891757965088\n",
      "step 133 loss: 4.654147624969482\n",
      "step 134 loss: 4.629645347595215\n",
      "step 135 loss: 4.542174339294434\n",
      "step 136 loss: 4.556922435760498\n",
      "step 137 loss: 4.67123556137085\n",
      "step 138 loss: 4.524987697601318\n",
      "step 139 loss: 4.647885799407959\n",
      "step 140 loss: 4.586472034454346\n",
      "step 141 loss: 4.497965335845947\n",
      "step 142 loss: 4.537078857421875\n",
      "step 143 loss: 4.596789836883545\n",
      "step 144 loss: 4.5720415115356445\n",
      "step 145 loss: 4.529770374298096\n",
      "step 146 loss: 4.618703365325928\n",
      "step 147 loss: 4.436441421508789\n",
      "step 148 loss: 4.529646396636963\n",
      "step 149 loss: 4.468570709228516\n",
      "step 150 loss: 4.515491962432861\n",
      "step 151 loss: 4.515642166137695\n",
      "step 152 loss: 4.576569557189941\n",
      "step 153 loss: 4.565048694610596\n",
      "step 154 loss: 4.5591278076171875\n",
      "step 155 loss: 4.499317169189453\n",
      "step 156 loss: 4.570872783660889\n",
      "step 157 loss: 4.498655796051025\n",
      "step 158 loss: 4.330718517303467\n",
      "step 159 loss: 4.601070404052734\n",
      "step 160 loss: 4.492514610290527\n",
      "step 161 loss: 4.609500885009766\n",
      "step 162 loss: 4.566488265991211\n",
      "step 163 loss: 4.514853000640869\n",
      "step 164 loss: 4.45876407623291\n",
      "step 165 loss: 4.6014084815979\n",
      "step 166 loss: 4.534083366394043\n",
      "step 167 loss: 4.526052474975586\n",
      "step 168 loss: 4.538899898529053\n",
      "step 169 loss: 4.713564872741699\n",
      "step 170 loss: 4.577315807342529\n",
      "step 171 loss: 4.494469165802002\n",
      "step 172 loss: 4.575506210327148\n",
      "step 173 loss: 4.493772983551025\n",
      "step 174 loss: 4.477363109588623\n",
      "step 175 loss: 4.524717330932617\n",
      "step 176 loss: 4.612197399139404\n",
      "step 177 loss: 4.5325117111206055\n",
      "step 178 loss: 4.5275092124938965\n",
      "step 179 loss: 4.500175476074219\n",
      "step 180 loss: 4.587535381317139\n",
      "step 181 loss: 4.467714786529541\n",
      "step 182 loss: 4.580933094024658\n",
      "step 183 loss: 4.567305564880371\n",
      "step 184 loss: 4.345602512359619\n",
      "step 185 loss: 4.449349880218506\n",
      "step 186 loss: 4.593079566955566\n",
      "step 187 loss: 4.48002290725708\n",
      "step 188 loss: 4.556005477905273\n",
      "step 189 loss: 4.474159240722656\n",
      "step 190 loss: 4.440937519073486\n",
      "step 191 loss: 4.430146217346191\n",
      "step 192 loss: 4.467485427856445\n",
      "step 193 loss: 4.475931644439697\n",
      "step 194 loss: 4.5216064453125\n",
      "step 195 loss: 4.486138820648193\n",
      "step 196 loss: 4.490366458892822\n",
      "step 197 loss: 4.462703227996826\n",
      "step 198 loss: 4.555963516235352\n",
      "step 199 loss: 4.485262870788574\n",
      "step 200 loss: 4.470171928405762\n",
      "step 201 loss: 4.474298000335693\n",
      "step 202 loss: 4.492599010467529\n",
      "step 203 loss: 4.549778461456299\n",
      "step 204 loss: 4.507059574127197\n",
      "step 205 loss: 4.519667625427246\n",
      "step 206 loss: 4.57536506652832\n",
      "step 207 loss: 4.4327921867370605\n",
      "step 208 loss: 4.514216899871826\n",
      "step 209 loss: 4.382073402404785\n",
      "step 210 loss: 4.390741348266602\n",
      "step 211 loss: 4.521138668060303\n",
      "step 212 loss: 4.573536396026611\n",
      "step 213 loss: 4.569770812988281\n",
      "step 214 loss: 4.453800678253174\n",
      "step 215 loss: 4.432650089263916\n",
      "step 216 loss: 4.508621692657471\n",
      "step 217 loss: 4.441047668457031\n",
      "step 218 loss: 4.565876007080078\n",
      "step 219 loss: 4.454717636108398\n",
      "step 220 loss: 4.4760847091674805\n",
      "step 221 loss: 4.500592231750488\n",
      "step 222 loss: 4.434113502502441\n",
      "step 223 loss: 4.429643154144287\n",
      "step 224 loss: 4.495214462280273\n",
      "step 225 loss: 4.439542770385742\n",
      "step 226 loss: 4.5203142166137695\n",
      "step 227 loss: 4.47563362121582\n",
      "step 228 loss: 4.429919242858887\n",
      "step 229 loss: 4.477661609649658\n",
      "step 230 loss: 4.450129985809326\n",
      "step 231 loss: 4.432395935058594\n",
      "step 232 loss: 4.455875396728516\n",
      "step 233 loss: 4.492155075073242\n",
      "step 234 loss: 4.459921836853027\n",
      "step 235 loss: 4.483809471130371\n",
      "step 236 loss: 4.447521686553955\n",
      "step 237 loss: 4.401882648468018\n",
      "step 238 loss: 4.394367218017578\n",
      "step 239 loss: 4.3660407066345215\n",
      "step 240 loss: 4.482306480407715\n",
      "step 241 loss: 4.391725540161133\n",
      "step 242 loss: 4.392483711242676\n",
      "step 243 loss: 4.376755237579346\n",
      "step 244 loss: 4.515342712402344\n",
      "step 245 loss: 4.53567361831665\n",
      "step 246 loss: 4.304040431976318\n",
      "step 247 loss: 4.43746280670166\n",
      "step 248 loss: 4.470716953277588\n",
      "step 249 loss: 4.569386959075928\n",
      "step 250 loss: 4.456277370452881\n",
      "step 251 loss: 4.4922966957092285\n",
      "step 252 loss: 4.448439121246338\n",
      "step 253 loss: 4.486955642700195\n",
      "step 254 loss: 4.459270477294922\n",
      "step 255 loss: 4.427060127258301\n",
      "step 256 loss: 4.353371620178223\n",
      "step 257 loss: 4.386946678161621\n",
      "step 258 loss: 4.431824684143066\n",
      "step 259 loss: 4.564876079559326\n",
      "step 260 loss: 4.418117523193359\n",
      "step 261 loss: 4.418209075927734\n",
      "step 262 loss: 4.333183288574219\n",
      "step 263 loss: 4.362520217895508\n",
      "step 264 loss: 4.4163007736206055\n",
      "step 265 loss: 4.55913782119751\n",
      "step 266 loss: 4.375359058380127\n",
      "step 267 loss: 4.409400463104248\n",
      "step 268 loss: 4.452630519866943\n",
      "step 269 loss: 4.478348255157471\n",
      "step 270 loss: 4.324471950531006\n",
      "step 271 loss: 4.456314563751221\n",
      "step 272 loss: 4.376288414001465\n",
      "step 273 loss: 4.293596267700195\n",
      "step 274 loss: 4.424172878265381\n",
      "step 275 loss: 4.428800106048584\n",
      "step 276 loss: 4.318821430206299\n",
      "step 277 loss: 4.461701393127441\n",
      "step 278 loss: 4.382500171661377\n",
      "step 279 loss: 4.421017646789551\n",
      "step 280 loss: 4.486953258514404\n",
      "step 281 loss: 4.482895374298096\n",
      "step 282 loss: 4.450580596923828\n",
      "step 283 loss: 4.492319107055664\n",
      "step 284 loss: 4.427441120147705\n",
      "step 285 loss: 4.386728763580322\n",
      "step 286 loss: 4.345512390136719\n",
      "step 287 loss: 4.3661956787109375\n",
      "step 288 loss: 4.318299293518066\n",
      "step 289 loss: 4.387112617492676\n",
      "step 290 loss: 4.4888763427734375\n",
      "step 291 loss: 4.357933521270752\n",
      "step 292 loss: 4.327674388885498\n",
      "step 293 loss: 4.40096378326416\n",
      "step 294 loss: 4.353971481323242\n",
      "step 295 loss: 4.3568291664123535\n",
      "step 296 loss: 4.342477321624756\n",
      "step 297 loss: 4.266013145446777\n",
      "step 298 loss: 4.337876796722412\n",
      "step 299 loss: 4.344598293304443\n",
      "step 300 loss: 4.320702075958252\n",
      "step 301 loss: 4.432687759399414\n",
      "step 302 loss: 4.452409267425537\n",
      "step 303 loss: 4.398097038269043\n",
      "step 304 loss: 4.406829833984375\n",
      "step 305 loss: 4.409237384796143\n",
      "step 306 loss: 4.325584411621094\n",
      "step 307 loss: 4.2630791664123535\n",
      "step 308 loss: 4.315336227416992\n",
      "step 309 loss: 4.342366695404053\n",
      "step 310 loss: 4.359736442565918\n",
      "step 311 loss: 4.297989368438721\n",
      "step 312 loss: 4.4174652099609375\n",
      "step 313 loss: 4.358924388885498\n",
      "step 314 loss: 4.39406681060791\n",
      "step 315 loss: 4.323540210723877\n",
      "step 316 loss: 4.425510406494141\n",
      "step 317 loss: 4.374497890472412\n",
      "step 318 loss: 4.39720344543457\n",
      "step 319 loss: 4.51607608795166\n",
      "step 320 loss: 4.3706231117248535\n",
      "step 321 loss: 4.3398542404174805\n",
      "step 322 loss: 4.36796236038208\n",
      "step 323 loss: 4.315086364746094\n",
      "step 324 loss: 4.464062213897705\n",
      "step 325 loss: 4.437236785888672\n",
      "step 326 loss: 4.310821056365967\n",
      "step 327 loss: 4.3863701820373535\n",
      "step 328 loss: 4.4178690910339355\n",
      "step 329 loss: 4.355533123016357\n",
      "step 330 loss: 4.301799297332764\n",
      "step 331 loss: 4.43022346496582\n",
      "step 332 loss: 4.357127666473389\n",
      "step 333 loss: 4.347062587738037\n",
      "step 334 loss: 4.356966972351074\n",
      "step 335 loss: 4.303592681884766\n",
      "step 336 loss: 4.331826210021973\n",
      "step 337 loss: 4.3977179527282715\n",
      "step 338 loss: 4.3371357917785645\n",
      "step 339 loss: 4.4080810546875\n",
      "step 340 loss: 4.423590183258057\n",
      "step 341 loss: 4.3682332038879395\n",
      "step 342 loss: 4.305820941925049\n",
      "step 343 loss: 4.368200778961182\n",
      "step 344 loss: 4.241950511932373\n",
      "step 345 loss: 4.32410192489624\n",
      "step 346 loss: 4.332009315490723\n",
      "step 347 loss: 4.370065212249756\n",
      "step 348 loss: 4.332385063171387\n",
      "step 349 loss: 4.219411849975586\n",
      "step 350 loss: 4.232100009918213\n",
      "step 351 loss: 4.314776420593262\n",
      "step 352 loss: 4.29428243637085\n",
      "step 353 loss: 4.303083419799805\n",
      "step 354 loss: 4.390524864196777\n",
      "step 355 loss: 4.338705539703369\n",
      "step 356 loss: 4.346969127655029\n",
      "step 357 loss: 4.196526527404785\n",
      "step 358 loss: 4.296916961669922\n",
      "step 359 loss: 4.326120853424072\n",
      "step 360 loss: 4.2620134353637695\n",
      "step 361 loss: 4.292870998382568\n",
      "step 362 loss: 4.328018665313721\n",
      "step 363 loss: 4.252666473388672\n",
      "step 364 loss: 4.3246307373046875\n",
      "step 365 loss: 4.314058303833008\n",
      "step 366 loss: 4.300394058227539\n",
      "step 367 loss: 4.383544445037842\n",
      "step 368 loss: 4.257555961608887\n",
      "step 369 loss: 4.264247417449951\n",
      "step 370 loss: 4.288216590881348\n",
      "step 371 loss: 4.337155818939209\n",
      "step 372 loss: 4.292644023895264\n",
      "step 373 loss: 4.307545185089111\n",
      "step 374 loss: 4.275758743286133\n",
      "step 375 loss: 4.360841751098633\n",
      "step 376 loss: 4.295166015625\n",
      "step 377 loss: 4.333244800567627\n",
      "step 378 loss: 4.204632759094238\n",
      "step 379 loss: 4.2867326736450195\n",
      "step 380 loss: 4.341768741607666\n",
      "step 381 loss: 4.3797831535339355\n",
      "step 382 loss: 4.331485271453857\n",
      "step 383 loss: 4.216884613037109\n",
      "step 384 loss: 4.288538932800293\n",
      "step 385 loss: 4.301705360412598\n",
      "step 386 loss: 4.342240333557129\n",
      "step 387 loss: 4.252357006072998\n",
      "step 388 loss: 4.238476276397705\n",
      "step 389 loss: 4.216968536376953\n",
      "step 390 loss: 4.273195743560791\n",
      "step 391 loss: 4.257238864898682\n",
      "step 392 loss: 4.287950038909912\n",
      "step 393 loss: 4.266450881958008\n",
      "step 394 loss: 4.1770243644714355\n",
      "step 395 loss: 4.316140651702881\n",
      "step 396 loss: 4.245153903961182\n",
      "step 397 loss: 4.328152179718018\n",
      "step 398 loss: 4.292815685272217\n",
      "step 399 loss: 4.289951801300049\n",
      "step 400 loss: 4.252743721008301\n",
      "step 401 loss: 4.30411958694458\n",
      "step 402 loss: 4.242709636688232\n",
      "step 403 loss: 4.225958824157715\n",
      "step 404 loss: 4.1903228759765625\n",
      "step 405 loss: 4.30992317199707\n",
      "step 406 loss: 4.259286403656006\n",
      "step 407 loss: 4.125640392303467\n",
      "step 408 loss: 4.287714958190918\n",
      "step 409 loss: 4.248891353607178\n",
      "step 410 loss: 4.23568058013916\n",
      "step 411 loss: 4.251068592071533\n",
      "step 412 loss: 4.223715305328369\n",
      "step 413 loss: 4.299403667449951\n",
      "step 414 loss: 4.20652961730957\n",
      "step 415 loss: 4.279550552368164\n",
      "step 416 loss: 4.3169074058532715\n",
      "step 417 loss: 4.208868503570557\n",
      "step 418 loss: 4.196509838104248\n",
      "step 419 loss: 4.230629920959473\n",
      "step 420 loss: 4.229272365570068\n",
      "step 421 loss: 4.275476455688477\n",
      "step 422 loss: 4.275202751159668\n",
      "step 423 loss: 4.2220659255981445\n",
      "step 424 loss: 4.210690498352051\n",
      "step 425 loss: 4.223529815673828\n",
      "step 426 loss: 4.183196544647217\n",
      "step 427 loss: 4.240714073181152\n",
      "step 428 loss: 4.241266250610352\n",
      "step 429 loss: 4.292657375335693\n",
      "step 430 loss: 4.313097953796387\n",
      "step 431 loss: 4.24993896484375\n",
      "step 432 loss: 4.199017524719238\n",
      "step 433 loss: 4.221012115478516\n",
      "step 434 loss: 4.328307151794434\n",
      "step 435 loss: 4.193411827087402\n",
      "step 436 loss: 4.1603169441223145\n",
      "step 437 loss: 4.242465019226074\n",
      "step 438 loss: 4.3096537590026855\n",
      "step 439 loss: 4.253547191619873\n",
      "step 440 loss: 4.132005214691162\n",
      "step 441 loss: 4.314097881317139\n",
      "step 442 loss: 4.23518705368042\n",
      "step 443 loss: 4.296624660491943\n",
      "step 444 loss: 4.21980094909668\n",
      "step 445 loss: 4.221186637878418\n",
      "step 446 loss: 4.336780548095703\n",
      "step 447 loss: 4.135269641876221\n",
      "step 448 loss: 4.215968608856201\n",
      "step 449 loss: 4.236926078796387\n",
      "step 450 loss: 4.335793972015381\n",
      "step 451 loss: 4.12945032119751\n",
      "step 452 loss: 4.24487829208374\n",
      "step 453 loss: 4.126373767852783\n",
      "step 454 loss: 4.145634651184082\n",
      "step 455 loss: 4.202071666717529\n",
      "step 456 loss: 4.305419921875\n",
      "step 457 loss: 4.238638401031494\n",
      "step 458 loss: 4.4081950187683105\n",
      "step 459 loss: 4.171300888061523\n",
      "step 460 loss: 4.250331401824951\n",
      "step 461 loss: 4.2414984703063965\n",
      "step 462 loss: 4.243366718292236\n",
      "step 463 loss: 4.242072105407715\n",
      "step 464 loss: 4.282060623168945\n",
      "step 465 loss: 4.3097615242004395\n",
      "step 466 loss: 4.082497596740723\n",
      "step 467 loss: 4.148505210876465\n",
      "step 468 loss: 4.248476505279541\n",
      "step 469 loss: 4.191603183746338\n",
      "step 470 loss: 4.153619289398193\n",
      "step 471 loss: 4.2981791496276855\n",
      "step 472 loss: 4.2974772453308105\n",
      "step 473 loss: 4.187955856323242\n",
      "step 474 loss: 4.345261573791504\n",
      "step 475 loss: 4.284713268280029\n",
      "step 476 loss: 4.263484477996826\n",
      "step 477 loss: 4.2056803703308105\n",
      "step 478 loss: 4.179148197174072\n",
      "step 479 loss: 4.173160076141357\n",
      "step 480 loss: 4.1298041343688965\n",
      "step 481 loss: 4.236143112182617\n",
      "step 482 loss: 4.175360679626465\n",
      "step 483 loss: 4.163142204284668\n",
      "step 484 loss: 4.247429370880127\n",
      "step 485 loss: 4.249817371368408\n",
      "step 486 loss: 4.160401344299316\n",
      "step 487 loss: 4.236296653747559\n",
      "step 488 loss: 4.160035610198975\n",
      "step 489 loss: 4.113036632537842\n",
      "step 490 loss: 4.207566738128662\n",
      "step 491 loss: 4.1977691650390625\n",
      "step 492 loss: 4.133930683135986\n",
      "step 493 loss: 4.212961673736572\n",
      "step 494 loss: 4.189043045043945\n",
      "step 495 loss: 4.191688060760498\n",
      "step 496 loss: 4.183392524719238\n",
      "step 497 loss: 4.195455551147461\n",
      "step 498 loss: 4.1344523429870605\n",
      "step 499 loss: 4.187105655670166\n",
      "step 500 loss: 4.241008758544922\n",
      "step 501 loss: 4.203170299530029\n",
      "step 502 loss: 4.241356372833252\n",
      "step 503 loss: 4.182045936584473\n",
      "step 504 loss: 4.191044330596924\n",
      "step 505 loss: 4.220983028411865\n",
      "step 506 loss: 4.171088218688965\n",
      "step 507 loss: 4.167454719543457\n",
      "step 508 loss: 4.131077766418457\n",
      "step 509 loss: 4.209474563598633\n",
      "step 510 loss: 4.108231544494629\n",
      "step 511 loss: 4.13828706741333\n",
      "step 512 loss: 4.129660606384277\n",
      "step 513 loss: 4.103019714355469\n",
      "step 514 loss: 4.183014869689941\n",
      "step 515 loss: 4.184825897216797\n",
      "step 516 loss: 4.134787082672119\n",
      "step 517 loss: 4.215568542480469\n",
      "step 518 loss: 4.16798734664917\n",
      "step 519 loss: 4.277894496917725\n",
      "step 520 loss: 4.153351783752441\n",
      "step 521 loss: 4.126376628875732\n",
      "step 522 loss: 4.138455390930176\n",
      "step 523 loss: 4.106184482574463\n",
      "step 524 loss: 4.130945205688477\n",
      "step 525 loss: 4.3001298904418945\n",
      "step 526 loss: 4.176097869873047\n",
      "step 527 loss: 4.135159015655518\n",
      "step 528 loss: 4.170862674713135\n",
      "step 529 loss: 4.090285301208496\n",
      "step 530 loss: 4.185361385345459\n",
      "step 531 loss: 4.143759727478027\n",
      "step 532 loss: 4.155360221862793\n",
      "step 533 loss: 4.0344038009643555\n",
      "step 534 loss: 4.0667290687561035\n",
      "step 535 loss: 4.222439765930176\n",
      "step 536 loss: 4.082912921905518\n",
      "step 537 loss: 4.129242897033691\n",
      "step 538 loss: 4.132920265197754\n",
      "step 539 loss: 4.161780834197998\n",
      "step 540 loss: 4.137168884277344\n",
      "step 541 loss: 4.107937812805176\n",
      "step 542 loss: 4.139737129211426\n",
      "step 543 loss: 4.2971391677856445\n",
      "step 544 loss: 4.118070125579834\n",
      "step 545 loss: 4.147751331329346\n",
      "step 546 loss: 4.209001541137695\n",
      "step 547 loss: 4.1749773025512695\n",
      "step 548 loss: 4.059118270874023\n",
      "step 549 loss: 4.131842136383057\n",
      "step 550 loss: 4.126287460327148\n",
      "step 551 loss: 3.978708505630493\n",
      "step 552 loss: 4.170923709869385\n",
      "step 553 loss: 4.07435941696167\n",
      "step 554 loss: 4.06634521484375\n",
      "step 555 loss: 4.192716121673584\n",
      "step 556 loss: 4.119386196136475\n",
      "step 557 loss: 4.039763927459717\n",
      "step 558 loss: 4.06020450592041\n",
      "step 559 loss: 4.08280086517334\n",
      "step 560 loss: 4.2289533615112305\n",
      "step 561 loss: 4.094883441925049\n",
      "step 562 loss: 4.144589900970459\n",
      "step 563 loss: 4.16586446762085\n",
      "step 564 loss: 4.116443634033203\n",
      "step 565 loss: 4.116353511810303\n",
      "step 566 loss: 4.104300498962402\n",
      "step 567 loss: 4.026384353637695\n",
      "step 568 loss: 4.115374565124512\n",
      "step 569 loss: 4.050411224365234\n",
      "step 570 loss: 4.137031078338623\n",
      "step 571 loss: 4.173072814941406\n",
      "step 572 loss: 4.151785373687744\n",
      "step 573 loss: 4.059543132781982\n",
      "step 574 loss: 4.0650482177734375\n",
      "step 575 loss: 4.067065238952637\n",
      "step 576 loss: 4.057379245758057\n",
      "step 577 loss: 4.041965007781982\n",
      "step 578 loss: 4.1370930671691895\n",
      "step 579 loss: 4.08039665222168\n",
      "step 580 loss: 4.123457908630371\n",
      "step 581 loss: 4.16098165512085\n",
      "step 582 loss: 4.146056652069092\n",
      "step 583 loss: 4.113654136657715\n",
      "step 584 loss: 4.0880022048950195\n",
      "step 585 loss: 4.091946601867676\n",
      "step 586 loss: 4.0134429931640625\n",
      "step 587 loss: 4.0536675453186035\n",
      "step 588 loss: 4.108937740325928\n",
      "step 589 loss: 4.006382465362549\n",
      "step 590 loss: 3.9008288383483887\n",
      "step 591 loss: 4.051321983337402\n",
      "step 592 loss: 4.134636878967285\n",
      "step 593 loss: 4.135444641113281\n",
      "step 594 loss: 4.14675760269165\n",
      "step 595 loss: 4.121784687042236\n",
      "step 596 loss: 4.015590667724609\n",
      "step 597 loss: 4.062835693359375\n",
      "step 598 loss: 4.189814567565918\n",
      "step 599 loss: 4.166134834289551\n",
      "step 600 loss: 4.161406517028809\n",
      "step 601 loss: 4.220907211303711\n",
      "step 602 loss: 4.046063423156738\n",
      "step 603 loss: 4.173744201660156\n",
      "step 604 loss: 4.012446403503418\n",
      "step 605 loss: 4.047348976135254\n",
      "step 606 loss: 3.9869141578674316\n",
      "step 607 loss: 4.071816921234131\n",
      "step 608 loss: 4.185130596160889\n",
      "step 609 loss: 4.011745452880859\n",
      "step 610 loss: 4.085838794708252\n",
      "step 611 loss: 4.124237060546875\n",
      "step 612 loss: 4.102682590484619\n",
      "step 613 loss: 4.046851634979248\n",
      "step 614 loss: 3.963552236557007\n",
      "step 615 loss: 4.031641483306885\n",
      "step 616 loss: 3.9672510623931885\n",
      "step 617 loss: 4.042137145996094\n",
      "step 618 loss: 4.079298496246338\n",
      "step 619 loss: 4.09992790222168\n",
      "step 620 loss: 4.128066062927246\n",
      "step 621 loss: 4.043214321136475\n",
      "step 622 loss: 4.060523509979248\n",
      "step 623 loss: 4.01271390914917\n",
      "step 624 loss: 4.077670097351074\n",
      "step 625 loss: 4.0671772956848145\n",
      "step 626 loss: 4.183566570281982\n",
      "step 627 loss: 4.072885513305664\n",
      "step 628 loss: 4.039381504058838\n",
      "step 629 loss: 4.047631740570068\n",
      "step 630 loss: 3.976364850997925\n",
      "step 631 loss: 4.000668525695801\n",
      "step 632 loss: 4.016371250152588\n",
      "step 633 loss: 4.0280070304870605\n",
      "step 634 loss: 4.061502933502197\n",
      "step 635 loss: 4.1014227867126465\n",
      "step 636 loss: 4.114346981048584\n",
      "step 637 loss: 4.0520124435424805\n",
      "step 638 loss: 4.061122417449951\n",
      "step 639 loss: 4.037114143371582\n",
      "step 640 loss: 3.997882604598999\n",
      "step 641 loss: 4.04996919631958\n",
      "step 642 loss: 4.019729137420654\n",
      "step 643 loss: 4.034851551055908\n",
      "step 644 loss: 4.015679836273193\n",
      "step 645 loss: 4.122811794281006\n",
      "step 646 loss: 4.030985355377197\n",
      "step 647 loss: 4.01088285446167\n",
      "step 648 loss: 3.996788501739502\n",
      "step 649 loss: 4.011423587799072\n",
      "step 650 loss: 3.9944283962249756\n",
      "step 651 loss: 3.9242653846740723\n",
      "step 652 loss: 4.110482692718506\n",
      "step 653 loss: 4.118231773376465\n",
      "step 654 loss: 4.013514041900635\n",
      "step 655 loss: 4.014627933502197\n",
      "step 656 loss: 3.9875717163085938\n",
      "step 657 loss: 4.0062665939331055\n",
      "step 658 loss: 4.0788397789001465\n",
      "step 659 loss: 4.053842067718506\n",
      "step 660 loss: 3.83886456489563\n",
      "step 661 loss: 4.086469650268555\n",
      "step 662 loss: 4.042399883270264\n",
      "step 663 loss: 3.9549317359924316\n",
      "step 664 loss: 3.9786388874053955\n",
      "step 665 loss: 4.025195598602295\n",
      "step 666 loss: 3.9420788288116455\n",
      "step 667 loss: 4.016241073608398\n",
      "step 668 loss: 3.9646685123443604\n",
      "step 669 loss: 4.080589294433594\n",
      "step 670 loss: 3.9078826904296875\n",
      "step 671 loss: 4.003201961517334\n",
      "step 672 loss: 4.012451648712158\n",
      "step 673 loss: 3.8875606060028076\n",
      "step 674 loss: 4.001824378967285\n",
      "step 675 loss: 4.11176061630249\n",
      "step 676 loss: 3.982671022415161\n",
      "step 677 loss: 4.0705132484436035\n",
      "step 678 loss: 4.046138286590576\n",
      "step 679 loss: 3.994206190109253\n",
      "step 680 loss: 4.0237507820129395\n",
      "step 681 loss: 3.983790397644043\n",
      "step 682 loss: 3.943782329559326\n",
      "step 683 loss: 3.968172788619995\n",
      "step 684 loss: 4.090149402618408\n",
      "step 685 loss: 4.048746109008789\n",
      "step 686 loss: 4.009445667266846\n",
      "step 687 loss: 3.962519645690918\n",
      "step 688 loss: 3.973649263381958\n",
      "step 689 loss: 3.9910852909088135\n",
      "step 690 loss: 3.905243158340454\n",
      "step 691 loss: 4.058043956756592\n",
      "step 692 loss: 4.14483118057251\n",
      "step 693 loss: 4.07642126083374\n",
      "step 694 loss: 4.076528549194336\n",
      "step 695 loss: 3.967332124710083\n",
      "step 696 loss: 3.943429470062256\n",
      "step 697 loss: 4.04005765914917\n",
      "step 698 loss: 3.924569606781006\n",
      "step 699 loss: 3.9607315063476562\n",
      "step 700 loss: 4.044336795806885\n",
      "step 701 loss: 3.9933393001556396\n",
      "step 702 loss: 3.92348313331604\n",
      "step 703 loss: 3.9715981483459473\n",
      "step 704 loss: 3.914837598800659\n",
      "step 705 loss: 4.024702548980713\n",
      "step 706 loss: 4.035682678222656\n",
      "step 707 loss: 4.139660358428955\n",
      "step 708 loss: 4.00142240524292\n",
      "step 709 loss: 3.9489877223968506\n",
      "step 710 loss: 3.9222586154937744\n",
      "step 711 loss: 4.008349895477295\n",
      "step 712 loss: 3.9790799617767334\n",
      "step 713 loss: 3.9534382820129395\n",
      "step 714 loss: 4.030018329620361\n",
      "step 715 loss: 3.99031400680542\n",
      "step 716 loss: 4.078669548034668\n",
      "step 717 loss: 3.8939573764801025\n",
      "step 718 loss: 4.037815093994141\n",
      "step 719 loss: 3.9522030353546143\n",
      "step 720 loss: 3.9099180698394775\n",
      "step 721 loss: 3.9152495861053467\n",
      "step 722 loss: 3.9796078205108643\n",
      "step 723 loss: 3.9519968032836914\n",
      "step 724 loss: 3.9523537158966064\n",
      "step 725 loss: 4.0057454109191895\n",
      "step 726 loss: 3.951666831970215\n",
      "step 727 loss: 3.9850854873657227\n",
      "step 728 loss: 3.857304811477661\n",
      "step 729 loss: 3.9445111751556396\n",
      "step 730 loss: 4.017353534698486\n",
      "step 731 loss: 3.9378905296325684\n",
      "step 732 loss: 3.940047025680542\n",
      "step 733 loss: 3.955509662628174\n",
      "step 734 loss: 3.9435293674468994\n",
      "step 735 loss: 3.8927388191223145\n",
      "step 736 loss: 3.914546251296997\n",
      "step 737 loss: 3.890690326690674\n",
      "step 738 loss: 3.9116694927215576\n",
      "step 739 loss: 3.984910249710083\n",
      "step 740 loss: 3.9705166816711426\n",
      "step 741 loss: 4.001867771148682\n",
      "step 742 loss: 3.8392083644866943\n",
      "step 743 loss: 3.90970778465271\n",
      "step 744 loss: 3.9352409839630127\n",
      "step 745 loss: 3.9633171558380127\n",
      "step 746 loss: 4.0142621994018555\n",
      "step 747 loss: 3.952476739883423\n",
      "step 748 loss: 3.892951011657715\n",
      "step 749 loss: 3.8761167526245117\n",
      "step 750 loss: 3.8449132442474365\n",
      "step 751 loss: 3.94313383102417\n",
      "step 752 loss: 3.984642267227173\n",
      "step 753 loss: 3.847524404525757\n",
      "step 754 loss: 3.954896926879883\n",
      "step 755 loss: 3.8268864154815674\n",
      "step 756 loss: 3.9071507453918457\n",
      "step 757 loss: 3.955454111099243\n",
      "step 758 loss: 3.971069097518921\n",
      "step 759 loss: 3.99383282661438\n",
      "step 760 loss: 3.9616293907165527\n",
      "step 761 loss: 3.8796799182891846\n",
      "step 762 loss: 3.8662948608398438\n",
      "step 763 loss: 3.8481481075286865\n",
      "step 764 loss: 3.9333536624908447\n",
      "step 765 loss: 3.8907876014709473\n",
      "step 766 loss: 3.7697365283966064\n",
      "step 767 loss: 3.983266830444336\n",
      "step 768 loss: 3.8987653255462646\n",
      "step 769 loss: 3.897134304046631\n",
      "step 770 loss: 3.9567453861236572\n",
      "step 771 loss: 4.013120651245117\n",
      "step 772 loss: 3.9661242961883545\n",
      "step 773 loss: 3.931126356124878\n",
      "step 774 loss: 3.8733646869659424\n",
      "step 775 loss: 3.8448598384857178\n",
      "step 776 loss: 3.911426544189453\n",
      "step 777 loss: 3.9176671504974365\n",
      "step 778 loss: 3.8600244522094727\n",
      "step 779 loss: 3.881734848022461\n",
      "step 780 loss: 3.971513032913208\n",
      "step 781 loss: 3.910503625869751\n",
      "step 782 loss: 3.9087677001953125\n",
      "step 783 loss: 3.8960015773773193\n",
      "step 784 loss: 3.7988874912261963\n",
      "step 785 loss: 3.890516519546509\n",
      "step 786 loss: 3.9300410747528076\n",
      "step 787 loss: 3.883873224258423\n",
      "step 788 loss: 3.9379279613494873\n",
      "step 789 loss: 3.787778377532959\n",
      "step 790 loss: 3.9234354496002197\n",
      "step 791 loss: 3.920180559158325\n",
      "step 792 loss: 3.9769718647003174\n",
      "step 793 loss: 3.8104124069213867\n",
      "step 794 loss: 3.886444330215454\n",
      "step 795 loss: 3.867879867553711\n",
      "step 796 loss: 3.8430237770080566\n",
      "step 797 loss: 3.9011383056640625\n",
      "step 798 loss: 3.8764278888702393\n",
      "step 799 loss: 3.853212356567383\n",
      "step 800 loss: 4.091874122619629\n",
      "step 801 loss: 3.7964301109313965\n",
      "step 802 loss: 3.8136682510375977\n",
      "step 803 loss: 3.911313533782959\n",
      "step 804 loss: 3.8497474193573\n",
      "step 805 loss: 3.876377582550049\n",
      "step 806 loss: 3.939631223678589\n",
      "step 807 loss: 3.8660216331481934\n",
      "step 808 loss: 3.899229049682617\n",
      "step 809 loss: 3.9984078407287598\n",
      "step 810 loss: 3.913729429244995\n",
      "step 811 loss: 3.861097812652588\n",
      "step 812 loss: 3.8573079109191895\n",
      "step 813 loss: 3.979907512664795\n",
      "step 814 loss: 3.889765739440918\n",
      "step 815 loss: 3.8157804012298584\n",
      "step 816 loss: 3.827575445175171\n",
      "step 817 loss: 3.992100715637207\n",
      "step 818 loss: 3.816178798675537\n",
      "step 819 loss: 3.9157137870788574\n",
      "step 820 loss: 3.8982222080230713\n",
      "step 821 loss: 3.835663080215454\n",
      "step 822 loss: 3.855438709259033\n",
      "step 823 loss: 3.855520248413086\n",
      "step 824 loss: 3.923577070236206\n",
      "step 825 loss: 3.9133663177490234\n",
      "step 826 loss: 3.914663553237915\n",
      "step 827 loss: 3.967420816421509\n",
      "step 828 loss: 3.830130100250244\n",
      "step 829 loss: 3.87532639503479\n",
      "step 830 loss: 3.9086313247680664\n",
      "step 831 loss: 3.8405354022979736\n",
      "step 832 loss: 3.9321916103363037\n",
      "step 833 loss: 3.825958728790283\n",
      "step 834 loss: 3.819948196411133\n",
      "step 835 loss: 3.8718132972717285\n",
      "step 836 loss: 3.907766819000244\n",
      "step 837 loss: 3.773632287979126\n",
      "step 838 loss: 3.8466389179229736\n",
      "step 839 loss: 3.853872299194336\n",
      "step 840 loss: 3.8514111042022705\n",
      "step 841 loss: 3.8162355422973633\n",
      "step 842 loss: 3.914602279663086\n",
      "step 843 loss: 3.9167592525482178\n",
      "step 844 loss: 3.8536086082458496\n",
      "step 845 loss: 3.9334263801574707\n",
      "step 846 loss: 3.935450315475464\n",
      "step 847 loss: 3.842902421951294\n",
      "step 848 loss: 3.8955814838409424\n",
      "step 849 loss: 3.861664056777954\n",
      "step 850 loss: 3.786890745162964\n",
      "step 851 loss: 3.818082809448242\n",
      "step 852 loss: 3.878074884414673\n",
      "step 853 loss: 3.8595242500305176\n",
      "step 854 loss: 3.758294105529785\n",
      "step 855 loss: 3.9402174949645996\n",
      "step 856 loss: 3.8833606243133545\n",
      "step 857 loss: 3.7524325847625732\n",
      "step 858 loss: 3.9947397708892822\n",
      "step 859 loss: 3.7760915756225586\n",
      "step 860 loss: 3.9155192375183105\n",
      "step 861 loss: 3.7810893058776855\n",
      "step 862 loss: 3.9591102600097656\n",
      "step 863 loss: 3.7640063762664795\n",
      "step 864 loss: 3.88435697555542\n",
      "step 865 loss: 3.9048147201538086\n",
      "step 866 loss: 3.8947560787200928\n",
      "step 867 loss: 3.854034423828125\n",
      "step 868 loss: 3.860032081604004\n",
      "step 869 loss: 3.7541089057922363\n",
      "step 870 loss: 3.8346974849700928\n",
      "step 871 loss: 3.870063304901123\n",
      "step 872 loss: 3.81599760055542\n",
      "step 873 loss: 3.949742078781128\n",
      "step 874 loss: 3.8479371070861816\n",
      "step 875 loss: 3.8559534549713135\n",
      "step 876 loss: 3.772221803665161\n",
      "step 877 loss: 3.803873300552368\n",
      "step 878 loss: 3.9097228050231934\n",
      "step 879 loss: 3.899336576461792\n",
      "step 880 loss: 3.895540237426758\n",
      "step 881 loss: 3.952613592147827\n",
      "step 882 loss: 3.792426586151123\n",
      "step 883 loss: 3.7160146236419678\n",
      "step 884 loss: 3.8731722831726074\n",
      "step 885 loss: 3.8485169410705566\n",
      "step 886 loss: 3.6980462074279785\n",
      "step 887 loss: 3.7697324752807617\n",
      "step 888 loss: 3.876688003540039\n",
      "step 889 loss: 3.7508533000946045\n",
      "step 890 loss: 3.838090658187866\n",
      "step 891 loss: 3.8375675678253174\n",
      "step 892 loss: 3.7868475914001465\n",
      "step 893 loss: 3.7474727630615234\n",
      "step 894 loss: 3.8088219165802\n",
      "step 895 loss: 3.800238847732544\n",
      "step 896 loss: 3.7082066535949707\n",
      "step 897 loss: 3.820268154144287\n",
      "step 898 loss: 3.755006790161133\n",
      "step 899 loss: 3.8472073078155518\n",
      "step 900 loss: 3.7458465099334717\n",
      "step 901 loss: 3.6638474464416504\n",
      "step 902 loss: 3.7747509479522705\n",
      "step 903 loss: 3.780081272125244\n",
      "step 904 loss: 3.7829368114471436\n",
      "step 905 loss: 3.736419200897217\n",
      "step 906 loss: 3.850062608718872\n",
      "step 907 loss: 3.8312721252441406\n",
      "step 908 loss: 3.7938942909240723\n",
      "step 909 loss: 3.7926952838897705\n",
      "step 910 loss: 3.8269946575164795\n",
      "step 911 loss: 3.8559958934783936\n",
      "step 912 loss: 3.7840473651885986\n",
      "step 913 loss: 3.7321388721466064\n",
      "step 914 loss: 3.784118413925171\n",
      "step 915 loss: 3.756901979446411\n",
      "step 916 loss: 3.7629847526550293\n",
      "step 917 loss: 3.846207618713379\n",
      "step 918 loss: 3.7437210083007812\n",
      "step 919 loss: 3.6656668186187744\n",
      "step 920 loss: 3.8249151706695557\n",
      "step 921 loss: 3.759944200515747\n",
      "step 922 loss: 3.7926018238067627\n",
      "step 923 loss: 3.864443778991699\n",
      "step 924 loss: 3.8805627822875977\n",
      "step 925 loss: 3.801201105117798\n",
      "step 926 loss: 3.84193754196167\n",
      "step 927 loss: 3.8085856437683105\n",
      "step 928 loss: 3.7221577167510986\n",
      "step 929 loss: 3.775733470916748\n",
      "step 930 loss: 3.829355478286743\n",
      "step 931 loss: 3.6943883895874023\n",
      "step 932 loss: 3.843280076980591\n",
      "step 933 loss: 3.8346707820892334\n",
      "step 934 loss: 3.71374773979187\n",
      "step 935 loss: 3.7734007835388184\n",
      "step 936 loss: 3.7802255153656006\n",
      "step 937 loss: 3.692798137664795\n",
      "step 938 loss: 3.717385768890381\n",
      "step 939 loss: 3.8322548866271973\n",
      "step 940 loss: 3.733841896057129\n",
      "step 941 loss: 3.7778618335723877\n",
      "step 942 loss: 3.7434279918670654\n",
      "step 943 loss: 3.787424087524414\n",
      "step 944 loss: 3.822571277618408\n",
      "step 945 loss: 3.7631609439849854\n",
      "step 946 loss: 3.723034143447876\n",
      "step 947 loss: 3.7819623947143555\n",
      "step 948 loss: 3.8402867317199707\n",
      "step 949 loss: 3.76932692527771\n",
      "step 950 loss: 3.6820056438446045\n",
      "step 951 loss: 3.710742473602295\n",
      "step 952 loss: 3.828204393386841\n",
      "step 953 loss: 3.6605026721954346\n",
      "step 954 loss: 3.721555709838867\n",
      "step 955 loss: 3.8667991161346436\n",
      "step 956 loss: 3.797116756439209\n",
      "step 957 loss: 3.742626667022705\n",
      "step 958 loss: 3.7811102867126465\n",
      "step 959 loss: 3.7572481632232666\n",
      "step 960 loss: 3.7966699600219727\n",
      "step 961 loss: 3.673724412918091\n",
      "step 962 loss: 3.799081802368164\n",
      "step 963 loss: 3.7688403129577637\n",
      "step 964 loss: 3.7928287982940674\n",
      "step 965 loss: 3.602792739868164\n",
      "step 966 loss: 3.771890640258789\n",
      "step 967 loss: 3.7849206924438477\n",
      "step 968 loss: 3.857020378112793\n",
      "step 969 loss: 3.7483012676239014\n",
      "step 970 loss: 3.765547037124634\n",
      "step 971 loss: 3.7111082077026367\n",
      "step 972 loss: 3.7425317764282227\n",
      "step 973 loss: 3.7984230518341064\n",
      "step 974 loss: 3.68129825592041\n",
      "step 975 loss: 3.8259241580963135\n",
      "step 976 loss: 3.735400438308716\n",
      "step 977 loss: 3.769521474838257\n",
      "step 978 loss: 3.7350871562957764\n",
      "step 979 loss: 3.7153098583221436\n",
      "step 980 loss: 3.76216197013855\n",
      "step 981 loss: 3.7517895698547363\n",
      "step 982 loss: 3.7342140674591064\n",
      "step 983 loss: 3.762563467025757\n",
      "step 984 loss: 3.8221662044525146\n",
      "step 985 loss: 3.7267885208129883\n",
      "step 986 loss: 3.7548272609710693\n",
      "step 987 loss: 3.7328603267669678\n",
      "step 988 loss: 3.7540950775146484\n",
      "step 989 loss: 3.749436855316162\n",
      "step 990 loss: 3.7779340744018555\n",
      "step 991 loss: 3.7048087120056152\n",
      "step 992 loss: 3.7296507358551025\n",
      "step 993 loss: 3.838878870010376\n",
      "step 994 loss: 3.7662394046783447\n",
      "step 995 loss: 3.7627222537994385\n",
      "step 996 loss: 3.7991316318511963\n",
      "step 997 loss: 3.7017226219177246\n",
      "step 998 loss: 3.7484006881713867\n",
      "step 999 loss: 3.7218432426452637\n",
      "step 1000 loss: 3.7031264305114746\n",
      "step 1001 loss: 3.782907247543335\n",
      "step 1002 loss: 3.7152466773986816\n",
      "step 1003 loss: 3.720212459564209\n",
      "step 1004 loss: 3.800234317779541\n",
      "step 1005 loss: 3.757471799850464\n",
      "step 1006 loss: 3.6364376544952393\n",
      "step 1007 loss: 3.760831117630005\n",
      "step 1008 loss: 3.7188496589660645\n",
      "step 1009 loss: 3.630361318588257\n",
      "step 1010 loss: 3.6405413150787354\n",
      "step 1011 loss: 3.7757205963134766\n",
      "step 1012 loss: 3.6973118782043457\n",
      "step 1013 loss: 3.7231245040893555\n",
      "step 1014 loss: 3.5803158283233643\n",
      "step 1015 loss: 3.726654529571533\n",
      "step 1016 loss: 3.6934609413146973\n",
      "step 1017 loss: 3.743215560913086\n",
      "step 1018 loss: 3.6173276901245117\n",
      "step 1019 loss: 3.8004379272460938\n",
      "step 1020 loss: 3.7124485969543457\n",
      "step 1021 loss: 3.800271987915039\n",
      "step 1022 loss: 3.716634750366211\n",
      "step 1023 loss: 3.709751605987549\n",
      "step 1024 loss: 3.7220816612243652\n",
      "step 1025 loss: 3.8044400215148926\n",
      "step 1026 loss: 3.73506236076355\n",
      "step 1027 loss: 3.7992122173309326\n",
      "step 1028 loss: 3.698068857192993\n",
      "step 1029 loss: 3.7698898315429688\n",
      "step 1030 loss: 3.603816270828247\n",
      "step 1031 loss: 3.7575056552886963\n",
      "step 1032 loss: 3.70637845993042\n",
      "step 1033 loss: 3.823737382888794\n",
      "step 1034 loss: 3.6753180027008057\n",
      "step 1035 loss: 3.727473258972168\n",
      "step 1036 loss: 3.7531323432922363\n",
      "step 1037 loss: 3.64057993888855\n",
      "step 1038 loss: 3.6905226707458496\n",
      "step 1039 loss: 3.6622612476348877\n",
      "step 1040 loss: 3.780179500579834\n",
      "step 1041 loss: 3.654752731323242\n",
      "step 1042 loss: 3.6927671432495117\n",
      "step 1043 loss: 3.6358463764190674\n",
      "step 1044 loss: 3.6413521766662598\n",
      "step 1045 loss: 3.681450605392456\n",
      "step 1046 loss: 3.6929705142974854\n",
      "step 1047 loss: 3.74322247505188\n",
      "step 1048 loss: 3.6376168727874756\n",
      "step 1049 loss: 3.6832947731018066\n",
      "step 1050 loss: 3.6374661922454834\n",
      "step 1051 loss: 3.72464919090271\n",
      "step 1052 loss: 3.680182933807373\n",
      "step 1053 loss: 3.695101261138916\n",
      "step 1054 loss: 3.7034623622894287\n",
      "step 1055 loss: 3.7018721103668213\n",
      "step 1056 loss: 3.693906784057617\n",
      "step 1057 loss: 3.687499523162842\n",
      "step 1058 loss: 3.6736395359039307\n",
      "step 1059 loss: 3.646308422088623\n",
      "step 1060 loss: 3.7427380084991455\n",
      "step 1061 loss: 3.7400929927825928\n",
      "step 1062 loss: 3.745924949645996\n",
      "step 1063 loss: 3.718719959259033\n",
      "step 1064 loss: 3.696930408477783\n",
      "step 1065 loss: 3.6293485164642334\n",
      "step 1066 loss: 3.7781944274902344\n",
      "step 1067 loss: 3.6728994846343994\n",
      "step 1068 loss: 3.664900541305542\n",
      "step 1069 loss: 3.7060306072235107\n",
      "step 1070 loss: 3.6606552600860596\n",
      "step 1071 loss: 3.5958712100982666\n",
      "step 1072 loss: 3.706502676010132\n",
      "step 1073 loss: 3.7158734798431396\n",
      "step 1074 loss: 3.683964252471924\n",
      "step 1075 loss: 3.6962385177612305\n",
      "step 1076 loss: 3.722975730895996\n",
      "step 1077 loss: 3.55263090133667\n",
      "step 1078 loss: 3.6872897148132324\n",
      "step 1079 loss: 3.6123909950256348\n",
      "step 1080 loss: 3.6745052337646484\n",
      "step 1081 loss: 3.6995091438293457\n",
      "step 1082 loss: 3.7010087966918945\n",
      "step 1083 loss: 3.5903289318084717\n",
      "step 1084 loss: 3.65871000289917\n",
      "step 1085 loss: 3.7404916286468506\n",
      "step 1086 loss: 3.725982904434204\n",
      "step 1087 loss: 3.730699062347412\n",
      "step 1088 loss: 3.658860445022583\n",
      "step 1089 loss: 3.6120684146881104\n",
      "step 1090 loss: 3.570401668548584\n",
      "step 1091 loss: 3.557579755783081\n",
      "step 1092 loss: 3.7160396575927734\n",
      "step 1093 loss: 3.6953184604644775\n",
      "step 1094 loss: 3.6379318237304688\n",
      "step 1095 loss: 3.623058557510376\n",
      "step 1096 loss: 3.6786084175109863\n",
      "step 1097 loss: 3.6158382892608643\n",
      "step 1098 loss: 3.705258369445801\n",
      "step 1099 loss: 3.6380467414855957\n",
      "step 1100 loss: 3.7115283012390137\n",
      "step 1101 loss: 3.718010425567627\n",
      "step 1102 loss: 3.7401063442230225\n",
      "step 1103 loss: 3.6600446701049805\n",
      "step 1104 loss: 3.6981968879699707\n",
      "step 1105 loss: 3.5930497646331787\n",
      "step 1106 loss: 3.6128361225128174\n",
      "step 1107 loss: 3.6008894443511963\n",
      "step 1108 loss: 3.6592724323272705\n",
      "step 1109 loss: 3.662022352218628\n",
      "step 1110 loss: 3.6483001708984375\n",
      "step 1111 loss: 3.7066268920898438\n",
      "step 1112 loss: 3.6229634284973145\n",
      "step 1113 loss: 3.5688893795013428\n",
      "step 1114 loss: 3.6427972316741943\n",
      "step 1115 loss: 3.654107093811035\n",
      "step 1116 loss: 3.634112596511841\n",
      "step 1117 loss: 3.662871837615967\n",
      "step 1118 loss: 3.6894266605377197\n",
      "step 1119 loss: 3.5960049629211426\n",
      "step 1120 loss: 3.576648473739624\n",
      "step 1121 loss: 3.7086565494537354\n",
      "step 1122 loss: 3.6895813941955566\n",
      "step 1123 loss: 3.6555051803588867\n",
      "step 1124 loss: 3.653339385986328\n",
      "step 1125 loss: 3.692491054534912\n",
      "step 1126 loss: 3.5773496627807617\n",
      "step 1127 loss: 3.6625685691833496\n",
      "step 1128 loss: 3.7072315216064453\n",
      "step 1129 loss: 3.610126256942749\n",
      "step 1130 loss: 3.579188346862793\n",
      "step 1131 loss: 3.6182146072387695\n",
      "step 1132 loss: 3.666369676589966\n",
      "step 1133 loss: 3.526841402053833\n",
      "step 1134 loss: 3.6040637493133545\n",
      "step 1135 loss: 3.594186305999756\n",
      "step 1136 loss: 3.522920846939087\n",
      "step 1137 loss: 3.6248080730438232\n",
      "step 1138 loss: 3.518343925476074\n",
      "step 1139 loss: 3.6332600116729736\n",
      "step 1140 loss: 3.6495521068573\n",
      "step 1141 loss: 3.531346082687378\n",
      "step 1142 loss: 3.6126112937927246\n",
      "step 1143 loss: 3.637186050415039\n",
      "step 1144 loss: 3.552629232406616\n",
      "step 1145 loss: 3.5870673656463623\n",
      "step 1146 loss: 3.6869986057281494\n",
      "step 1147 loss: 3.58492374420166\n",
      "step 1148 loss: 3.764836311340332\n",
      "step 1149 loss: 3.583444595336914\n",
      "step 1150 loss: 3.5866546630859375\n",
      "step 1151 loss: 3.58217191696167\n",
      "step 1152 loss: 3.6251399517059326\n",
      "step 1153 loss: 3.5823049545288086\n",
      "step 1154 loss: 3.6287307739257812\n",
      "step 1155 loss: 3.6182472705841064\n",
      "step 1156 loss: 3.6195218563079834\n",
      "step 1157 loss: 3.606658935546875\n",
      "step 1158 loss: 3.5944278240203857\n",
      "step 1159 loss: 3.5752921104431152\n",
      "step 1160 loss: 3.6108086109161377\n",
      "step 1161 loss: 3.514596939086914\n",
      "step 1162 loss: 3.5790786743164062\n",
      "step 1163 loss: 3.6739308834075928\n",
      "step 1164 loss: 3.632387638092041\n",
      "step 1165 loss: 3.589428186416626\n",
      "step 1166 loss: 3.5713486671447754\n",
      "step 1167 loss: 3.62399959564209\n",
      "step 1168 loss: 3.7934370040893555\n",
      "step 1169 loss: 3.586613416671753\n",
      "step 1170 loss: 3.7056400775909424\n",
      "step 1171 loss: 3.6715376377105713\n",
      "step 1172 loss: 3.6712124347686768\n",
      "step 1173 loss: 3.6337971687316895\n",
      "step 1174 loss: 3.515463352203369\n",
      "step 1175 loss: 3.530839443206787\n",
      "step 1176 loss: 3.602444887161255\n",
      "step 1177 loss: 3.4399514198303223\n",
      "step 1178 loss: 3.56752610206604\n",
      "step 1179 loss: 3.6563379764556885\n",
      "step 1180 loss: 3.6837165355682373\n",
      "step 1181 loss: 3.511150598526001\n",
      "step 1182 loss: 3.566258668899536\n",
      "step 1183 loss: 3.5967977046966553\n",
      "step 1184 loss: 3.5810275077819824\n",
      "step 1185 loss: 3.5475101470947266\n",
      "step 1186 loss: 3.6161155700683594\n",
      "step 1187 loss: 3.5445377826690674\n",
      "step 1188 loss: 3.6425201892852783\n",
      "step 1189 loss: 3.6139540672302246\n",
      "step 1190 loss: 3.571099281311035\n",
      "step 1191 loss: 3.498077869415283\n",
      "step 1192 loss: 3.573420524597168\n",
      "step 1193 loss: 3.472658157348633\n",
      "step 1194 loss: 3.565005302429199\n",
      "step 1195 loss: 3.5460596084594727\n",
      "step 1196 loss: 3.6399219036102295\n",
      "step 1197 loss: 3.623283863067627\n",
      "step 1198 loss: 3.480358600616455\n",
      "step 1199 loss: 3.5240726470947266\n",
      "step 1200 loss: 3.6330997943878174\n",
      "step 1201 loss: 3.579160451889038\n",
      "step 1202 loss: 3.587693929672241\n",
      "step 1203 loss: 3.6202731132507324\n",
      "step 1204 loss: 3.620326519012451\n",
      "step 1205 loss: 3.5656485557556152\n",
      "step 1206 loss: 3.5237483978271484\n",
      "step 1207 loss: 3.5738391876220703\n",
      "step 1208 loss: 3.5552873611450195\n",
      "step 1209 loss: 3.5596463680267334\n",
      "step 1210 loss: 3.4950919151306152\n",
      "step 1211 loss: 3.6482410430908203\n",
      "step 1212 loss: 3.5968363285064697\n",
      "step 1213 loss: 3.493680715560913\n",
      "step 1214 loss: 3.4964077472686768\n",
      "step 1215 loss: 3.6641929149627686\n",
      "step 1216 loss: 3.602942705154419\n",
      "step 1217 loss: 3.624434232711792\n",
      "step 1218 loss: 3.585644245147705\n",
      "step 1219 loss: 3.5007381439208984\n",
      "step 1220 loss: 3.539766550064087\n",
      "step 1221 loss: 3.5492172241210938\n",
      "step 1222 loss: 3.490917682647705\n",
      "step 1223 loss: 3.51904559135437\n",
      "step 1224 loss: 3.4721736907958984\n",
      "step 1225 loss: 3.4773976802825928\n",
      "step 1226 loss: 3.509939670562744\n",
      "step 1227 loss: 3.564687967300415\n",
      "step 1228 loss: 3.552997589111328\n",
      "step 1229 loss: 3.5423994064331055\n",
      "step 1230 loss: 3.5623960494995117\n",
      "step 1231 loss: 3.538153886795044\n",
      "step 1232 loss: 3.536280632019043\n",
      "step 1233 loss: 3.531245231628418\n",
      "step 1234 loss: 3.454775333404541\n",
      "step 1235 loss: 3.54789400100708\n",
      "step 1236 loss: 3.5984983444213867\n",
      "step 1237 loss: 3.4080920219421387\n",
      "step 1238 loss: 3.5121850967407227\n",
      "step 1239 loss: 3.580444812774658\n",
      "step 1240 loss: 3.5864603519439697\n",
      "step 1241 loss: 3.557406187057495\n",
      "step 1242 loss: 3.545795202255249\n",
      "step 1243 loss: 3.4888439178466797\n",
      "step 1244 loss: 3.5668466091156006\n",
      "step 1245 loss: 3.6221368312835693\n",
      "step 1246 loss: 3.4446849822998047\n",
      "step 1247 loss: 3.484032392501831\n",
      "step 1248 loss: 3.559168815612793\n",
      "step 1249 loss: 3.5010995864868164\n",
      "step 1250 loss: 3.4938368797302246\n",
      "step 1251 loss: 3.519526243209839\n",
      "step 1252 loss: 3.4392752647399902\n",
      "step 1253 loss: 3.610471725463867\n",
      "step 1254 loss: 3.521533250808716\n",
      "step 1255 loss: 3.4928481578826904\n",
      "step 1256 loss: 3.518273115158081\n",
      "step 1257 loss: 3.5123376846313477\n",
      "step 1258 loss: 3.465505838394165\n",
      "step 1259 loss: 3.496652841567993\n",
      "step 1260 loss: 3.5522241592407227\n",
      "step 1261 loss: 3.5230894088745117\n",
      "step 1262 loss: 3.6029927730560303\n",
      "step 1263 loss: 3.437776803970337\n",
      "step 1264 loss: 3.3874423503875732\n",
      "step 1265 loss: 3.577007293701172\n",
      "step 1266 loss: 3.607374906539917\n",
      "step 1267 loss: 3.5238289833068848\n",
      "step 1268 loss: 3.57861590385437\n",
      "step 1269 loss: 3.573648452758789\n",
      "step 1270 loss: 3.50441312789917\n",
      "step 1271 loss: 3.5406150817871094\n",
      "step 1272 loss: 3.4369168281555176\n",
      "step 1273 loss: 3.5720527172088623\n",
      "step 1274 loss: 3.526559591293335\n",
      "step 1275 loss: 3.5209712982177734\n",
      "step 1276 loss: 3.483614683151245\n",
      "step 1277 loss: 3.5633254051208496\n",
      "step 1278 loss: 3.549154281616211\n",
      "step 1279 loss: 3.4844651222229004\n",
      "step 1280 loss: 3.573679208755493\n",
      "step 1281 loss: 3.4974164962768555\n",
      "step 1282 loss: 3.5076534748077393\n",
      "step 1283 loss: 3.5806362628936768\n",
      "step 1284 loss: 3.586130142211914\n",
      "step 1285 loss: 3.450251579284668\n",
      "step 1286 loss: 3.4563374519348145\n",
      "step 1287 loss: 3.511596441268921\n",
      "step 1288 loss: 3.437350034713745\n",
      "step 1289 loss: 3.486992597579956\n",
      "step 1290 loss: 3.4683761596679688\n",
      "step 1291 loss: 3.653737783432007\n",
      "step 1292 loss: 3.4448578357696533\n",
      "step 1293 loss: 3.565568208694458\n",
      "step 1294 loss: 3.5819926261901855\n",
      "step 1295 loss: 3.446925640106201\n",
      "step 1296 loss: 3.5254018306732178\n",
      "step 1297 loss: 3.48252010345459\n",
      "step 1298 loss: 3.4582700729370117\n",
      "step 1299 loss: 3.532663345336914\n",
      "step 1300 loss: 3.422212600708008\n",
      "step 1301 loss: 3.442888021469116\n",
      "step 1302 loss: 3.554105758666992\n",
      "step 1303 loss: 3.5556223392486572\n",
      "step 1304 loss: 3.4727706909179688\n",
      "step 1305 loss: 3.488381862640381\n",
      "step 1306 loss: 3.52911114692688\n",
      "step 1307 loss: 3.714259147644043\n",
      "step 1308 loss: 3.452073097229004\n",
      "step 1309 loss: 3.4620003700256348\n",
      "step 1310 loss: 3.476330041885376\n",
      "step 1311 loss: 3.504683494567871\n",
      "step 1312 loss: 3.535121202468872\n",
      "step 1313 loss: 3.4344406127929688\n",
      "step 1314 loss: 3.5103952884674072\n",
      "step 1315 loss: 3.4605696201324463\n",
      "step 1316 loss: 3.4724884033203125\n",
      "step 1317 loss: 3.527571201324463\n",
      "step 1318 loss: 3.5068721771240234\n",
      "step 1319 loss: 3.5734705924987793\n",
      "step 1320 loss: 3.399096727371216\n",
      "step 1321 loss: 3.4857561588287354\n",
      "step 1322 loss: 3.5296640396118164\n",
      "step 1323 loss: 3.4879724979400635\n",
      "step 1324 loss: 3.5265307426452637\n",
      "step 1325 loss: 3.4688165187835693\n",
      "step 1326 loss: 3.5290603637695312\n",
      "step 1327 loss: 3.443890333175659\n",
      "step 1328 loss: 3.449613094329834\n",
      "step 1329 loss: 3.509552240371704\n",
      "step 1330 loss: 3.4678916931152344\n",
      "step 1331 loss: 3.4968485832214355\n",
      "step 1332 loss: 3.5202267169952393\n",
      "step 1333 loss: 3.4558186531066895\n",
      "step 1334 loss: 3.5068001747131348\n",
      "step 1335 loss: 3.5072696208953857\n",
      "step 1336 loss: 3.5525014400482178\n",
      "step 1337 loss: 3.4434726238250732\n",
      "step 1338 loss: 3.571101188659668\n",
      "step 1339 loss: 3.4548561573028564\n",
      "step 1340 loss: 3.4632833003997803\n",
      "step 1341 loss: 3.4369349479675293\n",
      "step 1342 loss: 3.3822615146636963\n",
      "step 1343 loss: 3.535661458969116\n",
      "step 1344 loss: 3.4337522983551025\n",
      "step 1345 loss: 3.4775731563568115\n",
      "step 1346 loss: 3.510284662246704\n",
      "step 1347 loss: 3.417717695236206\n",
      "step 1348 loss: 3.445279359817505\n",
      "step 1349 loss: 3.4823076725006104\n",
      "step 1350 loss: 3.370107650756836\n",
      "step 1351 loss: 3.471374034881592\n",
      "step 1352 loss: 3.5192925930023193\n",
      "step 1353 loss: 3.6559464931488037\n",
      "step 1354 loss: 3.503600597381592\n",
      "step 1355 loss: 3.4542174339294434\n",
      "step 1356 loss: 3.395535469055176\n",
      "step 1357 loss: 3.542638063430786\n",
      "step 1358 loss: 3.5179061889648438\n",
      "step 1359 loss: 3.4697811603546143\n",
      "step 1360 loss: 3.5883569717407227\n",
      "step 1361 loss: 3.485703468322754\n",
      "step 1362 loss: 3.508880138397217\n",
      "step 1363 loss: 3.41355299949646\n",
      "step 1364 loss: 3.5005950927734375\n",
      "step 1365 loss: 3.420196056365967\n",
      "step 1366 loss: 3.5008041858673096\n",
      "step 1367 loss: 3.4581148624420166\n",
      "step 1368 loss: 3.475146770477295\n",
      "step 1369 loss: 3.503950595855713\n",
      "step 1370 loss: 3.5230231285095215\n",
      "step 1371 loss: 3.4751977920532227\n",
      "step 1372 loss: 3.5159595012664795\n",
      "step 1373 loss: 3.449803590774536\n",
      "step 1374 loss: 3.432283639907837\n",
      "step 1375 loss: 3.4439685344696045\n",
      "step 1376 loss: 3.524268865585327\n",
      "step 1377 loss: 3.4145631790161133\n",
      "step 1378 loss: 3.381145477294922\n",
      "step 1379 loss: 3.417653799057007\n",
      "step 1380 loss: 3.4401133060455322\n",
      "step 1381 loss: 3.50807785987854\n",
      "step 1382 loss: 3.466536283493042\n",
      "step 1383 loss: 3.423640251159668\n",
      "step 1384 loss: 3.4272751808166504\n",
      "step 1385 loss: 3.4853501319885254\n",
      "step 1386 loss: 3.4735257625579834\n",
      "step 1387 loss: 3.401279926300049\n",
      "step 1388 loss: 3.3808538913726807\n",
      "step 1389 loss: 3.4114718437194824\n",
      "step 1390 loss: 3.357574224472046\n",
      "step 1391 loss: 3.4306716918945312\n",
      "step 1392 loss: 3.372471809387207\n",
      "step 1393 loss: 3.4324209690093994\n",
      "step 1394 loss: 3.5195586681365967\n",
      "step 1395 loss: 3.416290521621704\n",
      "step 1396 loss: 3.4945931434631348\n",
      "step 1397 loss: 3.3994767665863037\n",
      "step 1398 loss: 3.418031930923462\n",
      "step 1399 loss: 3.3873579502105713\n",
      "step 1400 loss: 3.4295449256896973\n",
      "step 1401 loss: 3.4826149940490723\n",
      "step 1402 loss: 3.439229726791382\n",
      "step 1403 loss: 3.4476943016052246\n",
      "step 1404 loss: 3.483285427093506\n",
      "step 1405 loss: 3.455709218978882\n",
      "step 1406 loss: 3.5017635822296143\n",
      "step 1407 loss: 3.342205762863159\n",
      "step 1408 loss: 3.442023992538452\n",
      "step 1409 loss: 3.3184354305267334\n",
      "step 1410 loss: 3.549442768096924\n",
      "step 1411 loss: 3.419128894805908\n",
      "step 1412 loss: 3.413914680480957\n",
      "step 1413 loss: 3.5213611125946045\n",
      "step 1414 loss: 3.3182179927825928\n",
      "step 1415 loss: 3.4288437366485596\n",
      "step 1416 loss: 3.5599522590637207\n",
      "step 1417 loss: 3.4645423889160156\n",
      "step 1418 loss: 3.4280426502227783\n",
      "step 1419 loss: 3.3368000984191895\n",
      "step 1420 loss: 3.4474635124206543\n",
      "step 1421 loss: 3.42976450920105\n",
      "step 1422 loss: 3.418321132659912\n",
      "step 1423 loss: 3.3696582317352295\n",
      "step 1424 loss: 3.37011456489563\n",
      "step 1425 loss: 3.3390207290649414\n",
      "step 1426 loss: 3.389244318008423\n",
      "step 1427 loss: 3.5148837566375732\n",
      "step 1428 loss: 3.5190179347991943\n",
      "step 1429 loss: 3.494694709777832\n",
      "step 1430 loss: 3.5232272148132324\n",
      "step 1431 loss: 3.447885751724243\n",
      "step 1432 loss: 3.4027743339538574\n",
      "step 1433 loss: 3.5169243812561035\n",
      "step 1434 loss: 3.347252607345581\n",
      "step 1435 loss: 3.487395763397217\n",
      "step 1436 loss: 3.5083625316619873\n",
      "step 1437 loss: 3.4344711303710938\n",
      "step 1438 loss: 3.3771183490753174\n",
      "step 1439 loss: 3.400205373764038\n",
      "step 1440 loss: 3.408931016921997\n",
      "step 1441 loss: 3.4084386825561523\n",
      "step 1442 loss: 3.4282820224761963\n",
      "step 1443 loss: 3.4899380207061768\n",
      "step 1444 loss: 3.4622035026550293\n",
      "step 1445 loss: 3.454435110092163\n",
      "step 1446 loss: 3.392287492752075\n",
      "step 1447 loss: 3.4733781814575195\n",
      "step 1448 loss: 3.4101672172546387\n",
      "step 1449 loss: 3.397820472717285\n",
      "step 1450 loss: 3.5309958457946777\n",
      "step 1451 loss: 3.471391201019287\n",
      "step 1452 loss: 3.3510324954986572\n",
      "step 1453 loss: 3.397899866104126\n",
      "step 1454 loss: 3.5351922512054443\n",
      "step 1455 loss: 3.3348703384399414\n",
      "step 1456 loss: 3.4817612171173096\n",
      "step 1457 loss: 3.4312028884887695\n",
      "step 1458 loss: 3.333197593688965\n",
      "step 1459 loss: 3.4200010299682617\n",
      "step 1460 loss: 3.4948270320892334\n",
      "step 1461 loss: 3.2539255619049072\n",
      "step 1462 loss: 3.362170457839966\n",
      "step 1463 loss: 3.3512048721313477\n",
      "step 1464 loss: 3.4175474643707275\n",
      "step 1465 loss: 3.339702844619751\n",
      "step 1466 loss: 3.409679651260376\n",
      "step 1467 loss: 3.3424134254455566\n",
      "step 1468 loss: 3.2950632572174072\n",
      "step 1469 loss: 3.4809083938598633\n",
      "step 1470 loss: 3.4115967750549316\n",
      "step 1471 loss: 3.357881784439087\n",
      "step 1472 loss: 3.3896117210388184\n",
      "step 1473 loss: 3.4157774448394775\n",
      "step 1474 loss: 3.4083497524261475\n",
      "step 1475 loss: 3.431300401687622\n",
      "step 1476 loss: 3.459585428237915\n",
      "step 1477 loss: 3.4450130462646484\n",
      "step 1478 loss: 3.3367908000946045\n",
      "step 1479 loss: 3.4236724376678467\n",
      "step 1480 loss: 3.4137020111083984\n",
      "step 1481 loss: 3.3728723526000977\n",
      "step 1482 loss: 3.449756383895874\n",
      "step 1483 loss: 3.389394998550415\n",
      "step 1484 loss: 3.521151542663574\n",
      "step 1485 loss: 3.312084197998047\n",
      "step 1486 loss: 3.3616294860839844\n",
      "step 1487 loss: 3.3605430126190186\n",
      "step 1488 loss: 3.4269793033599854\n",
      "step 1489 loss: 3.3952903747558594\n",
      "step 1490 loss: 3.5020298957824707\n",
      "step 1491 loss: 3.3805131912231445\n",
      "step 1492 loss: 3.386701822280884\n",
      "step 1493 loss: 3.3875632286071777\n",
      "step 1494 loss: 3.3284895420074463\n",
      "step 1495 loss: 3.3923118114471436\n",
      "step 1496 loss: 3.338139772415161\n",
      "step 1497 loss: 3.4128265380859375\n",
      "step 1498 loss: 3.390777826309204\n",
      "step 1499 loss: 3.348367214202881\n",
      "step 1500 loss: 3.4233598709106445\n",
      "step 1501 loss: 3.39288330078125\n",
      "step 1502 loss: 3.388066053390503\n",
      "step 1503 loss: 3.444711923599243\n",
      "step 1504 loss: 3.3806824684143066\n",
      "step 1505 loss: 3.501386880874634\n",
      "step 1506 loss: 3.309553384780884\n",
      "step 1507 loss: 3.3463478088378906\n",
      "step 1508 loss: 3.351846694946289\n",
      "step 1509 loss: 3.378767490386963\n",
      "step 1510 loss: 3.503511428833008\n",
      "step 1511 loss: 3.3934192657470703\n",
      "step 1512 loss: 3.378876209259033\n",
      "step 1513 loss: 3.3908567428588867\n",
      "step 1514 loss: 3.496499538421631\n",
      "step 1515 loss: 3.3138442039489746\n",
      "step 1516 loss: 3.449836254119873\n",
      "step 1517 loss: 3.4560325145721436\n",
      "step 1518 loss: 3.3906192779541016\n",
      "step 1519 loss: 3.2524003982543945\n",
      "step 1520 loss: 3.3751933574676514\n",
      "step 1521 loss: 3.3001160621643066\n",
      "step 1522 loss: 3.34665584564209\n",
      "step 1523 loss: 3.2594432830810547\n",
      "step 1524 loss: 3.2615320682525635\n",
      "step 1525 loss: 3.3249776363372803\n",
      "step 1526 loss: 3.4435842037200928\n",
      "step 1527 loss: 3.435304641723633\n",
      "step 1528 loss: 3.344970703125\n",
      "step 1529 loss: 3.3476548194885254\n",
      "step 1530 loss: 3.354159116744995\n",
      "step 1531 loss: 3.306086540222168\n",
      "step 1532 loss: 3.340585947036743\n",
      "step 1533 loss: 3.3046224117279053\n",
      "step 1534 loss: 3.4069578647613525\n",
      "step 1535 loss: 3.415764093399048\n",
      "step 1536 loss: 3.3137001991271973\n",
      "step 1537 loss: 3.4075300693511963\n",
      "step 1538 loss: 3.290207862854004\n",
      "step 1539 loss: 3.3751015663146973\n",
      "step 1540 loss: 3.309030055999756\n",
      "step 1541 loss: 3.339833974838257\n",
      "step 1542 loss: 3.3827764987945557\n",
      "step 1543 loss: 3.3936707973480225\n",
      "step 1544 loss: 3.362508535385132\n",
      "step 1545 loss: 3.3824479579925537\n",
      "step 1546 loss: 3.312434196472168\n",
      "step 1547 loss: 3.454007148742676\n",
      "step 1548 loss: 3.3055601119995117\n",
      "step 1549 loss: 3.3195154666900635\n",
      "step 1550 loss: 3.3353159427642822\n",
      "step 1551 loss: 3.417297124862671\n",
      "step 1552 loss: 3.293789863586426\n",
      "step 1553 loss: 3.3306312561035156\n",
      "step 1554 loss: 3.416907548904419\n",
      "step 1555 loss: 3.301453113555908\n",
      "step 1556 loss: 3.436062812805176\n",
      "step 1557 loss: 3.469021797180176\n",
      "step 1558 loss: 3.435584306716919\n",
      "step 1559 loss: 3.349391222000122\n",
      "step 1560 loss: 3.3126606941223145\n",
      "step 1561 loss: 3.3294200897216797\n",
      "step 1562 loss: 3.3651602268218994\n",
      "step 1563 loss: 3.3191781044006348\n",
      "step 1564 loss: 3.380181074142456\n",
      "step 1565 loss: 3.391569137573242\n",
      "step 1566 loss: 3.2952091693878174\n",
      "step 1567 loss: 3.3256611824035645\n",
      "step 1568 loss: 3.359511137008667\n",
      "step 1569 loss: 3.4075400829315186\n",
      "step 1570 loss: 3.430022716522217\n",
      "step 1571 loss: 3.323258638381958\n",
      "step 1572 loss: 3.3653500080108643\n",
      "step 1573 loss: 3.4653682708740234\n",
      "step 1574 loss: 3.2683157920837402\n",
      "step 1575 loss: 3.3194258213043213\n",
      "step 1576 loss: 3.3484725952148438\n",
      "step 1577 loss: 3.2155814170837402\n",
      "step 1578 loss: 3.3028101921081543\n",
      "step 1579 loss: 3.3451051712036133\n",
      "step 1580 loss: 3.3734982013702393\n",
      "step 1581 loss: 3.3062026500701904\n",
      "step 1582 loss: 3.3234729766845703\n",
      "step 1583 loss: 3.2721283435821533\n",
      "step 1584 loss: 3.4016647338867188\n",
      "step 1585 loss: 3.30568528175354\n",
      "step 1586 loss: 3.3113625049591064\n",
      "step 1587 loss: 3.3460938930511475\n",
      "step 1588 loss: 3.3312788009643555\n",
      "step 1589 loss: 3.2729952335357666\n",
      "step 1590 loss: 3.2950944900512695\n",
      "step 1591 loss: 3.352592706680298\n",
      "step 1592 loss: 3.2474911212921143\n",
      "step 1593 loss: 3.372765064239502\n",
      "step 1594 loss: 3.3818771839141846\n",
      "step 1595 loss: 3.263745069503784\n",
      "step 1596 loss: 3.3139703273773193\n",
      "step 1597 loss: 3.42132830619812\n",
      "step 1598 loss: 3.3624048233032227\n",
      "step 1599 loss: 3.2456133365631104\n",
      "step 1600 loss: 3.3018524646759033\n",
      "step 1601 loss: 3.253999710083008\n",
      "step 1602 loss: 3.296858072280884\n",
      "step 1603 loss: 3.295907974243164\n",
      "step 1604 loss: 3.3843302726745605\n",
      "step 1605 loss: 3.3571407794952393\n",
      "step 1606 loss: 3.302103042602539\n",
      "step 1607 loss: 3.359175205230713\n",
      "step 1608 loss: 3.3775041103363037\n",
      "step 1609 loss: 3.2703142166137695\n",
      "step 1610 loss: 3.3147075176239014\n",
      "step 1611 loss: 3.256938934326172\n",
      "step 1612 loss: 3.3060550689697266\n",
      "step 1613 loss: 3.2066619396209717\n",
      "step 1614 loss: 3.3618505001068115\n",
      "step 1615 loss: 3.2938528060913086\n",
      "step 1616 loss: 3.3000857830047607\n",
      "step 1617 loss: 3.356832504272461\n",
      "step 1618 loss: 3.3709065914154053\n",
      "step 1619 loss: 3.371234178543091\n",
      "step 1620 loss: 3.3865888118743896\n",
      "step 1621 loss: 3.310899019241333\n",
      "step 1622 loss: 3.340294122695923\n",
      "step 1623 loss: 3.3125557899475098\n",
      "step 1624 loss: 3.294494390487671\n",
      "step 1625 loss: 3.505683422088623\n",
      "step 1626 loss: 3.305337905883789\n",
      "step 1627 loss: 3.324841022491455\n",
      "step 1628 loss: 3.2673227787017822\n",
      "step 1629 loss: 3.263503313064575\n",
      "step 1630 loss: 3.304232120513916\n",
      "step 1631 loss: 3.33687686920166\n",
      "step 1632 loss: 3.3077878952026367\n",
      "step 1633 loss: 3.3110947608947754\n",
      "step 1634 loss: 3.3008930683135986\n",
      "step 1635 loss: 3.3087503910064697\n",
      "step 1636 loss: 3.285029411315918\n",
      "step 1637 loss: 3.3599419593811035\n",
      "step 1638 loss: 3.2827444076538086\n",
      "step 1639 loss: 3.3448894023895264\n",
      "step 1640 loss: 3.277167558670044\n",
      "step 1641 loss: 3.294511556625366\n",
      "step 1642 loss: 3.3695616722106934\n",
      "step 1643 loss: 3.422363042831421\n",
      "step 1644 loss: 3.3015012741088867\n",
      "step 1645 loss: 3.255213975906372\n",
      "step 1646 loss: 3.298783540725708\n",
      "step 1647 loss: 3.361366033554077\n",
      "step 1648 loss: 3.265536069869995\n",
      "step 1649 loss: 3.333904981613159\n",
      "step 1650 loss: 3.270113706588745\n",
      "step 1651 loss: 3.2647762298583984\n",
      "step 1652 loss: 3.3139522075653076\n",
      "step 1653 loss: 3.194774866104126\n",
      "step 1654 loss: 3.1747608184814453\n",
      "step 1655 loss: 3.3027358055114746\n",
      "step 1656 loss: 3.264709234237671\n",
      "step 1657 loss: 3.4184699058532715\n",
      "step 1658 loss: 3.3290112018585205\n",
      "step 1659 loss: 3.1541731357574463\n",
      "step 1660 loss: 3.1948845386505127\n",
      "step 1661 loss: 3.1730387210845947\n",
      "step 1662 loss: 3.2617030143737793\n",
      "step 1663 loss: 3.293816566467285\n",
      "step 1664 loss: 3.4305171966552734\n",
      "step 1665 loss: 3.333914041519165\n",
      "step 1666 loss: 3.325103998184204\n",
      "step 1667 loss: 3.304065465927124\n",
      "step 1668 loss: 3.3116402626037598\n",
      "step 1669 loss: 3.361510753631592\n",
      "step 1670 loss: 3.192896604537964\n",
      "step 1671 loss: 3.4265501499176025\n",
      "step 1672 loss: 3.2691969871520996\n",
      "step 1673 loss: 3.2735893726348877\n",
      "step 1674 loss: 3.285615921020508\n",
      "step 1675 loss: 3.2399675846099854\n",
      "step 1676 loss: 3.1580896377563477\n",
      "step 1677 loss: 3.283923625946045\n",
      "step 1678 loss: 3.3128159046173096\n",
      "step 1679 loss: 3.344569683074951\n",
      "step 1680 loss: 3.2132651805877686\n",
      "step 1681 loss: 3.243860960006714\n",
      "step 1682 loss: 3.27717661857605\n",
      "step 1683 loss: 3.2361583709716797\n",
      "step 1684 loss: 3.242154121398926\n",
      "step 1685 loss: 3.2967493534088135\n",
      "step 1686 loss: 3.2605741024017334\n",
      "step 1687 loss: 3.3072168827056885\n",
      "step 1688 loss: 3.269717216491699\n",
      "step 1689 loss: 3.2404513359069824\n",
      "step 1690 loss: 3.298534393310547\n",
      "step 1691 loss: 3.301046371459961\n",
      "step 1692 loss: 3.1865298748016357\n",
      "step 1693 loss: 3.2840192317962646\n",
      "step 1694 loss: 3.2392191886901855\n",
      "step 1695 loss: 3.203296661376953\n",
      "step 1696 loss: 3.3338305950164795\n",
      "step 1697 loss: 3.331444501876831\n",
      "step 1698 loss: 3.25314998626709\n",
      "step 1699 loss: 3.2560391426086426\n",
      "step 1700 loss: 3.283510446548462\n",
      "step 1701 loss: 3.310459852218628\n",
      "step 1702 loss: 3.223996162414551\n",
      "step 1703 loss: 3.245161533355713\n",
      "step 1704 loss: 3.2975637912750244\n",
      "step 1705 loss: 3.318686008453369\n",
      "step 1706 loss: 3.2451181411743164\n",
      "step 1707 loss: 3.293917417526245\n",
      "step 1708 loss: 3.2197742462158203\n",
      "step 1709 loss: 3.3238344192504883\n",
      "step 1710 loss: 3.34828519821167\n",
      "step 1711 loss: 3.355173349380493\n",
      "step 1712 loss: 3.3127379417419434\n",
      "step 1713 loss: 3.229020357131958\n",
      "step 1714 loss: 3.2682902812957764\n",
      "step 1715 loss: 3.2079074382781982\n",
      "step 1716 loss: 3.2293810844421387\n",
      "step 1717 loss: 3.228754758834839\n",
      "step 1718 loss: 3.2824368476867676\n",
      "step 1719 loss: 3.1947968006134033\n",
      "step 1720 loss: 3.1892452239990234\n",
      "step 1721 loss: 3.3158321380615234\n",
      "step 1722 loss: 3.3150107860565186\n",
      "step 1723 loss: 3.2511463165283203\n",
      "step 1724 loss: 3.1898937225341797\n",
      "step 1725 loss: 3.2145228385925293\n",
      "step 1726 loss: 3.2306580543518066\n",
      "step 1727 loss: 3.3287711143493652\n",
      "step 1728 loss: 3.200775623321533\n",
      "step 1729 loss: 3.19474458694458\n",
      "step 1730 loss: 3.2938179969787598\n",
      "step 1731 loss: 3.2441036701202393\n",
      "step 1732 loss: 3.316953420639038\n",
      "step 1733 loss: 3.169060230255127\n",
      "step 1734 loss: 3.2087576389312744\n",
      "step 1735 loss: 3.260514497756958\n",
      "step 1736 loss: 3.2894134521484375\n",
      "step 1737 loss: 3.211441993713379\n",
      "step 1738 loss: 3.1920688152313232\n",
      "step 1739 loss: 3.354128837585449\n",
      "step 1740 loss: 3.290461540222168\n",
      "step 1741 loss: 3.33241868019104\n",
      "step 1742 loss: 3.243396520614624\n",
      "step 1743 loss: 3.3228402137756348\n",
      "step 1744 loss: 3.253709554672241\n",
      "step 1745 loss: 3.1850337982177734\n",
      "step 1746 loss: 3.2708065509796143\n",
      "step 1747 loss: 3.2332403659820557\n",
      "step 1748 loss: 3.2145602703094482\n",
      "step 1749 loss: 3.3477554321289062\n",
      "step 1750 loss: 3.2488248348236084\n",
      "step 1751 loss: 3.2553226947784424\n",
      "step 1752 loss: 3.2281270027160645\n",
      "step 1753 loss: 3.283621072769165\n",
      "step 1754 loss: 3.2522151470184326\n",
      "step 1755 loss: 3.199564218521118\n",
      "step 1756 loss: 3.1470046043395996\n",
      "step 1757 loss: 3.239797830581665\n",
      "step 1758 loss: 3.235963821411133\n",
      "step 1759 loss: 3.29934024810791\n",
      "step 1760 loss: 3.264617443084717\n",
      "step 1761 loss: 3.253633975982666\n",
      "step 1762 loss: 3.289090394973755\n",
      "step 1763 loss: 3.360697031021118\n",
      "step 1764 loss: 3.2146542072296143\n",
      "step 1765 loss: 3.288250684738159\n",
      "step 1766 loss: 3.393120527267456\n",
      "step 1767 loss: 3.2064850330352783\n",
      "step 1768 loss: 3.250887393951416\n",
      "step 1769 loss: 3.124540328979492\n",
      "step 1770 loss: 3.33259654045105\n",
      "step 1771 loss: 3.2766246795654297\n",
      "step 1772 loss: 3.2970895767211914\n",
      "step 1773 loss: 3.2509167194366455\n",
      "step 1774 loss: 3.1512436866760254\n",
      "step 1775 loss: 3.220792293548584\n",
      "step 1776 loss: 3.18945050239563\n",
      "step 1777 loss: 3.2523484230041504\n",
      "step 1778 loss: 3.21089506149292\n",
      "step 1779 loss: 3.23282790184021\n",
      "step 1780 loss: 3.1959168910980225\n",
      "step 1781 loss: 3.2484803199768066\n",
      "step 1782 loss: 3.2951273918151855\n",
      "step 1783 loss: 3.293006658554077\n",
      "step 1784 loss: 3.164794921875\n",
      "step 1785 loss: 3.1379387378692627\n",
      "step 1786 loss: 3.2813334465026855\n",
      "step 1787 loss: 3.251490592956543\n",
      "step 1788 loss: 3.284731388092041\n",
      "step 1789 loss: 3.2634196281433105\n",
      "step 1790 loss: 3.383607864379883\n",
      "step 1791 loss: 3.3014841079711914\n",
      "step 1792 loss: 3.152881622314453\n",
      "step 1793 loss: 3.2044949531555176\n",
      "step 1794 loss: 3.21464467048645\n",
      "step 1795 loss: 3.251739263534546\n",
      "step 1796 loss: 3.2016429901123047\n",
      "step 1797 loss: 3.3023900985717773\n",
      "step 1798 loss: 3.181317090988159\n",
      "step 1799 loss: 3.2560224533081055\n",
      "step 1800 loss: 3.188281774520874\n",
      "step 1801 loss: 3.142737627029419\n",
      "step 1802 loss: 3.233375310897827\n",
      "step 1803 loss: 3.268373966217041\n",
      "step 1804 loss: 3.260279417037964\n",
      "step 1805 loss: 3.21990966796875\n",
      "step 1806 loss: 3.2062454223632812\n",
      "step 1807 loss: 3.1837644577026367\n",
      "step 1808 loss: 3.270448923110962\n",
      "step 1809 loss: 3.240145444869995\n",
      "step 1810 loss: 3.2017273902893066\n",
      "step 1811 loss: 3.3449759483337402\n",
      "step 1812 loss: 3.2808682918548584\n",
      "step 1813 loss: 3.135558843612671\n",
      "step 1814 loss: 3.2641661167144775\n",
      "step 1815 loss: 3.147162675857544\n",
      "step 1816 loss: 3.1737637519836426\n",
      "step 1817 loss: 3.225468158721924\n",
      "step 1818 loss: 3.166654348373413\n",
      "step 1819 loss: 3.2371485233306885\n",
      "step 1820 loss: 3.2115044593811035\n",
      "step 1821 loss: 3.2370917797088623\n",
      "step 1822 loss: 3.230342149734497\n",
      "step 1823 loss: 3.180807590484619\n",
      "step 1824 loss: 3.1261556148529053\n",
      "step 1825 loss: 3.2750484943389893\n",
      "step 1826 loss: 3.2684547901153564\n",
      "step 1827 loss: 3.18031907081604\n",
      "step 1828 loss: 3.1757001876831055\n",
      "step 1829 loss: 3.1403892040252686\n",
      "step 1830 loss: 3.137251853942871\n",
      "step 1831 loss: 3.193201780319214\n",
      "step 1832 loss: 3.169179677963257\n",
      "step 1833 loss: 3.2217323780059814\n",
      "step 1834 loss: 3.148465156555176\n",
      "step 1835 loss: 3.202866315841675\n",
      "step 1836 loss: 3.18298602104187\n",
      "step 1837 loss: 3.215474843978882\n",
      "step 1838 loss: 3.0985796451568604\n",
      "step 1839 loss: 3.1347897052764893\n",
      "step 1840 loss: 3.234564781188965\n",
      "step 1841 loss: 3.2387473583221436\n",
      "step 1842 loss: 3.243455410003662\n",
      "step 1843 loss: 3.289433240890503\n",
      "step 1844 loss: 3.1446478366851807\n",
      "step 1845 loss: 3.0913662910461426\n",
      "step 1846 loss: 3.265411615371704\n",
      "step 1847 loss: 3.217628240585327\n",
      "step 1848 loss: 3.209928274154663\n",
      "step 1849 loss: 3.19950532913208\n",
      "step 1850 loss: 3.186506986618042\n",
      "step 1851 loss: 3.158531665802002\n",
      "step 1852 loss: 3.2530322074890137\n",
      "step 1853 loss: 3.136744260787964\n",
      "step 1854 loss: 3.236229419708252\n",
      "step 1855 loss: 3.2270238399505615\n",
      "step 1856 loss: 3.1684365272521973\n",
      "step 1857 loss: 3.248812675476074\n",
      "step 1858 loss: 3.170032501220703\n",
      "step 1859 loss: 3.22448468208313\n",
      "step 1860 loss: 3.2327969074249268\n",
      "step 1861 loss: 3.1529855728149414\n",
      "step 1862 loss: 3.1956872940063477\n",
      "step 1863 loss: 3.196279287338257\n",
      "step 1864 loss: 3.170250177383423\n",
      "step 1865 loss: 3.182931900024414\n",
      "step 1866 loss: 3.1597397327423096\n",
      "step 1867 loss: 3.173687219619751\n",
      "step 1868 loss: 3.240507125854492\n",
      "step 1869 loss: 3.216970205307007\n",
      "step 1870 loss: 3.139387845993042\n",
      "step 1871 loss: 3.2718381881713867\n",
      "step 1872 loss: 3.1667943000793457\n",
      "step 1873 loss: 3.2273623943328857\n",
      "step 1874 loss: 3.111459970474243\n",
      "step 1875 loss: 3.1241326332092285\n",
      "step 1876 loss: 3.048426628112793\n",
      "step 1877 loss: 3.242657423019409\n",
      "step 1878 loss: 3.226447582244873\n",
      "step 1879 loss: 3.2108163833618164\n",
      "step 1880 loss: 3.219688653945923\n",
      "step 1881 loss: 3.2223494052886963\n",
      "step 1882 loss: 3.1774678230285645\n",
      "step 1883 loss: 3.2043519020080566\n",
      "step 1884 loss: 3.067566156387329\n",
      "step 1885 loss: 3.1854922771453857\n",
      "step 1886 loss: 3.168759822845459\n",
      "step 1887 loss: 3.1660726070404053\n",
      "step 1888 loss: 3.1845719814300537\n",
      "step 1889 loss: 3.2333579063415527\n",
      "step 1890 loss: 3.112468719482422\n",
      "step 1891 loss: 3.248905658721924\n",
      "step 1892 loss: 3.193861722946167\n",
      "step 1893 loss: 3.1332271099090576\n",
      "step 1894 loss: 3.132155418395996\n",
      "step 1895 loss: 3.2733216285705566\n",
      "step 1896 loss: 3.1544153690338135\n",
      "step 1897 loss: 3.154405117034912\n",
      "step 1898 loss: 3.2092809677124023\n",
      "step 1899 loss: 3.1713247299194336\n",
      "step 1900 loss: 3.2000553607940674\n",
      "step 1901 loss: 3.1810288429260254\n",
      "step 1902 loss: 3.1771464347839355\n",
      "step 1903 loss: 3.133437156677246\n",
      "step 1904 loss: 3.1616294384002686\n",
      "step 1905 loss: 3.067884922027588\n",
      "step 1906 loss: 3.1517398357391357\n",
      "step 1907 loss: 3.0834052562713623\n",
      "step 1908 loss: 3.1729912757873535\n",
      "step 1909 loss: 3.1626994609832764\n",
      "step 1910 loss: 3.249751091003418\n",
      "step 1911 loss: 3.195249557495117\n",
      "step 1912 loss: 3.08290433883667\n",
      "step 1913 loss: 3.1827571392059326\n",
      "step 1914 loss: 3.1378958225250244\n",
      "step 1915 loss: 3.206422805786133\n",
      "step 1916 loss: 3.127929210662842\n",
      "step 1917 loss: 3.118955612182617\n",
      "step 1918 loss: 3.067575216293335\n",
      "step 1919 loss: 3.1112189292907715\n",
      "step 1920 loss: 3.167844533920288\n",
      "step 1921 loss: 3.175004720687866\n",
      "step 1922 loss: 3.2001566886901855\n",
      "step 1923 loss: 3.1508004665374756\n",
      "step 1924 loss: 3.096160411834717\n",
      "step 1925 loss: 3.164214611053467\n",
      "step 1926 loss: 3.1305980682373047\n",
      "step 1927 loss: 3.1478798389434814\n",
      "step 1928 loss: 3.1440200805664062\n",
      "step 1929 loss: 3.149336814880371\n",
      "step 1930 loss: 3.200392484664917\n",
      "step 1931 loss: 3.085482120513916\n",
      "step 1932 loss: 3.2144322395324707\n",
      "step 1933 loss: 3.1413989067077637\n",
      "step 1934 loss: 3.2382497787475586\n",
      "step 1935 loss: 3.1616017818450928\n",
      "step 1936 loss: 3.2062597274780273\n",
      "step 1937 loss: 3.0590262413024902\n",
      "step 1938 loss: 3.10054874420166\n",
      "step 1939 loss: 3.1760027408599854\n",
      "step 1940 loss: 3.193441390991211\n",
      "step 1941 loss: 3.2057485580444336\n",
      "step 1942 loss: 3.1089072227478027\n",
      "step 1943 loss: 3.0877647399902344\n",
      "step 1944 loss: 3.1278698444366455\n",
      "step 1945 loss: 3.179140329360962\n",
      "step 1946 loss: 3.1848502159118652\n",
      "step 1947 loss: 3.1566693782806396\n",
      "step 1948 loss: 3.0711722373962402\n",
      "step 1949 loss: 3.1718311309814453\n",
      "step 1950 loss: 3.1148767471313477\n",
      "step 1951 loss: 3.1160824298858643\n",
      "step 1952 loss: 3.195338010787964\n",
      "step 1953 loss: 3.113147497177124\n",
      "step 1954 loss: 3.0903689861297607\n",
      "step 1955 loss: 3.1431329250335693\n",
      "step 1956 loss: 3.156747579574585\n",
      "step 1957 loss: 3.1508655548095703\n",
      "step 1958 loss: 3.2220771312713623\n",
      "step 1959 loss: 3.1531121730804443\n",
      "step 1960 loss: 3.2021515369415283\n",
      "step 1961 loss: 3.142089366912842\n",
      "step 1962 loss: 3.143181562423706\n",
      "step 1963 loss: 3.085115432739258\n",
      "step 1964 loss: 3.119288444519043\n",
      "step 1965 loss: 3.072899103164673\n",
      "step 1966 loss: 3.1777069568634033\n",
      "step 1967 loss: 3.1358895301818848\n",
      "step 1968 loss: 3.154249429702759\n",
      "step 1969 loss: 3.152416944503784\n",
      "step 1970 loss: 3.0898149013519287\n",
      "step 1971 loss: 3.0023396015167236\n",
      "step 1972 loss: 3.1756086349487305\n",
      "step 1973 loss: 3.135962963104248\n",
      "step 1974 loss: 3.0521023273468018\n",
      "step 1975 loss: 3.158364772796631\n",
      "step 1976 loss: 3.0889382362365723\n",
      "step 1977 loss: 3.155454635620117\n",
      "step 1978 loss: 3.140256404876709\n",
      "step 1979 loss: 3.222086191177368\n",
      "step 1980 loss: 3.0682859420776367\n",
      "step 1981 loss: 3.1442933082580566\n",
      "step 1982 loss: 3.096116065979004\n",
      "step 1983 loss: 3.188307523727417\n",
      "step 1984 loss: 3.134462356567383\n",
      "step 1985 loss: 3.0733675956726074\n",
      "step 1986 loss: 3.13222074508667\n",
      "step 1987 loss: 3.15771222114563\n",
      "step 1988 loss: 3.2608962059020996\n",
      "step 1989 loss: 3.1336147785186768\n",
      "step 1990 loss: 3.2295615673065186\n",
      "step 1991 loss: 3.021061897277832\n",
      "step 1992 loss: 3.1801645755767822\n",
      "step 1993 loss: 3.1033248901367188\n",
      "step 1994 loss: 3.061131477355957\n",
      "step 1995 loss: 2.991964101791382\n",
      "step 1996 loss: 3.1893320083618164\n",
      "step 1997 loss: 3.099975109100342\n",
      "step 1998 loss: 2.9654064178466797\n",
      "step 1999 loss: 3.1285109519958496\n",
      "step 2000 loss: 3.1371781826019287\n",
      "step 2001 loss: 3.0998754501342773\n",
      "step 2002 loss: 3.1844356060028076\n",
      "step 2003 loss: 3.1687443256378174\n",
      "step 2004 loss: 3.0856854915618896\n",
      "step 2005 loss: 3.087515115737915\n",
      "step 2006 loss: 3.0791869163513184\n",
      "step 2007 loss: 3.1380531787872314\n",
      "step 2008 loss: 3.1429102420806885\n",
      "step 2009 loss: 3.1544480323791504\n",
      "step 2010 loss: 3.0836427211761475\n",
      "step 2011 loss: 3.0858006477355957\n",
      "step 2012 loss: 3.1196279525756836\n",
      "step 2013 loss: 3.0560524463653564\n",
      "step 2014 loss: 3.15201997756958\n",
      "step 2015 loss: 3.032510995864868\n",
      "step 2016 loss: 3.1730923652648926\n",
      "step 2017 loss: 3.1327216625213623\n",
      "step 2018 loss: 3.060513734817505\n",
      "step 2019 loss: 3.130842924118042\n",
      "step 2020 loss: 3.1720478534698486\n",
      "step 2021 loss: 3.1365861892700195\n",
      "step 2022 loss: 3.1749072074890137\n",
      "step 2023 loss: 3.1894049644470215\n",
      "step 2024 loss: 3.0079870223999023\n",
      "step 2025 loss: 3.1649482250213623\n",
      "step 2026 loss: 3.1172094345092773\n",
      "step 2027 loss: 3.113755941390991\n",
      "step 2028 loss: 3.1096420288085938\n",
      "step 2029 loss: 3.091357946395874\n",
      "step 2030 loss: 3.1632370948791504\n",
      "step 2031 loss: 3.1491315364837646\n",
      "step 2032 loss: 3.069068431854248\n",
      "step 2033 loss: 3.138065814971924\n",
      "step 2034 loss: 3.1336331367492676\n",
      "step 2035 loss: 3.074863910675049\n",
      "step 2036 loss: 3.081613063812256\n",
      "step 2037 loss: 3.0854034423828125\n",
      "step 2038 loss: 3.1033976078033447\n",
      "step 2039 loss: 3.1194570064544678\n",
      "step 2040 loss: 3.03424334526062\n",
      "step 2041 loss: 3.1308095455169678\n",
      "step 2042 loss: 3.0935375690460205\n",
      "step 2043 loss: 3.0601656436920166\n",
      "step 2044 loss: 3.1926064491271973\n",
      "step 2045 loss: 3.1688320636749268\n",
      "step 2046 loss: 3.0792367458343506\n",
      "step 2047 loss: 3.096982717514038\n",
      "step 2048 loss: 3.1316347122192383\n",
      "step 2049 loss: 3.0840511322021484\n",
      "step 2050 loss: 3.1217265129089355\n",
      "step 2051 loss: 3.1120052337646484\n",
      "step 2052 loss: 3.1346521377563477\n",
      "step 2053 loss: 3.156332015991211\n",
      "step 2054 loss: 3.13847279548645\n",
      "step 2055 loss: 3.0932083129882812\n",
      "step 2056 loss: 3.090787172317505\n",
      "step 2057 loss: 3.0944058895111084\n",
      "step 2058 loss: 3.0682976245880127\n",
      "step 2059 loss: 3.0742251873016357\n",
      "step 2060 loss: 3.0254342555999756\n",
      "step 2061 loss: 2.9540445804595947\n",
      "step 2062 loss: 3.12485933303833\n",
      "step 2063 loss: 3.1176249980926514\n",
      "step 2064 loss: 3.0289976596832275\n",
      "step 2065 loss: 3.146937131881714\n",
      "step 2066 loss: 3.086996555328369\n",
      "step 2067 loss: 3.113874912261963\n",
      "step 2068 loss: 3.156153440475464\n",
      "step 2069 loss: 3.0886900424957275\n",
      "step 2070 loss: 3.0906407833099365\n",
      "step 2071 loss: 3.096355676651001\n",
      "step 2072 loss: 2.915548324584961\n",
      "step 2073 loss: 3.071183919906616\n",
      "step 2074 loss: 3.0095036029815674\n",
      "step 2075 loss: 3.068237543106079\n",
      "step 2076 loss: 3.118356704711914\n",
      "step 2077 loss: 3.1383936405181885\n",
      "step 2078 loss: 3.1075263023376465\n",
      "step 2079 loss: 3.0700249671936035\n",
      "step 2080 loss: 3.102172374725342\n",
      "step 2081 loss: 3.23335337638855\n",
      "step 2082 loss: 3.0973808765411377\n",
      "step 2083 loss: 3.029207706451416\n",
      "step 2084 loss: 3.0319344997406006\n",
      "step 2085 loss: 3.1349549293518066\n",
      "step 2086 loss: 3.0757288932800293\n",
      "step 2087 loss: 3.0725436210632324\n",
      "step 2088 loss: 2.999717950820923\n",
      "step 2089 loss: 3.132112741470337\n",
      "step 2090 loss: 3.0939130783081055\n",
      "step 2091 loss: 3.127272367477417\n",
      "step 2092 loss: 3.019232749938965\n",
      "step 2093 loss: 3.091869831085205\n",
      "step 2094 loss: 3.0361790657043457\n",
      "step 2095 loss: 3.056360960006714\n",
      "step 2096 loss: 3.1334023475646973\n",
      "step 2097 loss: 3.128350257873535\n",
      "step 2098 loss: 3.021678924560547\n",
      "step 2099 loss: 3.108039140701294\n",
      "step 2100 loss: 3.0028276443481445\n",
      "step 2101 loss: 3.146709680557251\n",
      "step 2102 loss: 3.1486430168151855\n",
      "step 2103 loss: 3.0929300785064697\n",
      "step 2104 loss: 3.0615363121032715\n",
      "step 2105 loss: 3.0417559146881104\n",
      "step 2106 loss: 3.063460111618042\n",
      "step 2107 loss: 3.009525775909424\n",
      "step 2108 loss: 3.1345720291137695\n",
      "step 2109 loss: 3.0597612857818604\n",
      "step 2110 loss: 3.011056900024414\n",
      "step 2111 loss: 3.080782890319824\n",
      "step 2112 loss: 3.0832319259643555\n",
      "step 2113 loss: 2.99129319190979\n",
      "step 2114 loss: 3.09507417678833\n",
      "step 2115 loss: 3.093895673751831\n",
      "step 2116 loss: 3.0154566764831543\n",
      "step 2117 loss: 3.078781843185425\n",
      "step 2118 loss: 3.050755023956299\n",
      "step 2119 loss: 3.0521421432495117\n",
      "step 2120 loss: 3.0700321197509766\n",
      "step 2121 loss: 3.046471357345581\n",
      "step 2122 loss: 3.036100149154663\n",
      "step 2123 loss: 3.100830554962158\n",
      "step 2124 loss: 3.050074338912964\n",
      "step 2125 loss: 3.081789016723633\n",
      "step 2126 loss: 3.0270721912384033\n",
      "step 2127 loss: 3.080142021179199\n",
      "step 2128 loss: 2.97471022605896\n",
      "step 2129 loss: 3.0658929347991943\n",
      "step 2130 loss: 3.025282859802246\n",
      "step 2131 loss: 3.0089457035064697\n",
      "step 2132 loss: 2.958301067352295\n",
      "step 2133 loss: 3.0811023712158203\n",
      "step 2134 loss: 3.1233348846435547\n",
      "step 2135 loss: 3.0981369018554688\n",
      "step 2136 loss: 3.0425126552581787\n",
      "step 2137 loss: 3.0476481914520264\n",
      "step 2138 loss: 3.1276116371154785\n",
      "step 2139 loss: 3.1049578189849854\n",
      "step 2140 loss: 3.1365420818328857\n",
      "step 2141 loss: 3.081303596496582\n",
      "step 2142 loss: 3.0375571250915527\n",
      "step 2143 loss: 3.0360519886016846\n",
      "step 2144 loss: 3.116507053375244\n",
      "step 2145 loss: 3.093311309814453\n",
      "step 2146 loss: 2.929199457168579\n",
      "step 2147 loss: 2.9194142818450928\n",
      "step 2148 loss: 3.0805773735046387\n",
      "step 2149 loss: 2.9242053031921387\n",
      "step 2150 loss: 3.083954095840454\n",
      "step 2151 loss: 2.9838807582855225\n",
      "step 2152 loss: 2.994476795196533\n",
      "step 2153 loss: 3.027378797531128\n",
      "step 2154 loss: 3.0352652072906494\n",
      "step 2155 loss: 3.086085081100464\n",
      "step 2156 loss: 3.0331084728240967\n",
      "step 2157 loss: 3.0685040950775146\n",
      "step 2158 loss: 3.0847280025482178\n",
      "step 2159 loss: 2.959824800491333\n",
      "step 2160 loss: 3.1087141036987305\n",
      "step 2161 loss: 3.1941535472869873\n",
      "step 2162 loss: 2.97802472114563\n",
      "step 2163 loss: 3.0077576637268066\n",
      "step 2164 loss: 3.016820192337036\n",
      "step 2165 loss: 2.9995243549346924\n",
      "step 2166 loss: 2.9696273803710938\n",
      "step 2167 loss: 3.079798460006714\n",
      "step 2168 loss: 3.039163827896118\n",
      "step 2169 loss: 3.1135241985321045\n",
      "step 2170 loss: 3.070789337158203\n",
      "step 2171 loss: 3.1557235717773438\n",
      "step 2172 loss: 3.088578701019287\n",
      "step 2173 loss: 3.032818078994751\n",
      "step 2174 loss: 3.0696630477905273\n",
      "step 2175 loss: 2.97894549369812\n",
      "step 2176 loss: 3.0052404403686523\n",
      "step 2177 loss: 3.047423839569092\n",
      "step 2178 loss: 3.1728386878967285\n",
      "step 2179 loss: 2.9482195377349854\n",
      "step 2180 loss: 3.0308878421783447\n",
      "step 2181 loss: 2.998441696166992\n",
      "step 2182 loss: 3.09611177444458\n",
      "step 2183 loss: 3.039339542388916\n",
      "step 2184 loss: 2.98217511177063\n",
      "step 2185 loss: 2.922358274459839\n",
      "step 2186 loss: 3.1762068271636963\n",
      "step 2187 loss: 3.018481731414795\n",
      "step 2188 loss: 3.0835893154144287\n",
      "step 2189 loss: 3.031970262527466\n",
      "step 2190 loss: 3.0637447834014893\n",
      "step 2191 loss: 3.098653793334961\n",
      "step 2192 loss: 3.0850043296813965\n",
      "step 2193 loss: 3.0631704330444336\n",
      "step 2194 loss: 3.0747463703155518\n",
      "step 2195 loss: 3.053783893585205\n",
      "step 2196 loss: 2.999281883239746\n",
      "step 2197 loss: 3.027580738067627\n",
      "step 2198 loss: 3.118748664855957\n",
      "step 2199 loss: 3.1091105937957764\n",
      "step 2200 loss: 3.058077812194824\n",
      "step 2201 loss: 3.1340770721435547\n",
      "step 2202 loss: 3.068110704421997\n",
      "step 2203 loss: 2.961151599884033\n",
      "step 2204 loss: 3.050320863723755\n",
      "step 2205 loss: 2.9409866333007812\n",
      "step 2206 loss: 2.945584774017334\n",
      "step 2207 loss: 3.0276601314544678\n",
      "step 2208 loss: 3.0631332397460938\n",
      "step 2209 loss: 3.0136499404907227\n",
      "step 2210 loss: 3.113133430480957\n",
      "step 2211 loss: 2.9898276329040527\n",
      "step 2212 loss: 3.0274531841278076\n",
      "step 2213 loss: 3.1515820026397705\n",
      "step 2214 loss: 2.9897565841674805\n",
      "step 2215 loss: 3.058990240097046\n",
      "step 2216 loss: 3.0847349166870117\n",
      "step 2217 loss: 2.998896598815918\n",
      "step 2218 loss: 2.941565752029419\n",
      "step 2219 loss: 2.9276726245880127\n",
      "step 2220 loss: 2.977642059326172\n",
      "step 2221 loss: 3.0451817512512207\n",
      "step 2222 loss: 3.068547248840332\n",
      "step 2223 loss: 2.981269121170044\n",
      "step 2224 loss: 2.9494478702545166\n",
      "step 2225 loss: 3.036046266555786\n",
      "step 2226 loss: 2.9564363956451416\n",
      "step 2227 loss: 3.0234267711639404\n",
      "step 2228 loss: 3.0278818607330322\n",
      "step 2229 loss: 3.061420202255249\n",
      "step 2230 loss: 3.0219883918762207\n",
      "step 2231 loss: 3.0100514888763428\n",
      "step 2232 loss: 3.1328694820404053\n",
      "step 2233 loss: 3.063810110092163\n",
      "step 2234 loss: 2.9380698204040527\n",
      "step 2235 loss: 3.1682097911834717\n",
      "step 2236 loss: 3.1176538467407227\n",
      "step 2237 loss: 3.022294759750366\n",
      "step 2238 loss: 3.0262868404388428\n",
      "step 2239 loss: 3.0371148586273193\n",
      "step 2240 loss: 3.019566059112549\n",
      "step 2241 loss: 3.0392754077911377\n",
      "step 2242 loss: 3.1068742275238037\n",
      "step 2243 loss: 3.062934398651123\n",
      "step 2244 loss: 2.967339038848877\n",
      "step 2245 loss: 2.979809045791626\n",
      "step 2246 loss: 2.931591749191284\n",
      "step 2247 loss: 2.9754481315612793\n",
      "step 2248 loss: 3.079509973526001\n",
      "step 2249 loss: 3.0183866024017334\n",
      "step 2250 loss: 3.084810733795166\n",
      "step 2251 loss: 2.859081983566284\n",
      "step 2252 loss: 3.0383999347686768\n",
      "step 2253 loss: 2.9613804817199707\n",
      "step 2254 loss: 3.0265395641326904\n",
      "step 2255 loss: 2.9436748027801514\n",
      "step 2256 loss: 3.0177001953125\n",
      "step 2257 loss: 3.0398786067962646\n",
      "step 2258 loss: 2.918114423751831\n",
      "step 2259 loss: 3.0790722370147705\n",
      "step 2260 loss: 3.1339504718780518\n",
      "step 2261 loss: 2.955155849456787\n",
      "step 2262 loss: 3.0349340438842773\n",
      "step 2263 loss: 3.032167673110962\n",
      "step 2264 loss: 3.0853018760681152\n",
      "step 2265 loss: 3.0692896842956543\n",
      "step 2266 loss: 2.953253746032715\n",
      "step 2267 loss: 3.0488224029541016\n",
      "step 2268 loss: 3.0339341163635254\n",
      "step 2269 loss: 3.0178611278533936\n",
      "step 2270 loss: 3.0758056640625\n",
      "step 2271 loss: 3.071317434310913\n",
      "step 2272 loss: 2.871304750442505\n",
      "step 2273 loss: 2.9051647186279297\n",
      "step 2274 loss: 2.946852445602417\n",
      "step 2275 loss: 3.0147757530212402\n",
      "step 2276 loss: 2.9781687259674072\n",
      "step 2277 loss: 2.9551308155059814\n",
      "step 2278 loss: 2.8475279808044434\n",
      "step 2279 loss: 2.9437575340270996\n",
      "step 2280 loss: 3.0870118141174316\n",
      "step 2281 loss: 3.0139145851135254\n",
      "step 2282 loss: 2.9863338470458984\n",
      "step 2283 loss: 3.001164436340332\n",
      "step 2284 loss: 3.017683982849121\n",
      "step 2285 loss: 2.984870433807373\n",
      "step 2286 loss: 3.040886163711548\n",
      "step 2287 loss: 3.0202524662017822\n",
      "step 2288 loss: 2.971613645553589\n",
      "step 2289 loss: 2.9977500438690186\n",
      "step 2290 loss: 3.0614383220672607\n",
      "step 2291 loss: 3.0148932933807373\n",
      "step 2292 loss: 2.9860613346099854\n",
      "step 2293 loss: 3.0165820121765137\n",
      "step 2294 loss: 3.031931161880493\n",
      "step 2295 loss: 3.0676095485687256\n",
      "step 2296 loss: 2.9420175552368164\n",
      "step 2297 loss: 3.0811285972595215\n",
      "step 2298 loss: 3.0700149536132812\n",
      "step 2299 loss: 2.900588274002075\n",
      "step 2300 loss: 2.958632707595825\n",
      "step 2301 loss: 3.0427277088165283\n",
      "step 2302 loss: 3.1179039478302\n",
      "step 2303 loss: 3.0285117626190186\n",
      "step 2304 loss: 2.912005662918091\n",
      "step 2305 loss: 3.016925096511841\n",
      "step 2306 loss: 2.9860873222351074\n",
      "step 2307 loss: 3.0668859481811523\n",
      "step 2308 loss: 2.9850966930389404\n",
      "step 2309 loss: 2.9085564613342285\n",
      "step 2310 loss: 3.003422498703003\n",
      "step 2311 loss: 3.0120720863342285\n",
      "step 2312 loss: 3.077281951904297\n",
      "step 2313 loss: 3.1211445331573486\n",
      "step 2314 loss: 2.9122209548950195\n",
      "step 2315 loss: 3.0806362628936768\n",
      "step 2316 loss: 2.9253013134002686\n",
      "step 2317 loss: 2.9660134315490723\n",
      "step 2318 loss: 2.964627742767334\n",
      "step 2319 loss: 2.976466655731201\n",
      "step 2320 loss: 3.0343809127807617\n",
      "step 2321 loss: 3.0096747875213623\n",
      "step 2322 loss: 2.9919893741607666\n",
      "step 2323 loss: 2.9902310371398926\n",
      "step 2324 loss: 3.0367085933685303\n",
      "step 2325 loss: 3.122474431991577\n",
      "step 2326 loss: 2.940030813217163\n",
      "step 2327 loss: 2.990438461303711\n",
      "step 2328 loss: 2.9970266819000244\n",
      "step 2329 loss: 3.0300586223602295\n",
      "step 2330 loss: 3.034355401992798\n",
      "step 2331 loss: 2.8923559188842773\n",
      "step 2332 loss: 2.956975221633911\n",
      "step 2333 loss: 3.0869176387786865\n",
      "step 2334 loss: 2.8774948120117188\n",
      "step 2335 loss: 3.0027313232421875\n",
      "step 2336 loss: 2.9986252784729004\n",
      "step 2337 loss: 2.908449172973633\n",
      "step 2338 loss: 3.0326623916625977\n",
      "step 2339 loss: 2.9842545986175537\n",
      "step 2340 loss: 2.9839093685150146\n",
      "step 2341 loss: 3.019772529602051\n",
      "step 2342 loss: 2.911618947982788\n",
      "step 2343 loss: 2.984022855758667\n",
      "step 2344 loss: 2.987377643585205\n",
      "step 2345 loss: 3.0399081707000732\n",
      "step 2346 loss: 2.9997200965881348\n",
      "step 2347 loss: 2.9883511066436768\n",
      "step 2348 loss: 3.008678436279297\n",
      "step 2349 loss: 2.9951987266540527\n",
      "step 2350 loss: 2.9610488414764404\n",
      "step 2351 loss: 2.9737558364868164\n",
      "step 2352 loss: 3.0012545585632324\n",
      "step 2353 loss: 2.9586029052734375\n",
      "step 2354 loss: 3.025743007659912\n",
      "step 2355 loss: 3.049149990081787\n",
      "step 2356 loss: 2.9277961254119873\n",
      "step 2357 loss: 3.0470027923583984\n",
      "step 2358 loss: 3.0806989669799805\n",
      "step 2359 loss: 2.964655876159668\n",
      "step 2360 loss: 3.17057728767395\n",
      "step 2361 loss: 3.0903232097625732\n",
      "step 2362 loss: 2.8694727420806885\n",
      "step 2363 loss: 3.0006725788116455\n",
      "step 2364 loss: 2.962352752685547\n",
      "step 2365 loss: 2.9524855613708496\n",
      "step 2366 loss: 2.929243803024292\n",
      "step 2367 loss: 2.968099355697632\n",
      "step 2368 loss: 3.0564470291137695\n",
      "step 2369 loss: 2.973501682281494\n",
      "step 2370 loss: 3.0198287963867188\n",
      "step 2371 loss: 2.882560968399048\n",
      "step 2372 loss: 2.975515127182007\n",
      "step 2373 loss: 2.9141452312469482\n",
      "step 2374 loss: 2.97524356842041\n",
      "step 2375 loss: 3.0018908977508545\n",
      "step 2376 loss: 2.983245611190796\n",
      "step 2377 loss: 2.847188711166382\n",
      "step 2378 loss: 3.0646073818206787\n",
      "step 2379 loss: 2.9503870010375977\n",
      "step 2380 loss: 2.98384952545166\n",
      "step 2381 loss: 3.0173094272613525\n",
      "step 2382 loss: 2.9790122509002686\n",
      "step 2383 loss: 3.072910785675049\n",
      "step 2384 loss: 2.941396474838257\n",
      "step 2385 loss: 3.1199309825897217\n",
      "step 2386 loss: 2.8872478008270264\n",
      "step 2387 loss: 2.9763009548187256\n",
      "step 2388 loss: 2.986454963684082\n",
      "step 2389 loss: 3.041753053665161\n",
      "step 2390 loss: 3.018838405609131\n",
      "step 2391 loss: 3.0098788738250732\n",
      "step 2392 loss: 2.9359755516052246\n",
      "step 2393 loss: 2.94661808013916\n",
      "step 2394 loss: 2.9673357009887695\n",
      "step 2395 loss: 2.99381422996521\n",
      "step 2396 loss: 2.9334754943847656\n",
      "step 2397 loss: 3.009455442428589\n",
      "step 2398 loss: 3.0259578227996826\n",
      "step 2399 loss: 2.979872226715088\n",
      "step 2400 loss: 2.981365919113159\n",
      "step 2401 loss: 2.990365743637085\n",
      "step 2402 loss: 2.942345380783081\n",
      "step 2403 loss: 3.0831260681152344\n",
      "step 2404 loss: 3.025279998779297\n",
      "step 2405 loss: 3.064959764480591\n",
      "step 2406 loss: 2.9057679176330566\n",
      "step 2407 loss: 3.0164809226989746\n",
      "step 2408 loss: 2.9644086360931396\n",
      "step 2409 loss: 3.0137789249420166\n",
      "step 2410 loss: 3.008354425430298\n",
      "step 2411 loss: 2.9626355171203613\n",
      "step 2412 loss: 2.973963737487793\n",
      "step 2413 loss: 3.0700175762176514\n",
      "step 2414 loss: 2.994379758834839\n",
      "step 2415 loss: 3.0188794136047363\n",
      "step 2416 loss: 2.9519522190093994\n",
      "step 2417 loss: 3.0560336112976074\n",
      "step 2418 loss: 2.9292452335357666\n",
      "step 2419 loss: 3.050422191619873\n",
      "step 2420 loss: 2.9843032360076904\n",
      "step 2421 loss: 3.022446393966675\n",
      "step 2422 loss: 2.883881092071533\n",
      "step 2423 loss: 2.9605607986450195\n",
      "step 2424 loss: 2.928565740585327\n",
      "step 2425 loss: 3.0147554874420166\n",
      "step 2426 loss: 2.9465949535369873\n",
      "step 2427 loss: 2.8898987770080566\n",
      "step 2428 loss: 3.003173828125\n",
      "step 2429 loss: 2.9537301063537598\n",
      "step 2430 loss: 2.919421672821045\n",
      "step 2431 loss: 2.969663619995117\n",
      "step 2432 loss: 2.9012649059295654\n",
      "step 2433 loss: 3.0119283199310303\n",
      "step 2434 loss: 2.9851505756378174\n",
      "step 2435 loss: 2.9392247200012207\n",
      "step 2436 loss: 3.0061275959014893\n",
      "step 2437 loss: 2.8908510208129883\n",
      "step 2438 loss: 3.030594825744629\n",
      "step 2439 loss: 3.0393195152282715\n",
      "step 2440 loss: 3.0884950160980225\n",
      "step 2441 loss: 2.9453890323638916\n",
      "step 2442 loss: 2.9548287391662598\n",
      "step 2443 loss: 2.926868200302124\n",
      "step 2444 loss: 2.9824492931365967\n",
      "step 2445 loss: 2.9646170139312744\n",
      "step 2446 loss: 2.9195783138275146\n",
      "step 2447 loss: 2.984762668609619\n",
      "step 2448 loss: 3.0028395652770996\n",
      "step 2449 loss: 3.0113065242767334\n",
      "step 2450 loss: 2.974921941757202\n",
      "step 2451 loss: 2.926969528198242\n",
      "step 2452 loss: 2.904402256011963\n",
      "step 2453 loss: 2.977170944213867\n",
      "step 2454 loss: 2.9568405151367188\n",
      "step 2455 loss: 3.067322015762329\n",
      "step 2456 loss: 2.876890182495117\n",
      "step 2457 loss: 2.881854772567749\n",
      "step 2458 loss: 2.9509599208831787\n",
      "step 2459 loss: 2.909736156463623\n",
      "step 2460 loss: 3.00323224067688\n",
      "step 2461 loss: 2.9419212341308594\n",
      "step 2462 loss: 2.9700703620910645\n",
      "step 2463 loss: 2.9479637145996094\n",
      "step 2464 loss: 2.95004940032959\n",
      "step 2465 loss: 2.951737403869629\n",
      "step 2466 loss: 3.009323835372925\n",
      "step 2467 loss: 2.9100797176361084\n",
      "step 2468 loss: 2.9179270267486572\n",
      "step 2469 loss: 2.9493048191070557\n",
      "step 2470 loss: 2.884925603866577\n",
      "step 2471 loss: 3.062793254852295\n",
      "step 2472 loss: 2.886305093765259\n",
      "step 2473 loss: 2.854666233062744\n",
      "step 2474 loss: 2.896415948867798\n",
      "step 2475 loss: 2.908766031265259\n",
      "step 2476 loss: 2.899261474609375\n",
      "step 2477 loss: 2.9292829036712646\n",
      "step 2478 loss: 2.883115291595459\n",
      "step 2479 loss: 3.0070016384124756\n",
      "step 2480 loss: 2.888296365737915\n",
      "step 2481 loss: 2.970019817352295\n",
      "step 2482 loss: 3.0161941051483154\n",
      "step 2483 loss: 2.8508124351501465\n",
      "step 2484 loss: 2.922701597213745\n",
      "step 2485 loss: 2.9443042278289795\n",
      "step 2486 loss: 2.880000591278076\n",
      "step 2487 loss: 2.91980242729187\n",
      "step 2488 loss: 2.9124112129211426\n",
      "step 2489 loss: 2.9421536922454834\n",
      "step 2490 loss: 2.911996603012085\n",
      "step 2491 loss: 2.9451522827148438\n",
      "step 2492 loss: 2.928617000579834\n",
      "step 2493 loss: 3.0002574920654297\n",
      "step 2494 loss: 2.796658515930176\n",
      "step 2495 loss: 2.898296356201172\n",
      "step 2496 loss: 2.957981586456299\n",
      "step 2497 loss: 3.0422871112823486\n",
      "step 2498 loss: 2.890578031539917\n",
      "step 2499 loss: 2.9896976947784424\n",
      "step 2500 loss: 2.9196817874908447\n",
      "step 2501 loss: 3.002495527267456\n",
      "step 2502 loss: 2.959759473800659\n",
      "step 2503 loss: 2.8683581352233887\n",
      "step 2504 loss: 2.9056341648101807\n",
      "step 2505 loss: 2.96315860748291\n",
      "step 2506 loss: 2.9347219467163086\n",
      "step 2507 loss: 2.950213670730591\n",
      "step 2508 loss: 3.007791042327881\n",
      "step 2509 loss: 2.8591275215148926\n",
      "step 2510 loss: 2.947270154953003\n",
      "step 2511 loss: 2.9054088592529297\n",
      "step 2512 loss: 2.936553716659546\n",
      "step 2513 loss: 2.860896348953247\n",
      "step 2514 loss: 2.8934338092803955\n",
      "step 2515 loss: 2.967453718185425\n",
      "step 2516 loss: 2.9874062538146973\n",
      "step 2517 loss: 2.918863534927368\n",
      "step 2518 loss: 2.973937511444092\n",
      "step 2519 loss: 2.9628677368164062\n",
      "step 2520 loss: 2.9133636951446533\n",
      "step 2521 loss: 3.0582563877105713\n",
      "step 2522 loss: 2.8939971923828125\n",
      "step 2523 loss: 2.9203829765319824\n",
      "step 2524 loss: 2.957836151123047\n",
      "step 2525 loss: 2.976834535598755\n",
      "step 2526 loss: 2.920924663543701\n",
      "step 2527 loss: 2.951456308364868\n",
      "step 2528 loss: 3.073383331298828\n",
      "step 2529 loss: 2.9041221141815186\n",
      "step 2530 loss: 2.889951229095459\n",
      "step 2531 loss: 2.958195924758911\n",
      "step 2532 loss: 2.9142019748687744\n",
      "step 2533 loss: 2.8576176166534424\n",
      "step 2534 loss: 3.03767728805542\n",
      "step 2535 loss: 2.8578929901123047\n",
      "step 2536 loss: 2.8522467613220215\n",
      "step 2537 loss: 3.0091893672943115\n",
      "step 2538 loss: 2.8855957984924316\n",
      "step 2539 loss: 2.8940088748931885\n",
      "step 2540 loss: 2.900139093399048\n",
      "step 2541 loss: 2.985046863555908\n",
      "step 2542 loss: 2.845860719680786\n",
      "step 2543 loss: 2.998785972595215\n",
      "step 2544 loss: 2.9601423740386963\n",
      "step 2545 loss: 2.856452465057373\n",
      "step 2546 loss: 2.8723697662353516\n",
      "step 2547 loss: 2.897765636444092\n",
      "step 2548 loss: 2.836461305618286\n",
      "step 2549 loss: 2.987748622894287\n",
      "step 2550 loss: 2.865995407104492\n",
      "step 2551 loss: 2.832012414932251\n",
      "step 2552 loss: 2.772550106048584\n",
      "step 2553 loss: 2.95123291015625\n",
      "step 2554 loss: 2.943087339401245\n",
      "step 2555 loss: 2.8773300647735596\n",
      "step 2556 loss: 2.9083356857299805\n",
      "step 2557 loss: 3.040323495864868\n",
      "step 2558 loss: 2.8420774936676025\n",
      "step 2559 loss: 2.9049978256225586\n",
      "step 2560 loss: 3.018717050552368\n",
      "step 2561 loss: 2.9456875324249268\n",
      "step 2562 loss: 2.8906805515289307\n",
      "step 2563 loss: 2.844960927963257\n",
      "step 2564 loss: 3.0332844257354736\n",
      "step 2565 loss: 2.801999807357788\n",
      "step 2566 loss: 2.9015660285949707\n",
      "step 2567 loss: 2.837791681289673\n",
      "step 2568 loss: 2.983182907104492\n",
      "step 2569 loss: 2.931647539138794\n",
      "step 2570 loss: 2.975310802459717\n",
      "step 2571 loss: 2.8827576637268066\n",
      "step 2572 loss: 2.914747953414917\n",
      "step 2573 loss: 3.00122332572937\n",
      "step 2574 loss: 2.9076688289642334\n",
      "step 2575 loss: 2.8677923679351807\n",
      "step 2576 loss: 2.8739137649536133\n",
      "step 2577 loss: 2.9637093544006348\n",
      "step 2578 loss: 2.946993589401245\n",
      "step 2579 loss: 2.8703110218048096\n",
      "step 2580 loss: 2.777315616607666\n",
      "step 2581 loss: 2.892508029937744\n",
      "step 2582 loss: 2.9391071796417236\n",
      "step 2583 loss: 2.962773323059082\n",
      "step 2584 loss: 2.872032642364502\n",
      "step 2585 loss: 2.9013867378234863\n",
      "step 2586 loss: 2.889827013015747\n",
      "step 2587 loss: 2.8452351093292236\n",
      "step 2588 loss: 2.8819034099578857\n",
      "step 2589 loss: 2.888742208480835\n",
      "step 2590 loss: 2.8512179851531982\n",
      "step 2591 loss: 2.8856892585754395\n",
      "step 2592 loss: 2.8518924713134766\n",
      "step 2593 loss: 3.1034507751464844\n",
      "step 2594 loss: 2.9097938537597656\n",
      "step 2595 loss: 2.9126973152160645\n",
      "step 2596 loss: 2.832943916320801\n",
      "step 2597 loss: 2.881762981414795\n",
      "step 2598 loss: 2.929321527481079\n",
      "step 2599 loss: 2.9124979972839355\n",
      "step 2600 loss: 2.8414011001586914\n",
      "step 2601 loss: 2.9381351470947266\n",
      "step 2602 loss: 2.868865728378296\n",
      "step 2603 loss: 2.914440631866455\n",
      "step 2604 loss: 2.9774258136749268\n",
      "step 2605 loss: 2.8320112228393555\n",
      "step 2606 loss: 2.860618829727173\n",
      "step 2607 loss: 2.9181900024414062\n",
      "step 2608 loss: 2.992793321609497\n",
      "step 2609 loss: 2.895111083984375\n",
      "step 2610 loss: 2.9246466159820557\n",
      "step 2611 loss: 2.876476764678955\n",
      "step 2612 loss: 2.822237491607666\n",
      "step 2613 loss: 2.8933329582214355\n",
      "step 2614 loss: 2.971468687057495\n",
      "step 2615 loss: 2.8394856452941895\n",
      "step 2616 loss: 2.90859055519104\n",
      "step 2617 loss: 2.8670315742492676\n",
      "step 2618 loss: 2.922029495239258\n",
      "step 2619 loss: 2.8924765586853027\n",
      "step 2620 loss: 2.917736768722534\n",
      "step 2621 loss: 2.815319776535034\n",
      "step 2622 loss: 2.883291721343994\n",
      "step 2623 loss: 2.8850560188293457\n",
      "step 2624 loss: 2.9169843196868896\n",
      "step 2625 loss: 3.014596700668335\n",
      "step 2626 loss: 2.820101737976074\n",
      "step 2627 loss: 2.834290027618408\n",
      "step 2628 loss: 2.9642930030822754\n",
      "step 2629 loss: 2.8855321407318115\n",
      "step 2630 loss: 2.7908482551574707\n",
      "step 2631 loss: 2.9423718452453613\n",
      "step 2632 loss: 2.9595725536346436\n",
      "step 2633 loss: 2.8835666179656982\n",
      "step 2634 loss: 2.771759510040283\n",
      "step 2635 loss: 2.8500144481658936\n",
      "step 2636 loss: 2.9358651638031006\n",
      "step 2637 loss: 2.768362522125244\n",
      "step 2638 loss: 2.825695276260376\n",
      "step 2639 loss: 2.9605352878570557\n",
      "step 2640 loss: 2.9144034385681152\n",
      "step 2641 loss: 2.9030301570892334\n",
      "step 2642 loss: 2.9079859256744385\n",
      "step 2643 loss: 2.876386880874634\n",
      "step 2644 loss: 2.8989510536193848\n",
      "step 2645 loss: 2.8117921352386475\n",
      "step 2646 loss: 2.8889458179473877\n",
      "step 2647 loss: 2.91746187210083\n",
      "step 2648 loss: 2.9113855361938477\n",
      "step 2649 loss: 2.851529121398926\n",
      "step 2650 loss: 2.9619991779327393\n",
      "step 2651 loss: 2.9741263389587402\n",
      "step 2652 loss: 2.8915040493011475\n",
      "step 2653 loss: 2.84401535987854\n",
      "step 2654 loss: 2.8693535327911377\n",
      "step 2655 loss: 2.788264274597168\n",
      "step 2656 loss: 2.854719400405884\n",
      "step 2657 loss: 2.8885905742645264\n",
      "step 2658 loss: 2.872464895248413\n",
      "step 2659 loss: 2.890793561935425\n",
      "step 2660 loss: 2.8050882816314697\n",
      "step 2661 loss: 2.935861349105835\n",
      "step 2662 loss: 2.9189112186431885\n",
      "step 2663 loss: 2.872091054916382\n",
      "step 2664 loss: 2.9350082874298096\n",
      "step 2665 loss: 2.8307299613952637\n",
      "step 2666 loss: 2.8691399097442627\n",
      "step 2667 loss: 2.8211960792541504\n",
      "step 2668 loss: 2.931060552597046\n",
      "step 2669 loss: 2.923621892929077\n",
      "step 2670 loss: 2.9768550395965576\n",
      "step 2671 loss: 2.915132522583008\n",
      "step 2672 loss: 2.9190964698791504\n",
      "step 2673 loss: 3.0208661556243896\n",
      "step 2674 loss: 2.9284589290618896\n",
      "step 2675 loss: 2.883631706237793\n",
      "step 2676 loss: 2.992952823638916\n",
      "step 2677 loss: 2.967388153076172\n",
      "step 2678 loss: 2.945075035095215\n",
      "step 2679 loss: 2.8645713329315186\n",
      "step 2680 loss: 2.897557258605957\n",
      "step 2681 loss: 2.7628273963928223\n",
      "step 2682 loss: 2.9657416343688965\n",
      "step 2683 loss: 2.9202423095703125\n",
      "step 2684 loss: 2.8297204971313477\n",
      "step 2685 loss: 2.7885096073150635\n",
      "step 2686 loss: 2.742485761642456\n",
      "step 2687 loss: 2.9222195148468018\n",
      "step 2688 loss: 2.8196659088134766\n",
      "step 2689 loss: 2.9081902503967285\n",
      "step 2690 loss: 2.782928943634033\n",
      "step 2691 loss: 2.856600522994995\n",
      "step 2692 loss: 2.8603506088256836\n",
      "step 2693 loss: 2.7795116901397705\n",
      "step 2694 loss: 2.835683584213257\n",
      "step 2695 loss: 2.921006917953491\n",
      "step 2696 loss: 2.876309394836426\n",
      "step 2697 loss: 2.8745882511138916\n",
      "step 2698 loss: 2.7728519439697266\n",
      "step 2699 loss: 2.8995072841644287\n",
      "step 2700 loss: 2.8905837535858154\n",
      "step 2701 loss: 2.8760669231414795\n",
      "step 2702 loss: 2.9377992153167725\n",
      "step 2703 loss: 2.833820343017578\n",
      "step 2704 loss: 2.8750216960906982\n",
      "step 2705 loss: 2.8147165775299072\n",
      "step 2706 loss: 2.945772886276245\n",
      "step 2707 loss: 2.8551793098449707\n",
      "step 2708 loss: 2.8257246017456055\n",
      "step 2709 loss: 2.896160840988159\n",
      "step 2710 loss: 2.811885356903076\n",
      "step 2711 loss: 2.821260452270508\n",
      "step 2712 loss: 2.835782051086426\n",
      "step 2713 loss: 2.8541579246520996\n",
      "step 2714 loss: 2.866812229156494\n",
      "step 2715 loss: 2.856149911880493\n",
      "step 2716 loss: 2.9145278930664062\n",
      "step 2717 loss: 2.874692916870117\n",
      "step 2718 loss: 2.9502360820770264\n",
      "step 2719 loss: 2.9096317291259766\n",
      "step 2720 loss: 2.868516445159912\n",
      "step 2721 loss: 2.858367919921875\n",
      "step 2722 loss: 2.897754192352295\n",
      "step 2723 loss: 2.925497531890869\n",
      "step 2724 loss: 2.8464226722717285\n",
      "step 2725 loss: 2.8934903144836426\n",
      "step 2726 loss: 2.9165666103363037\n",
      "step 2727 loss: 2.944995164871216\n",
      "step 2728 loss: 2.874319553375244\n",
      "step 2729 loss: 2.930736780166626\n",
      "step 2730 loss: 2.8895092010498047\n",
      "step 2731 loss: 2.8860790729522705\n",
      "step 2732 loss: 2.8191115856170654\n",
      "step 2733 loss: 2.978058099746704\n",
      "step 2734 loss: 2.807647228240967\n",
      "step 2735 loss: 2.840251922607422\n",
      "step 2736 loss: 2.8866684436798096\n",
      "step 2737 loss: 2.813384771347046\n",
      "step 2738 loss: 2.7526447772979736\n",
      "step 2739 loss: 2.843871831893921\n",
      "step 2740 loss: 2.7601468563079834\n",
      "step 2741 loss: 2.94073486328125\n",
      "step 2742 loss: 2.8249475955963135\n",
      "step 2743 loss: 2.8114922046661377\n",
      "step 2744 loss: 2.992952823638916\n",
      "step 2745 loss: 2.8083624839782715\n",
      "step 2746 loss: 2.8759894371032715\n",
      "step 2747 loss: 2.888209104537964\n",
      "step 2748 loss: 2.8960306644439697\n",
      "step 2749 loss: 2.884876251220703\n",
      "step 2750 loss: 2.8619205951690674\n",
      "step 2751 loss: 2.720263957977295\n",
      "step 2752 loss: 2.8309967517852783\n",
      "step 2753 loss: 2.839766502380371\n",
      "step 2754 loss: 2.8970279693603516\n",
      "step 2755 loss: 2.9504358768463135\n",
      "step 2756 loss: 2.791386842727661\n",
      "step 2757 loss: 2.9090442657470703\n",
      "step 2758 loss: 2.848266363143921\n",
      "step 2759 loss: 2.8043105602264404\n",
      "step 2760 loss: 2.7122271060943604\n",
      "step 2761 loss: 2.8315439224243164\n",
      "step 2762 loss: 2.864145040512085\n",
      "step 2763 loss: 2.9266860485076904\n",
      "step 2764 loss: 2.8663170337677\n",
      "step 2765 loss: 2.782512903213501\n",
      "step 2766 loss: 2.8926000595092773\n",
      "step 2767 loss: 2.796600818634033\n",
      "step 2768 loss: 2.734926223754883\n",
      "step 2769 loss: 2.860382080078125\n",
      "step 2770 loss: 2.9373891353607178\n",
      "step 2771 loss: 2.8848304748535156\n",
      "step 2772 loss: 2.870936393737793\n",
      "step 2773 loss: 2.9125523567199707\n",
      "step 2774 loss: 2.888251781463623\n",
      "step 2775 loss: 2.943720579147339\n",
      "step 2776 loss: 2.8452651500701904\n",
      "step 2777 loss: 2.8488352298736572\n",
      "step 2778 loss: 2.890799045562744\n",
      "step 2779 loss: 2.951345205307007\n",
      "step 2780 loss: 2.726951837539673\n",
      "step 2781 loss: 2.8627142906188965\n",
      "step 2782 loss: 2.903149127960205\n",
      "step 2783 loss: 2.8952035903930664\n",
      "step 2784 loss: 2.831380605697632\n",
      "step 2785 loss: 2.8735849857330322\n",
      "step 2786 loss: 2.9107415676116943\n",
      "step 2787 loss: 2.8596348762512207\n",
      "step 2788 loss: 2.9029035568237305\n",
      "step 2789 loss: 2.8552660942077637\n",
      "step 2790 loss: 2.928049325942993\n",
      "step 2791 loss: 2.892218589782715\n",
      "step 2792 loss: 2.8618714809417725\n",
      "step 2793 loss: 2.8673534393310547\n",
      "step 2794 loss: 2.8497085571289062\n",
      "step 2795 loss: 2.783519744873047\n",
      "step 2796 loss: 2.806072950363159\n",
      "step 2797 loss: 2.8420376777648926\n",
      "step 2798 loss: 2.771099328994751\n",
      "step 2799 loss: 2.8744940757751465\n",
      "step 2800 loss: 2.9735329151153564\n",
      "step 2801 loss: 2.7426633834838867\n",
      "step 2802 loss: 2.7792928218841553\n",
      "step 2803 loss: 2.7981150150299072\n",
      "step 2804 loss: 2.8473427295684814\n",
      "step 2805 loss: 2.9010539054870605\n",
      "step 2806 loss: 2.8566958904266357\n",
      "step 2807 loss: 2.9527251720428467\n",
      "step 2808 loss: 2.7577974796295166\n",
      "step 2809 loss: 2.917422294616699\n",
      "step 2810 loss: 2.8640389442443848\n",
      "step 2811 loss: 2.8002026081085205\n",
      "step 2812 loss: 2.8194456100463867\n",
      "step 2813 loss: 2.8187761306762695\n",
      "step 2814 loss: 2.8414342403411865\n",
      "step 2815 loss: 2.763331413269043\n",
      "step 2816 loss: 2.775535821914673\n",
      "step 2817 loss: 2.831378221511841\n",
      "step 2818 loss: 2.889362335205078\n",
      "step 2819 loss: 2.9327399730682373\n",
      "step 2820 loss: 2.752016067504883\n",
      "step 2821 loss: 2.8842618465423584\n",
      "step 2822 loss: 2.8366291522979736\n",
      "step 2823 loss: 2.7978017330169678\n",
      "step 2824 loss: 2.852869987487793\n",
      "step 2825 loss: 2.9041669368743896\n",
      "step 2826 loss: 2.946866035461426\n",
      "step 2827 loss: 2.8412258625030518\n",
      "step 2828 loss: 2.6716110706329346\n",
      "step 2829 loss: 2.9643242359161377\n",
      "step 2830 loss: 2.7803995609283447\n",
      "step 2831 loss: 2.8647310733795166\n",
      "step 2832 loss: 2.7721149921417236\n",
      "step 2833 loss: 2.9833950996398926\n",
      "step 2834 loss: 2.7502989768981934\n",
      "step 2835 loss: 2.8813955783843994\n",
      "step 2836 loss: 2.859661340713501\n",
      "step 2837 loss: 2.8391478061676025\n",
      "step 2838 loss: 2.881484031677246\n",
      "step 2839 loss: 2.841099262237549\n",
      "step 2840 loss: 2.861518144607544\n",
      "step 2841 loss: 2.7409214973449707\n",
      "step 2842 loss: 2.8100879192352295\n",
      "step 2843 loss: 2.927021026611328\n",
      "step 2844 loss: 2.855769395828247\n",
      "step 2845 loss: 2.7975709438323975\n",
      "step 2846 loss: 2.863377332687378\n",
      "step 2847 loss: 2.827524423599243\n",
      "step 2848 loss: 2.770864486694336\n",
      "step 2849 loss: 2.825584888458252\n",
      "step 2850 loss: 2.7729458808898926\n",
      "step 2851 loss: 2.8541789054870605\n",
      "step 2852 loss: 2.9578697681427\n",
      "step 2853 loss: 2.8618218898773193\n",
      "step 2854 loss: 2.9470415115356445\n",
      "step 2855 loss: 2.8124756813049316\n",
      "step 2856 loss: 2.8129453659057617\n",
      "step 2857 loss: 2.887542724609375\n",
      "step 2858 loss: 2.8506970405578613\n",
      "step 2859 loss: 2.897874355316162\n",
      "step 2860 loss: 2.8730881214141846\n",
      "step 2861 loss: 2.778639793395996\n",
      "step 2862 loss: 2.812518835067749\n",
      "step 2863 loss: 2.922020435333252\n",
      "step 2864 loss: 2.796685218811035\n",
      "step 2865 loss: 3.000771999359131\n",
      "step 2866 loss: 2.887993812561035\n",
      "step 2867 loss: 2.7880122661590576\n",
      "step 2868 loss: 2.845083713531494\n",
      "step 2869 loss: 2.907573938369751\n",
      "step 2870 loss: 2.7596874237060547\n",
      "step 2871 loss: 2.7516300678253174\n",
      "step 2872 loss: 2.797384023666382\n",
      "step 2873 loss: 2.8014445304870605\n",
      "step 2874 loss: 2.8190245628356934\n",
      "step 2875 loss: 2.937370777130127\n",
      "step 2876 loss: 2.809859275817871\n",
      "step 2877 loss: 2.8013875484466553\n",
      "step 2878 loss: 2.7185730934143066\n",
      "step 2879 loss: 2.773261547088623\n",
      "step 2880 loss: 2.843362808227539\n",
      "step 2881 loss: 2.795741319656372\n",
      "step 2882 loss: 2.7983040809631348\n",
      "step 2883 loss: 2.9292118549346924\n",
      "step 2884 loss: 2.834050178527832\n",
      "step 2885 loss: 2.7503809928894043\n",
      "step 2886 loss: 2.7764647006988525\n",
      "step 2887 loss: 2.770946979522705\n",
      "step 2888 loss: 2.8177928924560547\n",
      "step 2889 loss: 2.8002309799194336\n",
      "step 2890 loss: 2.8371713161468506\n",
      "step 2891 loss: 2.7514359951019287\n",
      "step 2892 loss: 2.826714038848877\n",
      "step 2893 loss: 2.843538999557495\n",
      "step 2894 loss: 2.9417970180511475\n",
      "step 2895 loss: 2.8699018955230713\n",
      "step 2896 loss: 2.816441774368286\n",
      "step 2897 loss: 2.920309066772461\n",
      "step 2898 loss: 2.8175246715545654\n",
      "step 2899 loss: 2.9237565994262695\n",
      "step 2900 loss: 2.808624029159546\n",
      "step 2901 loss: 2.843127489089966\n",
      "step 2902 loss: 2.9119279384613037\n",
      "step 2903 loss: 2.8578686714172363\n",
      "step 2904 loss: 2.840189218521118\n",
      "step 2905 loss: 2.867262601852417\n",
      "step 2906 loss: 2.8660366535186768\n",
      "step 2907 loss: 2.861734390258789\n",
      "step 2908 loss: 2.8114123344421387\n",
      "step 2909 loss: 2.793778657913208\n",
      "step 2910 loss: 2.8717873096466064\n",
      "step 2911 loss: 2.8159379959106445\n",
      "step 2912 loss: 2.8705315589904785\n",
      "step 2913 loss: 2.821120023727417\n",
      "step 2914 loss: 2.856417417526245\n",
      "step 2915 loss: 2.74483060836792\n",
      "step 2916 loss: 2.746103286743164\n",
      "step 2917 loss: 2.8055033683776855\n",
      "step 2918 loss: 2.767315149307251\n",
      "step 2919 loss: 2.7757012844085693\n",
      "step 2920 loss: 2.8220407962799072\n",
      "step 2921 loss: 2.9193546772003174\n",
      "step 2922 loss: 2.907313823699951\n",
      "step 2923 loss: 2.9233548641204834\n",
      "step 2924 loss: 2.831981897354126\n",
      "step 2925 loss: 2.9268672466278076\n",
      "step 2926 loss: 2.79632568359375\n",
      "step 2927 loss: 2.8474557399749756\n",
      "step 2928 loss: 2.8754618167877197\n",
      "step 2929 loss: 2.7779932022094727\n",
      "step 2930 loss: 2.7593257427215576\n",
      "step 2931 loss: 2.7910380363464355\n",
      "step 2932 loss: 2.785309314727783\n",
      "step 2933 loss: 2.8042359352111816\n",
      "step 2934 loss: 2.947789430618286\n",
      "step 2935 loss: 2.7497005462646484\n",
      "step 2936 loss: 2.830610752105713\n",
      "step 2937 loss: 2.8056461811065674\n",
      "step 2938 loss: 2.837053060531616\n",
      "step 2939 loss: 2.746370315551758\n",
      "step 2940 loss: 2.854343891143799\n",
      "step 2941 loss: 2.800994634628296\n",
      "step 2942 loss: 2.7980759143829346\n",
      "step 2943 loss: 2.8926143646240234\n",
      "step 2944 loss: 2.7082574367523193\n",
      "step 2945 loss: 2.7082316875457764\n",
      "step 2946 loss: 2.815317392349243\n",
      "step 2947 loss: 2.917276382446289\n",
      "step 2948 loss: 2.9022676944732666\n",
      "step 2949 loss: 2.7784957885742188\n",
      "step 2950 loss: 2.7504312992095947\n",
      "step 2951 loss: 2.8503074645996094\n",
      "step 2952 loss: 2.8049476146698\n",
      "step 2953 loss: 2.8382985591888428\n",
      "step 2954 loss: 2.8780179023742676\n",
      "step 2955 loss: 2.845153570175171\n",
      "step 2956 loss: 2.8755059242248535\n",
      "step 2957 loss: 2.7772743701934814\n",
      "step 2958 loss: 2.7586302757263184\n",
      "step 2959 loss: 2.6903436183929443\n",
      "step 2960 loss: 2.8267195224761963\n",
      "step 2961 loss: 2.8788986206054688\n",
      "step 2962 loss: 2.8203370571136475\n",
      "step 2963 loss: 2.8567404747009277\n",
      "step 2964 loss: 2.882744789123535\n",
      "step 2965 loss: 2.821241855621338\n",
      "step 2966 loss: 2.819390296936035\n",
      "step 2967 loss: 2.8500516414642334\n",
      "step 2968 loss: 2.7921719551086426\n",
      "step 2969 loss: 2.941516876220703\n",
      "step 2970 loss: 2.7818119525909424\n",
      "step 2971 loss: 2.825270891189575\n",
      "step 2972 loss: 2.9229607582092285\n",
      "step 2973 loss: 2.8868298530578613\n",
      "step 2974 loss: 2.743924856185913\n",
      "step 2975 loss: 2.8335437774658203\n",
      "step 2976 loss: 2.749018430709839\n",
      "step 2977 loss: 2.8171935081481934\n",
      "step 2978 loss: 2.7692902088165283\n",
      "step 2979 loss: 2.769063949584961\n",
      "step 2980 loss: 2.8197593688964844\n",
      "step 2981 loss: 2.8059985637664795\n",
      "step 2982 loss: 2.85968279838562\n",
      "step 2983 loss: 2.857548236846924\n",
      "step 2984 loss: 2.818514108657837\n",
      "step 2985 loss: 2.8996877670288086\n",
      "step 2986 loss: 2.8182411193847656\n",
      "step 2987 loss: 2.728646993637085\n",
      "step 2988 loss: 2.7937915325164795\n",
      "step 2989 loss: 2.827455997467041\n",
      "step 2990 loss: 2.762758255004883\n",
      "step 2991 loss: 2.802076578140259\n",
      "step 2992 loss: 2.7983546257019043\n",
      "step 2993 loss: 2.640350818634033\n",
      "step 2994 loss: 2.707714319229126\n",
      "step 2995 loss: 2.864464044570923\n",
      "step 2996 loss: 2.792626142501831\n",
      "step 2997 loss: 2.746523857116699\n",
      "step 2998 loss: 2.9377808570861816\n",
      "step 2999 loss: 2.8297815322875977\n",
      "step 3000 loss: 2.776794672012329\n",
      "step 3001 loss: 2.7998206615448\n",
      "step 3002 loss: 2.8766863346099854\n",
      "step 3003 loss: 2.738776683807373\n",
      "step 3004 loss: 2.8960511684417725\n",
      "step 3005 loss: 2.755347967147827\n",
      "step 3006 loss: 2.7576985359191895\n",
      "step 3007 loss: 2.74898362159729\n",
      "step 3008 loss: 2.7717649936676025\n",
      "step 3009 loss: 2.8489859104156494\n",
      "step 3010 loss: 2.7932214736938477\n",
      "step 3011 loss: 2.7735042572021484\n",
      "step 3012 loss: 2.7220113277435303\n",
      "step 3013 loss: 2.8436484336853027\n",
      "step 3014 loss: 2.7755465507507324\n",
      "step 3015 loss: 2.8557887077331543\n",
      "step 3016 loss: 2.7090094089508057\n",
      "step 3017 loss: 2.7636284828186035\n",
      "step 3018 loss: 2.904876708984375\n",
      "step 3019 loss: 2.757000207901001\n",
      "step 3020 loss: 2.930415153503418\n",
      "step 3021 loss: 2.8338916301727295\n",
      "step 3022 loss: 2.757002592086792\n",
      "step 3023 loss: 2.811677932739258\n",
      "step 3024 loss: 2.723801374435425\n",
      "step 3025 loss: 2.7747113704681396\n",
      "step 3026 loss: 2.8356518745422363\n",
      "step 3027 loss: 2.8147618770599365\n",
      "step 3028 loss: 2.733133316040039\n",
      "step 3029 loss: 2.7387115955352783\n",
      "step 3030 loss: 2.703916549682617\n",
      "step 3031 loss: 2.7876408100128174\n",
      "step 3032 loss: 2.7838618755340576\n",
      "step 3033 loss: 2.7505838871002197\n",
      "step 3034 loss: 2.807966709136963\n",
      "step 3035 loss: 2.792924404144287\n",
      "step 3036 loss: 2.761110782623291\n",
      "step 3037 loss: 2.732924222946167\n",
      "step 3038 loss: 2.725419521331787\n",
      "step 3039 loss: 2.7265939712524414\n",
      "step 3040 loss: 2.8169257640838623\n",
      "step 3041 loss: 2.915900707244873\n",
      "step 3042 loss: 2.7824764251708984\n",
      "step 3043 loss: 2.7971951961517334\n",
      "step 3044 loss: 2.800365924835205\n",
      "step 3045 loss: 2.743406057357788\n",
      "step 3046 loss: 2.8005950450897217\n",
      "step 3047 loss: 2.8555283546447754\n",
      "step 3048 loss: 2.7026939392089844\n",
      "step 3049 loss: 2.817122459411621\n",
      "step 3050 loss: 2.862750768661499\n",
      "step 3051 loss: 2.811619281768799\n",
      "step 3052 loss: 2.8051486015319824\n",
      "step 3053 loss: 2.762864589691162\n",
      "step 3054 loss: 2.7812814712524414\n",
      "step 3055 loss: 2.804481267929077\n",
      "step 3056 loss: 2.6948888301849365\n",
      "step 3057 loss: 2.7959816455841064\n",
      "step 3058 loss: 2.8355870246887207\n",
      "step 3059 loss: 2.9965391159057617\n",
      "step 3060 loss: 2.8222508430480957\n",
      "step 3061 loss: 2.7812702655792236\n",
      "step 3062 loss: 2.7672812938690186\n",
      "step 3063 loss: 3.006873369216919\n",
      "step 3064 loss: 2.755815267562866\n",
      "step 3065 loss: 2.847527503967285\n",
      "step 3066 loss: 2.8064627647399902\n",
      "step 3067 loss: 2.8727762699127197\n",
      "step 3068 loss: 2.8106155395507812\n",
      "step 3069 loss: 2.7562973499298096\n",
      "step 3070 loss: 2.816925287246704\n",
      "step 3071 loss: 2.73620343208313\n",
      "step 3072 loss: 2.7074317932128906\n",
      "step 3073 loss: 2.8209357261657715\n",
      "step 3074 loss: 2.9148786067962646\n",
      "step 3075 loss: 2.7721810340881348\n",
      "step 3076 loss: 2.7322821617126465\n",
      "step 3077 loss: 2.7232296466827393\n",
      "step 3078 loss: 2.825307846069336\n",
      "step 3079 loss: 2.865571975708008\n",
      "step 3080 loss: 2.734438180923462\n",
      "step 3081 loss: 2.8371853828430176\n",
      "step 3082 loss: 2.8404784202575684\n",
      "step 3083 loss: 2.8500094413757324\n",
      "step 3084 loss: 2.8272805213928223\n",
      "step 3085 loss: 2.858499050140381\n",
      "step 3086 loss: 2.71994948387146\n",
      "step 3087 loss: 2.7524538040161133\n",
      "step 3088 loss: 2.7772104740142822\n",
      "step 3089 loss: 2.8187198638916016\n",
      "step 3090 loss: 2.764220714569092\n",
      "step 3091 loss: 2.7075002193450928\n",
      "step 3092 loss: 2.7473247051239014\n",
      "step 3093 loss: 2.9008045196533203\n",
      "step 3094 loss: 2.7842495441436768\n",
      "step 3095 loss: 2.704101085662842\n",
      "step 3096 loss: 2.7913949489593506\n",
      "step 3097 loss: 2.946279287338257\n",
      "step 3098 loss: 2.779003381729126\n",
      "step 3099 loss: 2.8381075859069824\n",
      "step 3100 loss: 2.748556137084961\n",
      "step 3101 loss: 2.731600522994995\n",
      "step 3102 loss: 2.8053762912750244\n",
      "step 3103 loss: 2.8365402221679688\n",
      "step 3104 loss: 2.671698808670044\n",
      "step 3105 loss: 2.762213945388794\n",
      "step 3106 loss: 2.860673189163208\n",
      "step 3107 loss: 2.751246929168701\n",
      "step 3108 loss: 2.898096799850464\n",
      "step 3109 loss: 2.749342203140259\n",
      "step 3110 loss: 2.7676608562469482\n",
      "step 3111 loss: 2.7770981788635254\n",
      "step 3112 loss: 2.8970634937286377\n",
      "step 3113 loss: 2.730539560317993\n",
      "step 3114 loss: 2.902310371398926\n",
      "step 3115 loss: 2.882211685180664\n",
      "step 3116 loss: 2.7747154235839844\n",
      "step 3117 loss: 2.829211473464966\n",
      "step 3118 loss: 2.8903887271881104\n",
      "step 3119 loss: 2.766129493713379\n",
      "step 3120 loss: 2.860147476196289\n",
      "step 3121 loss: 2.8637468814849854\n",
      "step 3122 loss: 2.8968865871429443\n",
      "step 3123 loss: 2.8285138607025146\n",
      "step 3124 loss: 2.7744505405426025\n",
      "step 3125 loss: 2.757042646408081\n",
      "step 3126 loss: 2.773007392883301\n",
      "step 3127 loss: 2.953606367111206\n",
      "step 3128 loss: 2.9144465923309326\n",
      "step 3129 loss: 2.6088967323303223\n",
      "step 3130 loss: 2.663707971572876\n",
      "step 3131 loss: 2.877912759780884\n",
      "step 3132 loss: 2.8069305419921875\n",
      "step 3133 loss: 2.8227241039276123\n",
      "step 3134 loss: 2.7855706214904785\n",
      "step 3135 loss: 2.754098653793335\n",
      "step 3136 loss: 2.8490054607391357\n",
      "step 3137 loss: 2.789400815963745\n",
      "step 3138 loss: 2.7995781898498535\n",
      "step 3139 loss: 2.7649269104003906\n",
      "step 3140 loss: 2.8660826683044434\n",
      "step 3141 loss: 2.7313132286071777\n",
      "step 3142 loss: 2.8632495403289795\n",
      "step 3143 loss: 2.8196792602539062\n",
      "step 3144 loss: 2.7786781787872314\n",
      "step 3145 loss: 2.7280523777008057\n",
      "step 3146 loss: 2.79011869430542\n",
      "step 3147 loss: 2.7838993072509766\n",
      "step 3148 loss: 2.765873908996582\n",
      "step 3149 loss: 2.9067389965057373\n",
      "step 3150 loss: 2.7026727199554443\n",
      "step 3151 loss: 2.7507998943328857\n",
      "step 3152 loss: 2.6625239849090576\n",
      "step 3153 loss: 2.6818363666534424\n",
      "step 3154 loss: 2.8514485359191895\n",
      "step 3155 loss: 2.7250354290008545\n",
      "step 3156 loss: 2.832301139831543\n",
      "step 3157 loss: 2.764732837677002\n",
      "step 3158 loss: 2.8445045948028564\n",
      "step 3159 loss: 2.818591594696045\n",
      "step 3160 loss: 2.776306629180908\n",
      "step 3161 loss: 2.6855580806732178\n",
      "step 3162 loss: 2.842292070388794\n",
      "step 3163 loss: 2.8829739093780518\n",
      "step 3164 loss: 2.77500319480896\n",
      "step 3165 loss: 2.566124439239502\n",
      "step 3166 loss: 2.7647202014923096\n",
      "step 3167 loss: 2.8767929077148438\n",
      "step 3168 loss: 2.619636058807373\n",
      "step 3169 loss: 2.7546043395996094\n",
      "step 3170 loss: 2.751042604446411\n",
      "step 3171 loss: 2.8072359561920166\n",
      "step 3172 loss: 2.919389486312866\n",
      "step 3173 loss: 2.794578790664673\n",
      "step 3174 loss: 2.7667455673217773\n",
      "step 3175 loss: 2.768059492111206\n",
      "step 3176 loss: 2.706947088241577\n",
      "step 3177 loss: 2.758378267288208\n",
      "step 3178 loss: 2.753437042236328\n",
      "step 3179 loss: 2.858980417251587\n",
      "step 3180 loss: 2.860924005508423\n",
      "step 3181 loss: 2.8139185905456543\n",
      "step 3182 loss: 2.761413812637329\n",
      "step 3183 loss: 2.673412322998047\n",
      "step 3184 loss: 2.857736349105835\n",
      "step 3185 loss: 2.7923712730407715\n",
      "step 3186 loss: 2.8839492797851562\n",
      "step 3187 loss: 2.730894088745117\n",
      "step 3188 loss: 2.7005341053009033\n",
      "step 3189 loss: 2.6602845191955566\n",
      "step 3190 loss: 2.7573347091674805\n",
      "step 3191 loss: 2.82279896736145\n",
      "step 3192 loss: 2.7096107006073\n",
      "step 3193 loss: 2.769840717315674\n",
      "step 3194 loss: 2.634749174118042\n",
      "step 3195 loss: 2.8527700901031494\n",
      "step 3196 loss: 2.7736473083496094\n",
      "step 3197 loss: 2.6628873348236084\n",
      "step 3198 loss: 2.854886293411255\n",
      "step 3199 loss: 2.7712931632995605\n",
      "step 3200 loss: 2.687368392944336\n",
      "step 3201 loss: 2.727930784225464\n",
      "step 3202 loss: 2.755666971206665\n",
      "step 3203 loss: 2.7466938495635986\n",
      "step 3204 loss: 2.811821222305298\n",
      "step 3205 loss: 2.6347572803497314\n",
      "step 3206 loss: 2.7347073554992676\n",
      "step 3207 loss: 2.7288260459899902\n",
      "step 3208 loss: 2.6580872535705566\n",
      "step 3209 loss: 2.7681634426116943\n",
      "step 3210 loss: 2.868626117706299\n",
      "step 3211 loss: 2.7204949855804443\n",
      "step 3212 loss: 2.7573862075805664\n",
      "step 3213 loss: 2.6677675247192383\n",
      "step 3214 loss: 2.6906824111938477\n",
      "step 3215 loss: 2.8481879234313965\n",
      "step 3216 loss: 2.7776834964752197\n",
      "step 3217 loss: 2.6204066276550293\n",
      "step 3218 loss: 2.716891288757324\n",
      "step 3219 loss: 2.797010660171509\n",
      "step 3220 loss: 2.724135637283325\n",
      "step 3221 loss: 2.7236735820770264\n",
      "step 3222 loss: 2.7762084007263184\n",
      "step 3223 loss: 2.6837925910949707\n",
      "step 3224 loss: 2.6856112480163574\n",
      "step 3225 loss: 2.725145101547241\n",
      "step 3226 loss: 2.7947185039520264\n",
      "step 3227 loss: 2.794113874435425\n",
      "step 3228 loss: 2.679119110107422\n",
      "step 3229 loss: 2.7625412940979004\n",
      "step 3230 loss: 2.7301626205444336\n",
      "step 3231 loss: 2.728349447250366\n",
      "step 3232 loss: 2.669936418533325\n",
      "step 3233 loss: 2.6638264656066895\n",
      "step 3234 loss: 2.729644536972046\n",
      "step 3235 loss: 2.764136791229248\n",
      "step 3236 loss: 2.7992570400238037\n",
      "step 3237 loss: 2.759354591369629\n",
      "step 3238 loss: 2.8263370990753174\n",
      "step 3239 loss: 2.6686599254608154\n",
      "step 3240 loss: 2.8805580139160156\n",
      "step 3241 loss: 2.812178134918213\n",
      "step 3242 loss: 2.771514654159546\n",
      "step 3243 loss: 2.791306734085083\n",
      "step 3244 loss: 2.8033483028411865\n",
      "step 3245 loss: 2.7162861824035645\n",
      "step 3246 loss: 2.754105567932129\n",
      "step 3247 loss: 2.8204503059387207\n",
      "step 3248 loss: 2.840548515319824\n",
      "step 3249 loss: 2.651270866394043\n",
      "step 3250 loss: 2.6834397315979004\n",
      "step 3251 loss: 2.799877405166626\n",
      "step 3252 loss: 2.6444690227508545\n",
      "step 3253 loss: 2.7947983741760254\n",
      "step 3254 loss: 2.754484176635742\n",
      "step 3255 loss: 2.796147108078003\n",
      "step 3256 loss: 2.7432138919830322\n",
      "step 3257 loss: 2.720815420150757\n",
      "step 3258 loss: 2.685600757598877\n",
      "step 3259 loss: 2.740600109100342\n",
      "step 3260 loss: 2.702270984649658\n",
      "step 3261 loss: 2.711266040802002\n",
      "step 3262 loss: 2.6648662090301514\n",
      "step 3263 loss: 2.8229761123657227\n",
      "step 3264 loss: 2.8286705017089844\n",
      "step 3265 loss: 2.7079200744628906\n",
      "step 3266 loss: 2.6888835430145264\n",
      "step 3267 loss: 2.6101653575897217\n",
      "step 3268 loss: 2.654026508331299\n",
      "step 3269 loss: 2.735675573348999\n",
      "step 3270 loss: 2.7768499851226807\n",
      "step 3271 loss: 2.6222808361053467\n",
      "step 3272 loss: 2.798978805541992\n",
      "step 3273 loss: 2.811953067779541\n",
      "step 3274 loss: 2.7519617080688477\n",
      "step 3275 loss: 2.655744791030884\n",
      "step 3276 loss: 2.7178401947021484\n",
      "step 3277 loss: 2.773315191268921\n",
      "step 3278 loss: 2.823218584060669\n",
      "step 3279 loss: 2.7118749618530273\n",
      "step 3280 loss: 2.81923508644104\n",
      "step 3281 loss: 2.79971981048584\n",
      "step 3282 loss: 2.628917932510376\n",
      "step 3283 loss: 2.710526943206787\n",
      "step 3284 loss: 2.774540424346924\n",
      "step 3285 loss: 2.71333384513855\n",
      "step 3286 loss: 2.836557626724243\n",
      "step 3287 loss: 2.7321836948394775\n",
      "step 3288 loss: 2.634934186935425\n",
      "step 3289 loss: 2.719271421432495\n",
      "step 3290 loss: 2.70597505569458\n",
      "step 3291 loss: 2.845144748687744\n",
      "step 3292 loss: 2.793958902359009\n",
      "step 3293 loss: 2.7854197025299072\n",
      "step 3294 loss: 2.6753218173980713\n",
      "step 3295 loss: 2.736374855041504\n",
      "step 3296 loss: 2.765766143798828\n",
      "step 3297 loss: 2.702637195587158\n",
      "step 3298 loss: 2.6270716190338135\n",
      "step 3299 loss: 2.820556879043579\n",
      "step 3300 loss: 2.682086944580078\n",
      "step 3301 loss: 2.6481640338897705\n",
      "step 3302 loss: 2.7787656784057617\n",
      "step 3303 loss: 2.799853801727295\n",
      "step 3304 loss: 2.851804733276367\n",
      "step 3305 loss: 2.7019600868225098\n",
      "step 3306 loss: 2.769946813583374\n",
      "step 3307 loss: 2.7057077884674072\n",
      "step 3308 loss: 2.6774895191192627\n",
      "step 3309 loss: 2.697019100189209\n",
      "step 3310 loss: 2.6501917839050293\n",
      "step 3311 loss: 2.7080671787261963\n",
      "step 3312 loss: 2.8062784671783447\n",
      "step 3313 loss: 2.6626245975494385\n",
      "step 3314 loss: 2.727511167526245\n",
      "step 3315 loss: 2.8125505447387695\n",
      "step 3316 loss: 2.718134641647339\n",
      "step 3317 loss: 2.7363200187683105\n",
      "step 3318 loss: 2.8845341205596924\n",
      "step 3319 loss: 2.73028826713562\n",
      "step 3320 loss: 2.7007381916046143\n",
      "step 3321 loss: 2.662208318710327\n",
      "step 3322 loss: 2.6426029205322266\n",
      "step 3323 loss: 2.732747793197632\n",
      "step 3324 loss: 2.7203500270843506\n",
      "step 3325 loss: 2.745800495147705\n",
      "step 3326 loss: 2.7554709911346436\n",
      "step 3327 loss: 2.7856292724609375\n",
      "step 3328 loss: 2.7245326042175293\n",
      "step 3329 loss: 2.6538889408111572\n",
      "step 3330 loss: 2.677867889404297\n",
      "step 3331 loss: 2.706878900527954\n",
      "step 3332 loss: 2.57057523727417\n",
      "step 3333 loss: 2.715057373046875\n",
      "step 3334 loss: 2.6435697078704834\n",
      "step 3335 loss: 2.771031618118286\n",
      "step 3336 loss: 2.718587875366211\n",
      "step 3337 loss: 2.6792120933532715\n",
      "step 3338 loss: 2.7508668899536133\n",
      "step 3339 loss: 2.8661773204803467\n",
      "step 3340 loss: 2.8621840476989746\n",
      "step 3341 loss: 2.6445353031158447\n",
      "step 3342 loss: 2.7331061363220215\n",
      "step 3343 loss: 2.613715410232544\n",
      "step 3344 loss: 2.748621940612793\n",
      "step 3345 loss: 2.7043826580047607\n",
      "step 3346 loss: 2.647005319595337\n",
      "step 3347 loss: 2.772977590560913\n",
      "step 3348 loss: 2.6736843585968018\n",
      "step 3349 loss: 2.6691768169403076\n",
      "step 3350 loss: 2.6384854316711426\n",
      "step 3351 loss: 2.861961603164673\n",
      "step 3352 loss: 2.7180752754211426\n",
      "step 3353 loss: 2.7793030738830566\n",
      "step 3354 loss: 2.785487174987793\n",
      "step 3355 loss: 2.8776161670684814\n",
      "step 3356 loss: 2.602649450302124\n",
      "step 3357 loss: 2.7195816040039062\n",
      "step 3358 loss: 2.695411205291748\n",
      "step 3359 loss: 2.6844377517700195\n",
      "step 3360 loss: 2.7364284992218018\n",
      "step 3361 loss: 2.745276689529419\n",
      "step 3362 loss: 2.744083881378174\n",
      "step 3363 loss: 2.718785047531128\n",
      "step 3364 loss: 2.6357438564300537\n",
      "step 3365 loss: 2.7446136474609375\n",
      "step 3366 loss: 2.737117290496826\n",
      "step 3367 loss: 2.7694201469421387\n",
      "step 3368 loss: 2.790065288543701\n",
      "step 3369 loss: 2.754713535308838\n",
      "step 3370 loss: 2.7286250591278076\n",
      "step 3371 loss: 2.6565394401550293\n",
      "step 3372 loss: 2.73347806930542\n",
      "step 3373 loss: 2.687697172164917\n",
      "step 3374 loss: 2.774784564971924\n",
      "step 3375 loss: 2.6524789333343506\n",
      "step 3376 loss: 2.7418625354766846\n",
      "step 3377 loss: 2.7448220252990723\n",
      "step 3378 loss: 2.7784676551818848\n",
      "step 3379 loss: 2.671450138092041\n",
      "step 3380 loss: 2.784026622772217\n",
      "step 3381 loss: 2.716195821762085\n",
      "step 3382 loss: 2.6947011947631836\n",
      "step 3383 loss: 2.66465163230896\n",
      "step 3384 loss: 2.708275318145752\n",
      "step 3385 loss: 2.758256196975708\n",
      "step 3386 loss: 2.774334192276001\n",
      "step 3387 loss: 2.532196044921875\n",
      "step 3388 loss: 2.6884260177612305\n",
      "step 3389 loss: 2.647268533706665\n",
      "step 3390 loss: 2.7020576000213623\n",
      "step 3391 loss: 2.698268413543701\n",
      "step 3392 loss: 2.6916143894195557\n",
      "step 3393 loss: 2.690021276473999\n",
      "step 3394 loss: 2.6200573444366455\n",
      "step 3395 loss: 2.780477285385132\n",
      "step 3396 loss: 2.6223175525665283\n",
      "step 3397 loss: 2.7526865005493164\n",
      "step 3398 loss: 2.8207168579101562\n",
      "step 3399 loss: 2.670393943786621\n",
      "step 3400 loss: 2.688863754272461\n",
      "step 3401 loss: 2.636408805847168\n",
      "step 3402 loss: 2.7632572650909424\n",
      "step 3403 loss: 2.8703248500823975\n",
      "step 3404 loss: 2.7334728240966797\n",
      "step 3405 loss: 2.7736356258392334\n",
      "step 3406 loss: 2.7264466285705566\n",
      "step 3407 loss: 2.695269823074341\n",
      "step 3408 loss: 2.804912805557251\n",
      "step 3409 loss: 2.7568109035491943\n",
      "step 3410 loss: 2.69506573677063\n",
      "step 3411 loss: 2.66827130317688\n",
      "step 3412 loss: 2.6966969966888428\n",
      "step 3413 loss: 2.8178837299346924\n",
      "step 3414 loss: 2.7063798904418945\n",
      "step 3415 loss: 2.76840877532959\n",
      "step 3416 loss: 2.680898666381836\n",
      "step 3417 loss: 2.8172123432159424\n",
      "step 3418 loss: 2.7227654457092285\n",
      "step 3419 loss: 2.7011990547180176\n",
      "step 3420 loss: 2.8423874378204346\n",
      "step 3421 loss: 2.733912944793701\n",
      "step 3422 loss: 2.68310546875\n",
      "step 3423 loss: 2.789476156234741\n",
      "step 3424 loss: 2.6978983879089355\n",
      "step 3425 loss: 2.654158115386963\n",
      "step 3426 loss: 2.7000250816345215\n",
      "step 3427 loss: 2.760357141494751\n",
      "step 3428 loss: 2.59895396232605\n",
      "step 3429 loss: 2.7310643196105957\n",
      "step 3430 loss: 2.796389579772949\n",
      "step 3431 loss: 2.7816755771636963\n",
      "step 3432 loss: 2.7316994667053223\n",
      "step 3433 loss: 2.8184566497802734\n",
      "step 3434 loss: 2.7962076663970947\n",
      "step 3435 loss: 2.6461544036865234\n",
      "step 3436 loss: 2.6637561321258545\n",
      "step 3437 loss: 2.696779727935791\n",
      "step 3438 loss: 2.7186639308929443\n",
      "step 3439 loss: 2.7936854362487793\n",
      "step 3440 loss: 2.8420748710632324\n",
      "step 3441 loss: 2.730609178543091\n",
      "step 3442 loss: 2.672877788543701\n",
      "step 3443 loss: 2.7298636436462402\n",
      "step 3444 loss: 2.726234197616577\n",
      "step 3445 loss: 2.8694751262664795\n",
      "step 3446 loss: 2.724191188812256\n",
      "step 3447 loss: 2.6393420696258545\n",
      "step 3448 loss: 2.710298776626587\n",
      "step 3449 loss: 2.7860267162323\n",
      "step 3450 loss: 2.541853427886963\n",
      "step 3451 loss: 2.7254600524902344\n",
      "step 3452 loss: 2.781022071838379\n",
      "step 3453 loss: 2.6076712608337402\n",
      "step 3454 loss: 2.6991822719573975\n",
      "step 3455 loss: 2.649412155151367\n",
      "step 3456 loss: 2.8866641521453857\n",
      "step 3457 loss: 2.6901729106903076\n",
      "step 3458 loss: 2.7957117557525635\n",
      "step 3459 loss: 2.7125134468078613\n",
      "step 3460 loss: 2.6835620403289795\n",
      "step 3461 loss: 2.740687608718872\n",
      "step 3462 loss: 2.637840986251831\n",
      "step 3463 loss: 2.6318929195404053\n",
      "step 3464 loss: 2.735142469406128\n",
      "step 3465 loss: 2.7351861000061035\n",
      "step 3466 loss: 2.787094831466675\n",
      "step 3467 loss: 2.8058230876922607\n",
      "step 3468 loss: 2.793905258178711\n",
      "step 3469 loss: 2.697587490081787\n",
      "step 3470 loss: 2.749450206756592\n",
      "step 3471 loss: 2.694021463394165\n",
      "step 3472 loss: 2.7214407920837402\n",
      "step 3473 loss: 2.7861135005950928\n",
      "step 3474 loss: 2.7329258918762207\n",
      "step 3475 loss: 2.660896062850952\n",
      "step 3476 loss: 2.551302194595337\n",
      "step 3477 loss: 2.672344923019409\n",
      "step 3478 loss: 2.650261878967285\n",
      "step 3479 loss: 2.6766726970672607\n",
      "step 3480 loss: 2.8519983291625977\n",
      "step 3481 loss: 2.6941909790039062\n",
      "step 3482 loss: 2.7141294479370117\n",
      "step 3483 loss: 2.667062520980835\n",
      "step 3484 loss: 2.6415517330169678\n",
      "step 3485 loss: 2.7427897453308105\n",
      "step 3486 loss: 2.6818082332611084\n",
      "step 3487 loss: 2.7203826904296875\n",
      "step 3488 loss: 2.6782779693603516\n",
      "step 3489 loss: 2.705399513244629\n",
      "step 3490 loss: 2.7488503456115723\n",
      "step 3491 loss: 2.679291248321533\n",
      "step 3492 loss: 2.788851022720337\n",
      "step 3493 loss: 2.644280433654785\n",
      "step 3494 loss: 2.746213912963867\n",
      "step 3495 loss: 2.68424654006958\n",
      "step 3496 loss: 2.7009377479553223\n",
      "step 3497 loss: 2.6877710819244385\n",
      "step 3498 loss: 2.674812078475952\n",
      "step 3499 loss: 2.6506221294403076\n",
      "step 3500 loss: 2.809856414794922\n",
      "step 3501 loss: 2.7272391319274902\n",
      "step 3502 loss: 2.6598141193389893\n",
      "step 3503 loss: 2.675028085708618\n",
      "step 3504 loss: 2.6075007915496826\n",
      "step 3505 loss: 2.704876184463501\n",
      "step 3506 loss: 2.7456037998199463\n",
      "step 3507 loss: 2.7145771980285645\n",
      "step 3508 loss: 2.677678108215332\n",
      "step 3509 loss: 2.622129201889038\n",
      "step 3510 loss: 2.745438814163208\n",
      "step 3511 loss: 2.810029983520508\n",
      "step 3512 loss: 2.7319319248199463\n",
      "step 3513 loss: 2.622814416885376\n",
      "step 3514 loss: 2.715475559234619\n",
      "step 3515 loss: 2.5458550453186035\n",
      "step 3516 loss: 2.8072166442871094\n",
      "step 3517 loss: 2.6998097896575928\n",
      "step 3518 loss: 2.6598961353302\n",
      "step 3519 loss: 2.681271553039551\n",
      "step 3520 loss: 2.6966288089752197\n",
      "step 3521 loss: 2.7311346530914307\n",
      "step 3522 loss: 2.5747640132904053\n",
      "step 3523 loss: 2.686523675918579\n",
      "step 3524 loss: 2.6695637702941895\n",
      "step 3525 loss: 2.7093703746795654\n",
      "step 3526 loss: 2.6642420291900635\n",
      "step 3527 loss: 2.7363085746765137\n",
      "step 3528 loss: 2.6981377601623535\n",
      "step 3529 loss: 2.7283012866973877\n",
      "step 3530 loss: 2.6287994384765625\n",
      "step 3531 loss: 2.620082139968872\n",
      "step 3532 loss: 2.7458853721618652\n",
      "step 3533 loss: 2.717076301574707\n",
      "step 3534 loss: 2.754579782485962\n",
      "step 3535 loss: 2.8079371452331543\n",
      "step 3536 loss: 2.654066801071167\n",
      "step 3537 loss: 2.7309587001800537\n",
      "step 3538 loss: 2.6952805519104004\n",
      "step 3539 loss: 2.6882681846618652\n",
      "step 3540 loss: 2.6721067428588867\n",
      "step 3541 loss: 2.667346954345703\n",
      "step 3542 loss: 2.6947414875030518\n",
      "step 3543 loss: 2.746249198913574\n",
      "step 3544 loss: 2.6552603244781494\n",
      "step 3545 loss: 2.695348024368286\n",
      "step 3546 loss: 2.7225875854492188\n",
      "step 3547 loss: 2.6552071571350098\n",
      "step 3548 loss: 2.6763107776641846\n",
      "step 3549 loss: 2.7581372261047363\n",
      "step 3550 loss: 2.6029300689697266\n",
      "step 3551 loss: 2.650193452835083\n",
      "step 3552 loss: 2.614001989364624\n",
      "step 3553 loss: 2.7373046875\n",
      "step 3554 loss: 2.6719725131988525\n",
      "step 3555 loss: 2.760491132736206\n",
      "step 3556 loss: 2.7105963230133057\n",
      "step 3557 loss: 2.6194076538085938\n",
      "step 3558 loss: 2.823261022567749\n",
      "step 3559 loss: 2.6925246715545654\n",
      "step 3560 loss: 2.6960537433624268\n",
      "step 3561 loss: 2.6596226692199707\n",
      "step 3562 loss: 2.7244677543640137\n",
      "step 3563 loss: 2.696687936782837\n",
      "step 3564 loss: 2.7490382194519043\n",
      "step 3565 loss: 2.699634313583374\n",
      "step 3566 loss: 2.7522692680358887\n",
      "step 3567 loss: 2.704214572906494\n",
      "step 3568 loss: 2.7014694213867188\n",
      "step 3569 loss: 2.6447653770446777\n",
      "step 3570 loss: 2.642185688018799\n",
      "step 3571 loss: 2.789485216140747\n",
      "step 3572 loss: 2.669065475463867\n",
      "step 3573 loss: 2.783966064453125\n",
      "step 3574 loss: 2.6041319370269775\n",
      "step 3575 loss: 2.5134823322296143\n",
      "step 3576 loss: 2.7260642051696777\n",
      "step 3577 loss: 2.653641939163208\n",
      "step 3578 loss: 2.6830050945281982\n",
      "step 3579 loss: 2.718841075897217\n",
      "step 3580 loss: 2.7280707359313965\n",
      "step 3581 loss: 2.6994166374206543\n",
      "step 3582 loss: 2.6236143112182617\n",
      "step 3583 loss: 2.6821937561035156\n",
      "step 3584 loss: 2.6172056198120117\n",
      "step 3585 loss: 2.6351890563964844\n",
      "step 3586 loss: 2.725940704345703\n",
      "step 3587 loss: 2.5589029788970947\n",
      "step 3588 loss: 2.651073455810547\n",
      "step 3589 loss: 2.579458236694336\n",
      "step 3590 loss: 2.756143093109131\n",
      "step 3591 loss: 2.6450634002685547\n",
      "step 3592 loss: 2.721508026123047\n",
      "step 3593 loss: 2.674375534057617\n",
      "step 3594 loss: 2.6595170497894287\n",
      "step 3595 loss: 2.775451183319092\n",
      "step 3596 loss: 2.6079907417297363\n",
      "step 3597 loss: 2.8257155418395996\n",
      "step 3598 loss: 2.7056143283843994\n",
      "step 3599 loss: 2.6972496509552\n",
      "step 3600 loss: 2.6931402683258057\n",
      "step 3601 loss: 2.671539068222046\n",
      "step 3602 loss: 2.6852736473083496\n",
      "step 3603 loss: 2.5747435092926025\n",
      "step 3604 loss: 2.696645975112915\n",
      "step 3605 loss: 2.727264642715454\n",
      "step 3606 loss: 2.7524373531341553\n",
      "step 3607 loss: 2.5498950481414795\n",
      "step 3608 loss: 2.694986581802368\n",
      "step 3609 loss: 2.721348524093628\n",
      "step 3610 loss: 2.606199026107788\n",
      "step 3611 loss: 2.7147879600524902\n",
      "step 3612 loss: 2.55615234375\n",
      "step 3613 loss: 2.7558205127716064\n",
      "step 3614 loss: 2.6419336795806885\n",
      "step 3615 loss: 2.60117506980896\n",
      "step 3616 loss: 2.6949260234832764\n",
      "step 3617 loss: 2.5850040912628174\n",
      "step 3618 loss: 2.6092708110809326\n",
      "step 3619 loss: 2.6269655227661133\n",
      "step 3620 loss: 2.6805624961853027\n",
      "step 3621 loss: 2.761660099029541\n",
      "step 3622 loss: 2.694805860519409\n",
      "step 3623 loss: 2.8960607051849365\n",
      "step 3624 loss: 2.714505910873413\n",
      "step 3625 loss: 2.6776938438415527\n",
      "step 3626 loss: 2.6365108489990234\n",
      "step 3627 loss: 2.6980154514312744\n",
      "step 3628 loss: 2.7210090160369873\n",
      "step 3629 loss: 2.77824068069458\n",
      "step 3630 loss: 2.719304084777832\n",
      "step 3631 loss: 2.618929147720337\n",
      "step 3632 loss: 2.7111458778381348\n",
      "step 3633 loss: 2.598607063293457\n",
      "step 3634 loss: 2.6253347396850586\n",
      "step 3635 loss: 2.7229530811309814\n",
      "step 3636 loss: 2.764321804046631\n",
      "step 3637 loss: 2.6281440258026123\n",
      "step 3638 loss: 2.694204568862915\n",
      "step 3639 loss: 2.6617727279663086\n",
      "step 3640 loss: 2.66516375541687\n",
      "step 3641 loss: 2.640627861022949\n",
      "step 3642 loss: 2.640486478805542\n",
      "step 3643 loss: 2.5566840171813965\n",
      "step 3644 loss: 2.584463596343994\n",
      "step 3645 loss: 2.721041202545166\n",
      "step 3646 loss: 2.597050905227661\n",
      "step 3647 loss: 2.7949442863464355\n",
      "step 3648 loss: 2.6451973915100098\n",
      "step 3649 loss: 2.6643433570861816\n",
      "step 3650 loss: 2.5831139087677\n",
      "step 3651 loss: 2.6304614543914795\n",
      "step 3652 loss: 2.624716281890869\n",
      "step 3653 loss: 2.7041618824005127\n",
      "step 3654 loss: 2.722944974899292\n",
      "step 3655 loss: 2.7877047061920166\n",
      "step 3656 loss: 2.704657793045044\n",
      "step 3657 loss: 2.6598024368286133\n",
      "step 3658 loss: 2.6280932426452637\n",
      "step 3659 loss: 2.670835256576538\n",
      "step 3660 loss: 2.618328809738159\n",
      "step 3661 loss: 2.7072105407714844\n",
      "step 3662 loss: 2.723996162414551\n",
      "step 3663 loss: 2.730196475982666\n",
      "step 3664 loss: 2.729419469833374\n",
      "step 3665 loss: 2.7419114112854004\n",
      "step 3666 loss: 2.72656512260437\n",
      "step 3667 loss: 2.747053384780884\n",
      "step 3668 loss: 2.5918734073638916\n",
      "step 3669 loss: 2.7086167335510254\n",
      "step 3670 loss: 2.6666061878204346\n",
      "step 3671 loss: 2.612182855606079\n",
      "step 3672 loss: 2.7594451904296875\n",
      "step 3673 loss: 2.709449291229248\n",
      "step 3674 loss: 2.690547227859497\n",
      "step 3675 loss: 2.6851730346679688\n",
      "step 3676 loss: 2.6547956466674805\n",
      "step 3677 loss: 2.7514445781707764\n",
      "step 3678 loss: 2.6361727714538574\n",
      "step 3679 loss: 2.5833146572113037\n",
      "step 3680 loss: 2.6619021892547607\n",
      "step 3681 loss: 2.6909804344177246\n",
      "step 3682 loss: 2.7940032482147217\n",
      "step 3683 loss: 2.6213700771331787\n",
      "step 3684 loss: 2.7517433166503906\n",
      "step 3685 loss: 2.60414457321167\n",
      "step 3686 loss: 2.576078176498413\n",
      "step 3687 loss: 2.6979856491088867\n",
      "step 3688 loss: 2.6741957664489746\n",
      "step 3689 loss: 2.802119731903076\n",
      "step 3690 loss: 2.6337273120880127\n",
      "step 3691 loss: 2.7618799209594727\n",
      "step 3692 loss: 2.649020195007324\n",
      "step 3693 loss: 2.6336891651153564\n",
      "step 3694 loss: 2.780062437057495\n",
      "step 3695 loss: 2.6135776042938232\n",
      "step 3696 loss: 2.5540573596954346\n",
      "step 3697 loss: 2.5987770557403564\n",
      "step 3698 loss: 2.619537115097046\n",
      "step 3699 loss: 2.552879810333252\n",
      "step 3700 loss: 2.665353298187256\n",
      "step 3701 loss: 2.726170778274536\n",
      "step 3702 loss: 2.775071144104004\n",
      "step 3703 loss: 2.528510808944702\n",
      "step 3704 loss: 2.9202067852020264\n",
      "step 3705 loss: 2.7367875576019287\n",
      "step 3706 loss: 2.762007236480713\n",
      "step 3707 loss: 2.578035593032837\n",
      "step 3708 loss: 2.6643245220184326\n",
      "step 3709 loss: 2.8410592079162598\n",
      "step 3710 loss: 2.7180848121643066\n",
      "step 3711 loss: 2.7201249599456787\n",
      "step 3712 loss: 2.632688283920288\n",
      "step 3713 loss: 2.7388498783111572\n",
      "step 3714 loss: 2.6419131755828857\n",
      "step 3715 loss: 2.645207166671753\n",
      "step 3716 loss: 2.7816336154937744\n",
      "step 3717 loss: 2.6096954345703125\n",
      "step 3718 loss: 2.687356472015381\n",
      "step 3719 loss: 2.57763409614563\n",
      "step 3720 loss: 2.73215651512146\n",
      "step 3721 loss: 2.6670618057250977\n",
      "step 3722 loss: 2.6861212253570557\n",
      "step 3723 loss: 2.639863967895508\n",
      "step 3724 loss: 2.7556495666503906\n",
      "step 3725 loss: 2.762399196624756\n",
      "step 3726 loss: 2.7252376079559326\n",
      "step 3727 loss: 2.7581732273101807\n",
      "step 3728 loss: 2.6807138919830322\n",
      "step 3729 loss: 2.6488707065582275\n",
      "step 3730 loss: 2.666083574295044\n",
      "step 3731 loss: 2.7102997303009033\n",
      "step 3732 loss: 2.6256332397460938\n",
      "step 3733 loss: 2.734875202178955\n",
      "step 3734 loss: 2.6276278495788574\n",
      "step 3735 loss: 2.771690607070923\n",
      "step 3736 loss: 2.703200101852417\n",
      "step 3737 loss: 2.729438304901123\n",
      "step 3738 loss: 2.747368574142456\n",
      "step 3739 loss: 2.691554307937622\n",
      "step 3740 loss: 2.7035715579986572\n",
      "step 3741 loss: 2.783114194869995\n",
      "step 3742 loss: 2.541649103164673\n",
      "step 3743 loss: 2.583263397216797\n",
      "step 3744 loss: 2.6156957149505615\n",
      "step 3745 loss: 2.639112710952759\n",
      "step 3746 loss: 2.6514523029327393\n",
      "step 3747 loss: 2.607391119003296\n",
      "step 3748 loss: 2.7823688983917236\n",
      "step 3749 loss: 2.570401430130005\n",
      "step 3750 loss: 2.6578738689422607\n",
      "step 3751 loss: 2.6665945053100586\n",
      "step 3752 loss: 2.740995168685913\n",
      "step 3753 loss: 2.793236017227173\n",
      "step 3754 loss: 2.6881000995635986\n",
      "step 3755 loss: 2.714906692504883\n",
      "step 3756 loss: 2.582040548324585\n",
      "step 3757 loss: 2.6073451042175293\n",
      "step 3758 loss: 2.6497180461883545\n",
      "step 3759 loss: 2.576780080795288\n",
      "step 3760 loss: 2.7032923698425293\n",
      "step 3761 loss: 2.715909957885742\n",
      "step 3762 loss: 2.5970990657806396\n",
      "step 3763 loss: 2.750129461288452\n",
      "step 3764 loss: 2.657907247543335\n",
      "step 3765 loss: 2.643928289413452\n",
      "step 3766 loss: 2.6734976768493652\n",
      "step 3767 loss: 2.698819875717163\n",
      "step 3768 loss: 2.7028305530548096\n",
      "step 3769 loss: 2.6608550548553467\n",
      "step 3770 loss: 2.8456218242645264\n",
      "step 3771 loss: 2.630922317504883\n",
      "step 3772 loss: 2.580859422683716\n",
      "step 3773 loss: 2.662036180496216\n",
      "step 3774 loss: 2.5664522647857666\n",
      "step 3775 loss: 2.6631524562835693\n",
      "step 3776 loss: 2.6151020526885986\n",
      "step 3777 loss: 2.84615159034729\n",
      "step 3778 loss: 2.602447271347046\n",
      "step 3779 loss: 2.579634428024292\n",
      "step 3780 loss: 2.569744825363159\n",
      "step 3781 loss: 2.7180051803588867\n",
      "step 3782 loss: 2.7192063331604004\n",
      "step 3783 loss: 2.635106325149536\n",
      "step 3784 loss: 2.6788508892059326\n",
      "step 3785 loss: 2.705327033996582\n",
      "step 3786 loss: 2.5756607055664062\n",
      "step 3787 loss: 2.6213550567626953\n",
      "step 3788 loss: 2.7625906467437744\n",
      "step 3789 loss: 2.6994900703430176\n",
      "step 3790 loss: 2.6361827850341797\n",
      "step 3791 loss: 2.699455738067627\n",
      "step 3792 loss: 2.5825047492980957\n",
      "step 3793 loss: 2.6107637882232666\n",
      "step 3794 loss: 2.6859447956085205\n",
      "step 3795 loss: 2.6395983695983887\n",
      "step 3796 loss: 2.825120687484741\n",
      "step 3797 loss: 2.713621139526367\n",
      "step 3798 loss: 2.689379930496216\n",
      "step 3799 loss: 2.6235201358795166\n",
      "step 3800 loss: 2.632939100265503\n",
      "step 3801 loss: 2.614190101623535\n",
      "step 3802 loss: 2.59185791015625\n",
      "step 3803 loss: 2.67077374458313\n",
      "step 3804 loss: 2.705657720565796\n",
      "step 3805 loss: 2.6291213035583496\n",
      "step 3806 loss: 2.581254243850708\n",
      "step 3807 loss: 2.646740198135376\n",
      "step 3808 loss: 2.6827540397644043\n",
      "step 3809 loss: 2.6706485748291016\n",
      "step 3810 loss: 2.6233155727386475\n",
      "step 3811 loss: 2.5521061420440674\n",
      "step 3812 loss: 2.6771738529205322\n",
      "step 3813 loss: 2.6330387592315674\n",
      "step 3814 loss: 2.6538445949554443\n",
      "step 3815 loss: 2.7392144203186035\n",
      "step 3816 loss: 2.7613954544067383\n",
      "step 3817 loss: 2.5698280334472656\n",
      "step 3818 loss: 2.6347715854644775\n",
      "step 3819 loss: 2.7456624507904053\n",
      "step 3820 loss: 2.709117889404297\n",
      "step 3821 loss: 2.700360059738159\n",
      "step 3822 loss: 2.7057151794433594\n",
      "step 3823 loss: 2.686966896057129\n",
      "step 3824 loss: 2.738612651824951\n",
      "step 3825 loss: 2.597468376159668\n",
      "step 3826 loss: 2.7037577629089355\n",
      "step 3827 loss: 2.642944574356079\n",
      "step 3828 loss: 2.669468402862549\n",
      "step 3829 loss: 2.591055393218994\n",
      "step 3830 loss: 2.7659029960632324\n",
      "step 3831 loss: 2.7513113021850586\n",
      "step 3832 loss: 2.6438753604888916\n",
      "step 3833 loss: 2.6567301750183105\n",
      "step 3834 loss: 2.6210319995880127\n",
      "step 3835 loss: 2.6995456218719482\n",
      "step 3836 loss: 2.712528944015503\n",
      "step 3837 loss: 2.5814104080200195\n",
      "step 3838 loss: 2.684107780456543\n",
      "step 3839 loss: 2.5865046977996826\n",
      "step 3840 loss: 2.638639450073242\n",
      "step 3841 loss: 2.632556676864624\n",
      "step 3842 loss: 2.645669460296631\n",
      "step 3843 loss: 2.7156894207000732\n",
      "step 3844 loss: 2.683288335800171\n",
      "step 3845 loss: 2.606384754180908\n",
      "step 3846 loss: 2.7084176540374756\n",
      "step 3847 loss: 2.646941661834717\n",
      "step 3848 loss: 2.7122106552124023\n",
      "step 3849 loss: 2.6808266639709473\n",
      "step 3850 loss: 2.6340527534484863\n",
      "step 3851 loss: 2.605217933654785\n",
      "step 3852 loss: 2.6643002033233643\n",
      "step 3853 loss: 2.575493335723877\n",
      "step 3854 loss: 2.6965529918670654\n",
      "step 3855 loss: 2.646923065185547\n",
      "step 3856 loss: 2.672104597091675\n",
      "step 3857 loss: 2.603076219558716\n",
      "step 3858 loss: 2.5971028804779053\n",
      "step 3859 loss: 2.681140422821045\n",
      "step 3860 loss: 2.6315088272094727\n",
      "step 3861 loss: 2.519648551940918\n",
      "step 3862 loss: 2.668382167816162\n",
      "step 3863 loss: 2.5698189735412598\n",
      "step 3864 loss: 2.593365430831909\n",
      "step 3865 loss: 2.7394566535949707\n",
      "step 3866 loss: 2.643754482269287\n",
      "step 3867 loss: 2.661288022994995\n",
      "step 3868 loss: 2.5645828247070312\n",
      "step 3869 loss: 2.6114213466644287\n",
      "step 3870 loss: 2.6316394805908203\n",
      "step 3871 loss: 2.719269275665283\n",
      "step 3872 loss: 2.6832773685455322\n",
      "step 3873 loss: 2.6175537109375\n",
      "step 3874 loss: 2.6700940132141113\n",
      "step 3875 loss: 2.561182737350464\n",
      "step 3876 loss: 2.5560476779937744\n",
      "step 3877 loss: 2.6348471641540527\n",
      "step 3878 loss: 2.6131553649902344\n",
      "step 3879 loss: 2.6151862144470215\n",
      "step 3880 loss: 2.5491766929626465\n",
      "step 3881 loss: 2.550300359725952\n",
      "step 3882 loss: 2.7074997425079346\n",
      "step 3883 loss: 2.6314010620117188\n",
      "step 3884 loss: 2.6962075233459473\n",
      "step 3885 loss: 2.673445701599121\n",
      "step 3886 loss: 2.6531004905700684\n",
      "step 3887 loss: 2.6999027729034424\n",
      "step 3888 loss: 2.656514883041382\n",
      "step 3889 loss: 2.7088074684143066\n",
      "step 3890 loss: 2.671044111251831\n",
      "step 3891 loss: 2.615431547164917\n",
      "step 3892 loss: 2.667560338973999\n",
      "step 3893 loss: 2.686617374420166\n",
      "step 3894 loss: 2.674560308456421\n",
      "step 3895 loss: 2.758896827697754\n",
      "step 3896 loss: 2.5509889125823975\n",
      "step 3897 loss: 2.5835821628570557\n",
      "step 3898 loss: 2.7133188247680664\n",
      "step 3899 loss: 2.63517427444458\n",
      "step 3900 loss: 2.75382924079895\n",
      "step 3901 loss: 2.7692630290985107\n",
      "step 3902 loss: 2.6581387519836426\n",
      "step 3903 loss: 2.636716842651367\n",
      "step 3904 loss: 2.623192310333252\n",
      "step 3905 loss: 2.681223154067993\n",
      "step 3906 loss: 2.580641508102417\n",
      "step 3907 loss: 2.7401692867279053\n",
      "step 3908 loss: 2.6514382362365723\n",
      "step 3909 loss: 2.7543601989746094\n",
      "step 3910 loss: 2.5229742527008057\n",
      "step 3911 loss: 2.6155943870544434\n",
      "step 3912 loss: 2.6481871604919434\n",
      "step 3913 loss: 2.6726884841918945\n",
      "step 3914 loss: 2.6097300052642822\n",
      "step 3915 loss: 2.5628154277801514\n",
      "step 3916 loss: 2.714460849761963\n",
      "step 3917 loss: 2.5796396732330322\n",
      "step 3918 loss: 2.651491165161133\n",
      "step 3919 loss: 2.5599026679992676\n",
      "step 3920 loss: 2.5532875061035156\n",
      "step 3921 loss: 2.5835092067718506\n",
      "step 3922 loss: 2.773402214050293\n",
      "step 3923 loss: 2.6284289360046387\n",
      "step 3924 loss: 2.629234790802002\n",
      "step 3925 loss: 2.703032970428467\n",
      "step 3926 loss: 2.5708069801330566\n",
      "step 3927 loss: 2.6158981323242188\n",
      "step 3928 loss: 2.6590933799743652\n",
      "step 3929 loss: 2.537846803665161\n",
      "step 3930 loss: 2.65643048286438\n",
      "step 3931 loss: 2.721156120300293\n",
      "step 3932 loss: 2.626757860183716\n",
      "step 3933 loss: 2.725148916244507\n",
      "step 3934 loss: 2.58585262298584\n",
      "step 3935 loss: 2.7043862342834473\n",
      "step 3936 loss: 2.5653772354125977\n",
      "step 3937 loss: 2.6683504581451416\n",
      "step 3938 loss: 2.641706705093384\n",
      "step 3939 loss: 2.687371015548706\n",
      "step 3940 loss: 2.525561809539795\n",
      "step 3941 loss: 2.644813299179077\n",
      "step 3942 loss: 2.675337076187134\n",
      "step 3943 loss: 2.5653536319732666\n",
      "step 3944 loss: 2.6057896614074707\n",
      "step 3945 loss: 2.7832672595977783\n",
      "step 3946 loss: 2.6950721740722656\n",
      "step 3947 loss: 2.7356555461883545\n",
      "step 3948 loss: 2.5334224700927734\n",
      "step 3949 loss: 2.669935464859009\n",
      "step 3950 loss: 2.5573410987854004\n",
      "step 3951 loss: 2.6069931983947754\n",
      "step 3952 loss: 2.5957624912261963\n",
      "step 3953 loss: 2.6344616413116455\n",
      "step 3954 loss: 2.5891642570495605\n",
      "step 3955 loss: 2.732403516769409\n",
      "step 3956 loss: 2.6752843856811523\n",
      "step 3957 loss: 2.710049629211426\n",
      "step 3958 loss: 2.555391311645508\n",
      "step 3959 loss: 2.5963144302368164\n",
      "step 3960 loss: 2.6464579105377197\n",
      "step 3961 loss: 2.7370002269744873\n",
      "step 3962 loss: 2.6613550186157227\n",
      "step 3963 loss: 2.6437196731567383\n",
      "step 3964 loss: 2.604527473449707\n",
      "step 3965 loss: 2.615370035171509\n",
      "step 3966 loss: 2.6966452598571777\n",
      "step 3967 loss: 2.6414921283721924\n",
      "step 3968 loss: 2.6577353477478027\n",
      "step 3969 loss: 2.6568613052368164\n",
      "step 3970 loss: 2.5808911323547363\n",
      "step 3971 loss: 2.586636543273926\n",
      "step 3972 loss: 2.7067694664001465\n",
      "step 3973 loss: 2.7432098388671875\n",
      "step 3974 loss: 2.778154134750366\n",
      "step 3975 loss: 2.6029469966888428\n",
      "step 3976 loss: 2.673065423965454\n",
      "step 3977 loss: 2.552788496017456\n",
      "step 3978 loss: 2.679055690765381\n",
      "step 3979 loss: 2.634350299835205\n",
      "step 3980 loss: 2.6716907024383545\n",
      "step 3981 loss: 2.6303184032440186\n",
      "step 3982 loss: 2.608915090560913\n",
      "step 3983 loss: 2.793679714202881\n",
      "step 3984 loss: 2.700284004211426\n",
      "step 3985 loss: 2.658517837524414\n",
      "step 3986 loss: 2.6391823291778564\n",
      "step 3987 loss: 2.620478868484497\n",
      "step 3988 loss: 2.5501372814178467\n",
      "step 3989 loss: 2.606010913848877\n",
      "step 3990 loss: 2.711052656173706\n",
      "step 3991 loss: 2.649447441101074\n",
      "step 3992 loss: 2.6112639904022217\n",
      "step 3993 loss: 2.666966438293457\n",
      "step 3994 loss: 2.618504524230957\n",
      "step 3995 loss: 2.7377238273620605\n",
      "step 3996 loss: 2.7398648262023926\n",
      "step 3997 loss: 2.519378423690796\n",
      "step 3998 loss: 2.6193108558654785\n",
      "step 3999 loss: 2.5059468746185303\n",
      "step 4000 loss: 2.5844571590423584\n",
      "step 4001 loss: 2.6468827724456787\n",
      "step 4002 loss: 2.64310359954834\n",
      "step 4003 loss: 2.6786437034606934\n",
      "step 4004 loss: 2.719311237335205\n",
      "step 4005 loss: 2.5602762699127197\n",
      "step 4006 loss: 2.829744338989258\n",
      "step 4007 loss: 2.6758689880371094\n",
      "step 4008 loss: 2.6496615409851074\n",
      "step 4009 loss: 2.5498292446136475\n",
      "step 4010 loss: 2.5598807334899902\n",
      "step 4011 loss: 2.6392860412597656\n",
      "step 4012 loss: 2.5275166034698486\n",
      "step 4013 loss: 2.550534963607788\n",
      "step 4014 loss: 2.5527682304382324\n",
      "step 4015 loss: 2.707853078842163\n",
      "step 4016 loss: 2.555673599243164\n",
      "step 4017 loss: 2.6992557048797607\n",
      "step 4018 loss: 2.4700253009796143\n",
      "step 4019 loss: 2.549886465072632\n",
      "step 4020 loss: 2.6913952827453613\n",
      "step 4021 loss: 2.589508295059204\n",
      "step 4022 loss: 2.560864210128784\n",
      "step 4023 loss: 2.520761013031006\n",
      "step 4024 loss: 2.6594650745391846\n",
      "step 4025 loss: 2.713038921356201\n",
      "step 4026 loss: 2.6534602642059326\n",
      "step 4027 loss: 2.5643603801727295\n",
      "step 4028 loss: 2.6851613521575928\n",
      "step 4029 loss: 2.7582626342773438\n",
      "step 4030 loss: 2.6211769580841064\n",
      "step 4031 loss: 2.5852251052856445\n",
      "step 4032 loss: 2.5614073276519775\n",
      "step 4033 loss: 2.633314609527588\n",
      "step 4034 loss: 2.56819224357605\n",
      "step 4035 loss: 2.5180165767669678\n",
      "step 4036 loss: 2.5467731952667236\n",
      "step 4037 loss: 2.460005283355713\n",
      "step 4038 loss: 2.8091676235198975\n",
      "step 4039 loss: 2.63657283782959\n",
      "step 4040 loss: 2.672598123550415\n",
      "step 4041 loss: 2.565871238708496\n",
      "step 4042 loss: 2.778313398361206\n",
      "step 4043 loss: 2.454099655151367\n",
      "step 4044 loss: 2.628509759902954\n",
      "step 4045 loss: 2.6823325157165527\n",
      "step 4046 loss: 2.754723310470581\n",
      "step 4047 loss: 2.534231424331665\n",
      "step 4048 loss: 2.500929594039917\n",
      "step 4049 loss: 2.5655312538146973\n",
      "step 4050 loss: 2.5656468868255615\n",
      "step 4051 loss: 2.7047841548919678\n",
      "step 4052 loss: 2.65681529045105\n",
      "step 4053 loss: 2.747065544128418\n",
      "step 4054 loss: 2.760014295578003\n",
      "step 4055 loss: 2.470371961593628\n",
      "step 4056 loss: 2.6794698238372803\n",
      "step 4057 loss: 2.7024435997009277\n",
      "step 4058 loss: 2.697427749633789\n",
      "step 4059 loss: 2.62520170211792\n",
      "step 4060 loss: 2.785540819168091\n",
      "step 4061 loss: 2.7302985191345215\n",
      "step 4062 loss: 2.5109078884124756\n",
      "step 4063 loss: 2.6298398971557617\n",
      "step 4064 loss: 2.7125937938690186\n",
      "step 4065 loss: 2.6347928047180176\n",
      "step 4066 loss: 2.615344762802124\n",
      "step 4067 loss: 2.7180817127227783\n",
      "step 4068 loss: 2.639188051223755\n",
      "step 4069 loss: 2.586116313934326\n",
      "step 4070 loss: 2.530186891555786\n",
      "step 4071 loss: 2.5580227375030518\n",
      "step 4072 loss: 2.584721803665161\n",
      "step 4073 loss: 2.61217999458313\n",
      "step 4074 loss: 2.5328311920166016\n",
      "step 4075 loss: 2.6588869094848633\n",
      "step 4076 loss: 2.661104679107666\n",
      "step 4077 loss: 2.608902931213379\n",
      "step 4078 loss: 2.605123281478882\n",
      "step 4079 loss: 2.757857084274292\n",
      "step 4080 loss: 2.6274423599243164\n",
      "step 4081 loss: 2.6075589656829834\n",
      "step 4082 loss: 2.6943511962890625\n",
      "step 4083 loss: 2.6303367614746094\n",
      "step 4084 loss: 2.681685209274292\n",
      "step 4085 loss: 2.6772613525390625\n",
      "step 4086 loss: 2.669116258621216\n",
      "step 4087 loss: 2.620870590209961\n",
      "step 4088 loss: 2.6486504077911377\n",
      "step 4089 loss: 2.569021224975586\n",
      "step 4090 loss: 2.584231376647949\n",
      "step 4091 loss: 2.556724786758423\n",
      "step 4092 loss: 2.5397439002990723\n",
      "step 4093 loss: 2.595344066619873\n",
      "step 4094 loss: 2.6103289127349854\n",
      "step 4095 loss: 2.6347601413726807\n",
      "step 4096 loss: 2.652567148208618\n",
      "step 4097 loss: 2.673750638961792\n",
      "step 4098 loss: 2.5718162059783936\n",
      "step 4099 loss: 2.7120282649993896\n",
      "step 4100 loss: 2.630505323410034\n",
      "step 4101 loss: 2.6108338832855225\n",
      "step 4102 loss: 2.6335129737854004\n",
      "step 4103 loss: 2.5499536991119385\n",
      "step 4104 loss: 2.4979395866394043\n",
      "step 4105 loss: 2.611212968826294\n",
      "step 4106 loss: 2.497812271118164\n",
      "step 4107 loss: 2.622662305831909\n",
      "step 4108 loss: 2.717092514038086\n",
      "step 4109 loss: 2.649721622467041\n",
      "step 4110 loss: 2.593477487564087\n",
      "step 4111 loss: 2.5857033729553223\n",
      "step 4112 loss: 2.6848020553588867\n",
      "step 4113 loss: 2.6658995151519775\n",
      "step 4114 loss: 2.6839916706085205\n",
      "step 4115 loss: 2.6858224868774414\n",
      "step 4116 loss: 2.536156177520752\n",
      "step 4117 loss: 2.5516443252563477\n",
      "step 4118 loss: 2.6616671085357666\n",
      "step 4119 loss: 2.5731616020202637\n",
      "step 4120 loss: 2.6238324642181396\n",
      "step 4121 loss: 2.6293070316314697\n",
      "step 4122 loss: 2.591546058654785\n",
      "step 4123 loss: 2.508345365524292\n",
      "step 4124 loss: 2.6722965240478516\n",
      "step 4125 loss: 2.64589786529541\n",
      "step 4126 loss: 2.71337890625\n",
      "step 4127 loss: 2.660332679748535\n",
      "step 4128 loss: 2.625962734222412\n",
      "step 4129 loss: 2.543837547302246\n",
      "step 4130 loss: 2.5914058685302734\n",
      "step 4131 loss: 2.5352258682250977\n",
      "step 4132 loss: 2.6006906032562256\n",
      "step 4133 loss: 2.5936036109924316\n",
      "step 4134 loss: 2.61737322807312\n",
      "step 4135 loss: 2.578760862350464\n",
      "step 4136 loss: 2.604623794555664\n",
      "step 4137 loss: 2.704911231994629\n",
      "step 4138 loss: 2.610234260559082\n",
      "step 4139 loss: 2.4487156867980957\n",
      "step 4140 loss: 2.527155876159668\n",
      "step 4141 loss: 2.6657726764678955\n",
      "step 4142 loss: 2.660557508468628\n",
      "step 4143 loss: 2.5949530601501465\n",
      "step 4144 loss: 2.655585765838623\n",
      "step 4145 loss: 2.6929149627685547\n",
      "step 4146 loss: 2.634390354156494\n",
      "step 4147 loss: 2.544224739074707\n",
      "step 4148 loss: 2.576491594314575\n",
      "step 4149 loss: 2.698443651199341\n",
      "step 4150 loss: 2.6596531867980957\n",
      "step 4151 loss: 2.6356217861175537\n",
      "step 4152 loss: 2.5722153186798096\n",
      "step 4153 loss: 2.426802158355713\n",
      "step 4154 loss: 2.544753313064575\n",
      "step 4155 loss: 2.578361988067627\n",
      "step 4156 loss: 2.7678613662719727\n",
      "step 4157 loss: 2.441220283508301\n",
      "step 4158 loss: 2.588256359100342\n",
      "step 4159 loss: 2.6190013885498047\n",
      "step 4160 loss: 2.592287063598633\n",
      "step 4161 loss: 2.6651041507720947\n",
      "step 4162 loss: 2.5811541080474854\n",
      "step 4163 loss: 2.698486089706421\n",
      "step 4164 loss: 2.598879337310791\n",
      "step 4165 loss: 2.5382027626037598\n",
      "step 4166 loss: 2.6753945350646973\n",
      "step 4167 loss: 2.618600368499756\n",
      "step 4168 loss: 2.609084129333496\n",
      "step 4169 loss: 2.707369804382324\n",
      "step 4170 loss: 2.582693099975586\n",
      "step 4171 loss: 2.65122652053833\n",
      "step 4172 loss: 2.736114740371704\n",
      "step 4173 loss: 2.5732743740081787\n",
      "step 4174 loss: 2.519120216369629\n",
      "step 4175 loss: 2.7197582721710205\n",
      "step 4176 loss: 2.454740285873413\n",
      "step 4177 loss: 2.526453733444214\n",
      "step 4178 loss: 2.62625789642334\n",
      "step 4179 loss: 2.615809917449951\n",
      "step 4180 loss: 2.664611577987671\n",
      "step 4181 loss: 2.6496613025665283\n",
      "step 4182 loss: 2.671238899230957\n",
      "step 4183 loss: 2.58439040184021\n",
      "step 4184 loss: 2.5260684490203857\n",
      "step 4185 loss: 2.5672500133514404\n",
      "step 4186 loss: 2.6509134769439697\n",
      "step 4187 loss: 2.5027096271514893\n",
      "step 4188 loss: 2.597027063369751\n",
      "step 4189 loss: 2.614210605621338\n",
      "step 4190 loss: 2.624499559402466\n",
      "step 4191 loss: 2.6064305305480957\n",
      "step 4192 loss: 2.6113359928131104\n",
      "step 4193 loss: 2.676933526992798\n",
      "step 4194 loss: 2.516554594039917\n",
      "step 4195 loss: 2.511934995651245\n",
      "step 4196 loss: 2.5304737091064453\n",
      "step 4197 loss: 2.62204647064209\n",
      "step 4198 loss: 2.553536891937256\n",
      "step 4199 loss: 2.5871779918670654\n",
      "step 4200 loss: 2.6259851455688477\n",
      "step 4201 loss: 2.6793994903564453\n",
      "step 4202 loss: 2.675579309463501\n",
      "step 4203 loss: 2.65813946723938\n",
      "step 4204 loss: 2.509845018386841\n",
      "step 4205 loss: 2.572981119155884\n",
      "step 4206 loss: 2.4984447956085205\n",
      "step 4207 loss: 2.517155885696411\n",
      "step 4208 loss: 2.5959763526916504\n",
      "step 4209 loss: 2.74927020072937\n",
      "step 4210 loss: 2.7974283695220947\n",
      "step 4211 loss: 2.617738723754883\n",
      "step 4212 loss: 2.678337335586548\n",
      "step 4213 loss: 2.6973190307617188\n",
      "step 4214 loss: 2.6038200855255127\n",
      "step 4215 loss: 2.477376699447632\n",
      "step 4216 loss: 2.6526665687561035\n",
      "step 4217 loss: 2.6756591796875\n",
      "step 4218 loss: 2.694261312484741\n",
      "step 4219 loss: 2.5147135257720947\n",
      "step 4220 loss: 2.4933619499206543\n",
      "step 4221 loss: 2.374894618988037\n",
      "step 4222 loss: 2.710284948348999\n",
      "step 4223 loss: 2.622127056121826\n",
      "step 4224 loss: 2.6431736946105957\n",
      "step 4225 loss: 2.562818765640259\n",
      "step 4226 loss: 2.6353676319122314\n",
      "step 4227 loss: 2.6801795959472656\n",
      "step 4228 loss: 2.475938081741333\n",
      "step 4229 loss: 2.7048394680023193\n",
      "step 4230 loss: 2.673429250717163\n",
      "step 4231 loss: 2.6128785610198975\n",
      "step 4232 loss: 2.5856730937957764\n",
      "step 4233 loss: 2.6743991374969482\n",
      "step 4234 loss: 2.7125697135925293\n",
      "step 4235 loss: 2.5010533332824707\n",
      "step 4236 loss: 2.6868600845336914\n",
      "step 4237 loss: 2.5342066287994385\n",
      "step 4238 loss: 2.6283652782440186\n",
      "step 4239 loss: 2.5496017932891846\n",
      "step 4240 loss: 2.5070865154266357\n",
      "step 4241 loss: 2.506834030151367\n",
      "step 4242 loss: 2.61250638961792\n",
      "step 4243 loss: 2.5928521156311035\n",
      "step 4244 loss: 2.5645499229431152\n",
      "step 4245 loss: 2.602433919906616\n",
      "step 4246 loss: 2.571864128112793\n",
      "step 4247 loss: 2.7150237560272217\n",
      "step 4248 loss: 2.5797343254089355\n",
      "step 4249 loss: 2.5166988372802734\n",
      "step 4250 loss: 2.582642078399658\n",
      "step 4251 loss: 2.6171202659606934\n",
      "step 4252 loss: 2.7323226928710938\n",
      "step 4253 loss: 2.57220196723938\n",
      "step 4254 loss: 2.671872138977051\n",
      "step 4255 loss: 2.5997002124786377\n",
      "step 4256 loss: 2.6493828296661377\n",
      "step 4257 loss: 2.4963431358337402\n",
      "step 4258 loss: 2.668039321899414\n",
      "step 4259 loss: 2.538787603378296\n",
      "step 4260 loss: 2.639608144760132\n",
      "step 4261 loss: 2.591761589050293\n",
      "step 4262 loss: 2.6811327934265137\n",
      "step 4263 loss: 2.572874069213867\n",
      "step 4264 loss: 2.625229835510254\n",
      "step 4265 loss: 2.7276360988616943\n",
      "step 4266 loss: 2.6953272819519043\n",
      "step 4267 loss: 2.665168523788452\n",
      "step 4268 loss: 2.5039398670196533\n",
      "step 4269 loss: 2.5873706340789795\n",
      "step 4270 loss: 2.513227701187134\n",
      "step 4271 loss: 2.545966148376465\n",
      "step 4272 loss: 2.622878313064575\n",
      "step 4273 loss: 2.642200469970703\n",
      "step 4274 loss: 2.543508529663086\n",
      "step 4275 loss: 2.63673734664917\n",
      "step 4276 loss: 2.61134672164917\n",
      "step 4277 loss: 2.5287888050079346\n",
      "step 4278 loss: 2.623117208480835\n",
      "step 4279 loss: 2.5972907543182373\n",
      "step 4280 loss: 2.6746246814727783\n",
      "step 4281 loss: 2.701011896133423\n",
      "step 4282 loss: 2.6534273624420166\n",
      "step 4283 loss: 2.6724555492401123\n",
      "step 4284 loss: 2.531046152114868\n",
      "step 4285 loss: 2.616626262664795\n",
      "step 4286 loss: 2.5937297344207764\n",
      "step 4287 loss: 2.5985100269317627\n",
      "step 4288 loss: 2.5933451652526855\n",
      "step 4289 loss: 2.650202751159668\n",
      "step 4290 loss: 2.5968739986419678\n",
      "step 4291 loss: 2.645050287246704\n",
      "step 4292 loss: 2.681670665740967\n",
      "step 4293 loss: 2.6356403827667236\n",
      "step 4294 loss: 2.663670063018799\n",
      "step 4295 loss: 2.562044382095337\n",
      "step 4296 loss: 2.6750035285949707\n",
      "step 4297 loss: 2.5838375091552734\n",
      "step 4298 loss: 2.6128227710723877\n",
      "step 4299 loss: 2.6595211029052734\n",
      "step 4300 loss: 2.5507774353027344\n",
      "step 4301 loss: 2.6454267501831055\n",
      "step 4302 loss: 2.6476199626922607\n",
      "step 4303 loss: 2.6283481121063232\n",
      "step 4304 loss: 2.4771475791931152\n",
      "step 4305 loss: 2.510087490081787\n",
      "step 4306 loss: 2.687187671661377\n",
      "step 4307 loss: 2.6124203205108643\n",
      "step 4308 loss: 2.687333345413208\n",
      "step 4309 loss: 2.58492374420166\n",
      "step 4310 loss: 2.6888506412506104\n",
      "step 4311 loss: 2.6206679344177246\n",
      "step 4312 loss: 2.5834271907806396\n",
      "step 4313 loss: 2.569145917892456\n",
      "step 4314 loss: 2.694852113723755\n",
      "step 4315 loss: 2.6804935932159424\n",
      "step 4316 loss: 2.627410411834717\n",
      "step 4317 loss: 2.6510956287384033\n",
      "step 4318 loss: 2.6232221126556396\n",
      "step 4319 loss: 2.6253788471221924\n",
      "step 4320 loss: 2.6212596893310547\n",
      "step 4321 loss: 2.648510217666626\n",
      "step 4322 loss: 2.637143850326538\n",
      "step 4323 loss: 2.7184531688690186\n",
      "step 4324 loss: 2.5515921115875244\n",
      "step 4325 loss: 2.728198528289795\n",
      "step 4326 loss: 2.7307322025299072\n",
      "step 4327 loss: 2.5826799869537354\n",
      "step 4328 loss: 2.603682041168213\n",
      "step 4329 loss: 2.5734970569610596\n",
      "step 4330 loss: 2.58709454536438\n",
      "step 4331 loss: 2.6013004779815674\n",
      "step 4332 loss: 2.6252310276031494\n",
      "step 4333 loss: 2.605685234069824\n",
      "step 4334 loss: 2.5722131729125977\n",
      "step 4335 loss: 2.576463460922241\n",
      "step 4336 loss: 2.66611909866333\n",
      "step 4337 loss: 2.6076550483703613\n",
      "step 4338 loss: 2.502412796020508\n",
      "step 4339 loss: 2.6776626110076904\n",
      "step 4340 loss: 2.6092143058776855\n",
      "step 4341 loss: 2.517498016357422\n",
      "step 4342 loss: 2.629395008087158\n",
      "step 4343 loss: 2.613781452178955\n",
      "step 4344 loss: 2.6122028827667236\n",
      "step 4345 loss: 2.63376784324646\n",
      "step 4346 loss: 2.6449027061462402\n",
      "step 4347 loss: 2.535583734512329\n",
      "step 4348 loss: 2.672398328781128\n",
      "step 4349 loss: 2.641725778579712\n",
      "step 4350 loss: 2.495124578475952\n",
      "step 4351 loss: 2.5959107875823975\n",
      "step 4352 loss: 2.6337192058563232\n",
      "step 4353 loss: 2.65729022026062\n",
      "step 4354 loss: 2.5855705738067627\n",
      "step 4355 loss: 2.5384421348571777\n",
      "step 4356 loss: 2.6234967708587646\n",
      "step 4357 loss: 2.6969664096832275\n",
      "step 4358 loss: 2.5534729957580566\n",
      "step 4359 loss: 2.5528693199157715\n",
      "step 4360 loss: 2.579864501953125\n",
      "step 4361 loss: 2.603269100189209\n",
      "step 4362 loss: 2.678224802017212\n",
      "step 4363 loss: 2.5752761363983154\n",
      "step 4364 loss: 2.518752336502075\n",
      "step 4365 loss: 2.532090425491333\n",
      "step 4366 loss: 2.623401165008545\n",
      "step 4367 loss: 2.5891404151916504\n",
      "step 4368 loss: 2.5371339321136475\n",
      "step 4369 loss: 2.5472946166992188\n",
      "step 4370 loss: 2.610260248184204\n",
      "step 4371 loss: 2.6392805576324463\n",
      "step 4372 loss: 2.614236354827881\n",
      "step 4373 loss: 2.5682060718536377\n",
      "step 4374 loss: 2.6393253803253174\n",
      "step 4375 loss: 2.5125372409820557\n",
      "step 4376 loss: 2.6982460021972656\n",
      "step 4377 loss: 2.5533177852630615\n",
      "step 4378 loss: 2.70129656791687\n",
      "step 4379 loss: 2.7009048461914062\n",
      "step 4380 loss: 2.6000595092773438\n",
      "step 4381 loss: 2.5580735206604004\n",
      "step 4382 loss: 2.6213910579681396\n",
      "step 4383 loss: 2.744643449783325\n",
      "step 4384 loss: 2.5228748321533203\n",
      "step 4385 loss: 2.57755970954895\n",
      "step 4386 loss: 2.5411629676818848\n",
      "step 4387 loss: 2.523564577102661\n",
      "step 4388 loss: 2.65972638130188\n",
      "step 4389 loss: 2.5801475048065186\n",
      "step 4390 loss: 2.6152796745300293\n",
      "step 4391 loss: 2.6271862983703613\n",
      "step 4392 loss: 2.556442975997925\n",
      "step 4393 loss: 2.639455556869507\n",
      "step 4394 loss: 2.5472359657287598\n",
      "step 4395 loss: 2.530017375946045\n",
      "step 4396 loss: 2.5722286701202393\n",
      "step 4397 loss: 2.6843981742858887\n",
      "step 4398 loss: 2.5362446308135986\n",
      "step 4399 loss: 2.557795286178589\n",
      "step 4400 loss: 2.5834596157073975\n",
      "step 4401 loss: 2.7270662784576416\n",
      "step 4402 loss: 2.530200958251953\n",
      "step 4403 loss: 2.714026689529419\n",
      "step 4404 loss: 2.5452065467834473\n",
      "step 4405 loss: 2.570974349975586\n",
      "step 4406 loss: 2.5370540618896484\n",
      "step 4407 loss: 2.501274585723877\n",
      "step 4408 loss: 2.611917495727539\n",
      "step 4409 loss: 2.720486640930176\n",
      "step 4410 loss: 2.5439698696136475\n",
      "step 4411 loss: 2.5574615001678467\n",
      "step 4412 loss: 2.545079469680786\n",
      "step 4413 loss: 2.579699993133545\n",
      "step 4414 loss: 2.6619927883148193\n",
      "step 4415 loss: 2.567824125289917\n",
      "step 4416 loss: 2.575897216796875\n",
      "step 4417 loss: 2.6134796142578125\n",
      "step 4418 loss: 2.6513888835906982\n",
      "step 4419 loss: 2.58636474609375\n",
      "step 4420 loss: 2.5930871963500977\n",
      "step 4421 loss: 2.6741247177124023\n",
      "step 4422 loss: 2.4874794483184814\n",
      "step 4423 loss: 2.597369432449341\n",
      "step 4424 loss: 2.552255630493164\n",
      "step 4425 loss: 2.5177242755889893\n",
      "step 4426 loss: 2.5733940601348877\n",
      "step 4427 loss: 2.5454142093658447\n",
      "step 4428 loss: 2.5448880195617676\n",
      "step 4429 loss: 2.5756051540374756\n",
      "step 4430 loss: 2.5636019706726074\n",
      "step 4431 loss: 2.6629812717437744\n",
      "step 4432 loss: 2.636422634124756\n",
      "step 4433 loss: 2.7708113193511963\n",
      "step 4434 loss: 2.6310060024261475\n",
      "step 4435 loss: 2.5112929344177246\n",
      "step 4436 loss: 2.6718807220458984\n",
      "step 4437 loss: 2.591207981109619\n",
      "step 4438 loss: 2.433696746826172\n",
      "step 4439 loss: 2.7286407947540283\n",
      "step 4440 loss: 2.452733039855957\n",
      "step 4441 loss: 2.6418278217315674\n",
      "step 4442 loss: 2.5618162155151367\n",
      "step 4443 loss: 2.5041615962982178\n",
      "step 4444 loss: 2.7256977558135986\n",
      "step 4445 loss: 2.4841392040252686\n",
      "step 4446 loss: 2.6610782146453857\n",
      "step 4447 loss: 2.6636884212493896\n",
      "step 4448 loss: 2.5088272094726562\n",
      "step 4449 loss: 2.6744089126586914\n",
      "step 4450 loss: 2.4887173175811768\n",
      "step 4451 loss: 2.701939105987549\n",
      "step 4452 loss: 2.617825508117676\n",
      "step 4453 loss: 2.6287319660186768\n",
      "step 4454 loss: 2.574157238006592\n",
      "step 4455 loss: 2.6730804443359375\n",
      "step 4456 loss: 2.5862722396850586\n",
      "step 4457 loss: 2.620210886001587\n",
      "step 4458 loss: 2.4931983947753906\n",
      "step 4459 loss: 2.568713665008545\n",
      "step 4460 loss: 2.6387672424316406\n",
      "step 4461 loss: 2.5360448360443115\n",
      "step 4462 loss: 2.515045166015625\n",
      "step 4463 loss: 2.560227870941162\n",
      "step 4464 loss: 2.698160409927368\n",
      "step 4465 loss: 2.490915060043335\n",
      "step 4466 loss: 2.610391616821289\n",
      "step 4467 loss: 2.502872943878174\n",
      "step 4468 loss: 2.528792142868042\n",
      "step 4469 loss: 2.685328483581543\n",
      "step 4470 loss: 2.525179147720337\n",
      "step 4471 loss: 2.7282066345214844\n",
      "step 4472 loss: 2.538398027420044\n",
      "step 4473 loss: 2.6173200607299805\n",
      "step 4474 loss: 2.564700126647949\n",
      "step 4475 loss: 2.569631576538086\n",
      "step 4476 loss: 2.5658140182495117\n",
      "step 4477 loss: 2.642655849456787\n",
      "step 4478 loss: 2.701140880584717\n",
      "step 4479 loss: 2.6152586936950684\n",
      "step 4480 loss: 2.5960893630981445\n",
      "step 4481 loss: 2.6269185543060303\n",
      "step 4482 loss: 2.630584239959717\n",
      "step 4483 loss: 2.5034337043762207\n",
      "step 4484 loss: 2.622833013534546\n",
      "step 4485 loss: 2.582075357437134\n",
      "step 4486 loss: 2.5458486080169678\n",
      "step 4487 loss: 2.6624813079833984\n",
      "step 4488 loss: 2.611651659011841\n",
      "step 4489 loss: 2.630084991455078\n",
      "step 4490 loss: 2.579868793487549\n",
      "step 4491 loss: 2.552258014678955\n",
      "step 4492 loss: 2.581609010696411\n",
      "step 4493 loss: 2.6741726398468018\n",
      "step 4494 loss: 2.638584613800049\n",
      "step 4495 loss: 2.591432809829712\n",
      "step 4496 loss: 2.6753041744232178\n",
      "step 4497 loss: 2.4678382873535156\n",
      "step 4498 loss: 2.5601062774658203\n",
      "step 4499 loss: 2.617534637451172\n",
      "step 4500 loss: 2.6057393550872803\n",
      "step 4501 loss: 2.63511323928833\n",
      "step 4502 loss: 2.6532390117645264\n",
      "step 4503 loss: 2.6541686058044434\n",
      "step 4504 loss: 2.482180118560791\n",
      "step 4505 loss: 2.6198062896728516\n",
      "step 4506 loss: 2.575716733932495\n",
      "step 4507 loss: 2.5719289779663086\n",
      "step 4508 loss: 2.6346888542175293\n",
      "step 4509 loss: 2.639669418334961\n",
      "step 4510 loss: 2.6212375164031982\n",
      "step 4511 loss: 2.4531214237213135\n",
      "step 4512 loss: 2.6215083599090576\n",
      "step 4513 loss: 2.618255376815796\n",
      "step 4514 loss: 2.5613529682159424\n",
      "step 4515 loss: 2.6449906826019287\n",
      "step 4516 loss: 2.4723873138427734\n",
      "step 4517 loss: 2.5630042552948\n",
      "step 4518 loss: 2.599602460861206\n",
      "step 4519 loss: 2.5941762924194336\n",
      "step 4520 loss: 2.6659862995147705\n",
      "step 4521 loss: 2.702822208404541\n",
      "step 4522 loss: 2.5984046459198\n",
      "step 4523 loss: 2.6591763496398926\n",
      "step 4524 loss: 2.6333279609680176\n",
      "step 4525 loss: 2.632317304611206\n",
      "step 4526 loss: 2.611175060272217\n",
      "step 4527 loss: 2.5839693546295166\n",
      "step 4528 loss: 2.5795934200286865\n",
      "step 4529 loss: 2.547427177429199\n",
      "step 4530 loss: 2.594254493713379\n",
      "step 4531 loss: 2.6188604831695557\n",
      "step 4532 loss: 2.545661449432373\n",
      "step 4533 loss: 2.5644848346710205\n",
      "step 4534 loss: 2.4920289516448975\n",
      "step 4535 loss: 2.6357786655426025\n",
      "step 4536 loss: 2.520069122314453\n",
      "step 4537 loss: 2.606752395629883\n",
      "step 4538 loss: 2.8031184673309326\n",
      "step 4539 loss: 2.469114303588867\n",
      "step 4540 loss: 2.661524772644043\n",
      "step 4541 loss: 2.7321126461029053\n",
      "step 4542 loss: 2.554126024246216\n",
      "step 4543 loss: 2.6176843643188477\n",
      "step 4544 loss: 2.6290745735168457\n",
      "step 4545 loss: 2.471672773361206\n",
      "step 4546 loss: 2.6890459060668945\n",
      "step 4547 loss: 2.530033826828003\n",
      "step 4548 loss: 2.6043224334716797\n",
      "step 4549 loss: 2.6040096282958984\n",
      "step 4550 loss: 2.496635913848877\n",
      "step 4551 loss: 2.589841365814209\n",
      "step 4552 loss: 2.5830695629119873\n",
      "step 4553 loss: 2.5939552783966064\n",
      "step 4554 loss: 2.6186516284942627\n",
      "step 4555 loss: 2.5288143157958984\n",
      "step 4556 loss: 2.557525157928467\n",
      "step 4557 loss: 2.6244585514068604\n",
      "step 4558 loss: 2.78035306930542\n",
      "step 4559 loss: 2.453603506088257\n",
      "step 4560 loss: 2.632441759109497\n",
      "step 4561 loss: 2.5766284465789795\n",
      "step 4562 loss: 2.617150068283081\n",
      "step 4563 loss: 2.5387380123138428\n",
      "step 4564 loss: 2.6110591888427734\n",
      "step 4565 loss: 2.6023099422454834\n",
      "step 4566 loss: 2.474081516265869\n",
      "step 4567 loss: 2.648362874984741\n",
      "step 4568 loss: 2.671509265899658\n",
      "step 4569 loss: 2.5404958724975586\n",
      "step 4570 loss: 2.6572229862213135\n",
      "step 4571 loss: 2.586902379989624\n",
      "step 4572 loss: 2.6059467792510986\n",
      "step 4573 loss: 2.54011607170105\n",
      "step 4574 loss: 2.6020405292510986\n",
      "step 4575 loss: 2.628875494003296\n",
      "step 4576 loss: 2.6164863109588623\n",
      "step 4577 loss: 2.743042469024658\n",
      "step 4578 loss: 2.6387431621551514\n",
      "step 4579 loss: 2.644862174987793\n",
      "step 4580 loss: 2.7163753509521484\n",
      "step 4581 loss: 2.544538736343384\n",
      "step 4582 loss: 2.6543824672698975\n",
      "step 4583 loss: 2.502026319503784\n",
      "step 4584 loss: 2.5867061614990234\n",
      "step 4585 loss: 2.4700958728790283\n",
      "step 4586 loss: 2.5443732738494873\n",
      "step 4587 loss: 2.5283188819885254\n",
      "step 4588 loss: 2.5427510738372803\n",
      "step 4589 loss: 2.6723215579986572\n",
      "step 4590 loss: 2.558835029602051\n",
      "step 4591 loss: 2.53043794631958\n",
      "step 4592 loss: 2.7105538845062256\n",
      "step 4593 loss: 2.570388078689575\n",
      "step 4594 loss: 2.6594817638397217\n",
      "step 4595 loss: 2.4218549728393555\n",
      "step 4596 loss: 2.6038966178894043\n",
      "step 4597 loss: 2.6725008487701416\n",
      "step 4598 loss: 2.5804007053375244\n",
      "step 4599 loss: 2.541897773742676\n",
      "step 4600 loss: 2.6198649406433105\n",
      "step 4601 loss: 2.6387147903442383\n",
      "step 4602 loss: 2.597214698791504\n",
      "step 4603 loss: 2.590684413909912\n",
      "step 4604 loss: 2.6310596466064453\n",
      "step 4605 loss: 2.6131012439727783\n",
      "step 4606 loss: 2.6105520725250244\n",
      "step 4607 loss: 2.568669557571411\n",
      "step 4608 loss: 2.638218402862549\n",
      "step 4609 loss: 2.723642587661743\n",
      "step 4610 loss: 2.521526336669922\n",
      "step 4611 loss: 2.6943845748901367\n",
      "step 4612 loss: 2.5937933921813965\n",
      "step 4613 loss: 2.5792195796966553\n",
      "step 4614 loss: 2.643770456314087\n",
      "step 4615 loss: 2.5915982723236084\n",
      "step 4616 loss: 2.5155177116394043\n",
      "step 4617 loss: 2.640645980834961\n",
      "step 4618 loss: 2.5683786869049072\n",
      "step 4619 loss: 2.560730457305908\n",
      "step 4620 loss: 2.59598970413208\n",
      "step 4621 loss: 2.572495460510254\n",
      "step 4622 loss: 2.6317620277404785\n",
      "step 4623 loss: 2.578714370727539\n",
      "step 4624 loss: 2.684483528137207\n",
      "step 4625 loss: 2.5704426765441895\n",
      "step 4626 loss: 2.512871503829956\n",
      "step 4627 loss: 2.580486297607422\n",
      "step 4628 loss: 2.549807071685791\n",
      "step 4629 loss: 2.6659562587738037\n",
      "step 4630 loss: 2.5608737468719482\n",
      "step 4631 loss: 2.609057903289795\n",
      "step 4632 loss: 2.594193458557129\n",
      "step 4633 loss: 2.5128257274627686\n",
      "step 4634 loss: 2.5644357204437256\n",
      "step 4635 loss: 2.5442161560058594\n",
      "step 4636 loss: 2.6985666751861572\n",
      "step 4637 loss: 2.668473243713379\n",
      "step 4638 loss: 2.48661732673645\n",
      "step 4639 loss: 2.653017282485962\n",
      "step 4640 loss: 2.7375519275665283\n",
      "step 4641 loss: 2.692877769470215\n",
      "step 4642 loss: 2.51774263381958\n",
      "step 4643 loss: 2.6848959922790527\n",
      "step 4644 loss: 2.7191762924194336\n",
      "step 4645 loss: 2.6318063735961914\n",
      "step 4646 loss: 2.482442617416382\n",
      "step 4647 loss: 2.5016775131225586\n",
      "step 4648 loss: 2.6514487266540527\n",
      "step 4649 loss: 2.5642244815826416\n",
      "step 4650 loss: 2.478983163833618\n",
      "step 4651 loss: 2.5398430824279785\n",
      "step 4652 loss: 2.6731200218200684\n",
      "step 4653 loss: 2.603174924850464\n",
      "step 4654 loss: 2.5698890686035156\n",
      "step 4655 loss: 2.6158275604248047\n",
      "step 4656 loss: 2.523693561553955\n",
      "step 4657 loss: 2.5046989917755127\n",
      "step 4658 loss: 2.547015905380249\n",
      "step 4659 loss: 2.523959159851074\n",
      "step 4660 loss: 2.490959405899048\n",
      "step 4661 loss: 2.6691391468048096\n",
      "step 4662 loss: 2.5332159996032715\n",
      "step 4663 loss: 2.5600225925445557\n",
      "step 4664 loss: 2.497230291366577\n",
      "step 4665 loss: 2.4674062728881836\n",
      "step 4666 loss: 2.578505516052246\n",
      "step 4667 loss: 2.5406384468078613\n",
      "step 4668 loss: 2.672577381134033\n",
      "step 4669 loss: 2.5251405239105225\n",
      "step 4670 loss: 2.605792760848999\n",
      "step 4671 loss: 2.63895320892334\n",
      "step 4672 loss: 2.7496516704559326\n",
      "step 4673 loss: 2.5198974609375\n",
      "step 4674 loss: 2.5283868312835693\n",
      "step 4675 loss: 2.6784472465515137\n",
      "step 4676 loss: 2.4812419414520264\n",
      "step 4677 loss: 2.5411925315856934\n",
      "step 4678 loss: 2.7468795776367188\n",
      "step 4679 loss: 2.723159074783325\n",
      "step 4680 loss: 2.5260095596313477\n",
      "step 4681 loss: 2.6005313396453857\n",
      "step 4682 loss: 2.5225841999053955\n",
      "step 4683 loss: 2.4286184310913086\n",
      "step 4684 loss: 2.5915400981903076\n",
      "step 4685 loss: 2.595047950744629\n",
      "step 4686 loss: 2.5907387733459473\n",
      "step 4687 loss: 2.760495662689209\n",
      "step 4688 loss: 2.739567756652832\n",
      "step 4689 loss: 2.5400021076202393\n",
      "step 4690 loss: 2.54061222076416\n",
      "step 4691 loss: 2.703859806060791\n",
      "step 4692 loss: 2.5731358528137207\n",
      "step 4693 loss: 2.5439679622650146\n",
      "step 4694 loss: 2.487994909286499\n",
      "step 4695 loss: 2.6780788898468018\n",
      "step 4696 loss: 2.6092445850372314\n",
      "step 4697 loss: 2.6738078594207764\n",
      "step 4698 loss: 2.5959248542785645\n",
      "step 4699 loss: 2.632328510284424\n",
      "step 4700 loss: 2.5730929374694824\n",
      "step 4701 loss: 2.5373644828796387\n",
      "step 4702 loss: 2.5164802074432373\n",
      "step 4703 loss: 2.6401000022888184\n",
      "step 4704 loss: 2.646658420562744\n",
      "step 4705 loss: 2.5469071865081787\n",
      "step 4706 loss: 2.453068256378174\n",
      "step 4707 loss: 2.503850221633911\n",
      "step 4708 loss: 2.534026861190796\n",
      "step 4709 loss: 2.6749250888824463\n",
      "step 4710 loss: 2.536092519760132\n",
      "step 4711 loss: 2.6132614612579346\n",
      "step 4712 loss: 2.5525968074798584\n",
      "step 4713 loss: 2.6216952800750732\n",
      "step 4714 loss: 2.6806788444519043\n",
      "step 4715 loss: 2.536822557449341\n",
      "step 4716 loss: 2.630903720855713\n",
      "step 4717 loss: 2.6166317462921143\n",
      "step 4718 loss: 2.528437614440918\n",
      "step 4719 loss: 2.6145076751708984\n",
      "step 4720 loss: 2.636096477508545\n",
      "step 4721 loss: 2.601745128631592\n",
      "step 4722 loss: 2.5083110332489014\n",
      "step 4723 loss: 2.5301549434661865\n",
      "step 4724 loss: 2.703489065170288\n",
      "step 4725 loss: 2.61067795753479\n",
      "step 4726 loss: 2.5378220081329346\n",
      "step 4727 loss: 2.5704283714294434\n",
      "step 4728 loss: 2.6858856678009033\n",
      "step 4729 loss: 2.4285426139831543\n",
      "step 4730 loss: 2.460221529006958\n",
      "step 4731 loss: 2.592195749282837\n",
      "step 4732 loss: 2.516153335571289\n",
      "step 4733 loss: 2.5924527645111084\n",
      "step 4734 loss: 2.5497426986694336\n",
      "step 4735 loss: 2.505405902862549\n",
      "step 4736 loss: 2.470055103302002\n",
      "step 4737 loss: 2.62382173538208\n",
      "step 4738 loss: 2.5409982204437256\n",
      "step 4739 loss: 2.5035769939422607\n",
      "step 4740 loss: 2.627487897872925\n",
      "step 4741 loss: 2.591501474380493\n",
      "step 4742 loss: 2.514291763305664\n",
      "step 4743 loss: 2.558349609375\n",
      "step 4744 loss: 2.5238850116729736\n",
      "step 4745 loss: 2.6240077018737793\n",
      "step 4746 loss: 2.6223204135894775\n",
      "step 4747 loss: 2.5850627422332764\n",
      "step 4748 loss: 2.5782551765441895\n",
      "step 4749 loss: 2.716041088104248\n",
      "step 4750 loss: 2.498039960861206\n",
      "step 4751 loss: 2.6774940490722656\n",
      "step 4752 loss: 2.6174724102020264\n",
      "step 4753 loss: 2.438553810119629\n",
      "step 4754 loss: 2.5638179779052734\n",
      "step 4755 loss: 2.506931781768799\n",
      "step 4756 loss: 2.6053802967071533\n",
      "step 4757 loss: 2.7546639442443848\n",
      "step 4758 loss: 2.571697235107422\n",
      "step 4759 loss: 2.6558878421783447\n",
      "step 4760 loss: 2.5702450275421143\n",
      "step 4761 loss: 2.5895822048187256\n",
      "step 4762 loss: 2.584913969039917\n",
      "step 4763 loss: 2.5495834350585938\n",
      "step 4764 loss: 2.4500062465667725\n",
      "step 4765 loss: 2.408931255340576\n",
      "step 4766 loss: 2.474153518676758\n",
      "step 4767 loss: 2.5191268920898438\n",
      "step 4768 loss: 2.506046772003174\n",
      "step 4769 loss: 2.5756163597106934\n",
      "step 4770 loss: 2.526836633682251\n",
      "step 4771 loss: 2.5762815475463867\n",
      "step 4772 loss: 2.574791431427002\n",
      "step 4773 loss: 2.6611030101776123\n",
      "step 4774 loss: 2.5962226390838623\n",
      "step 4775 loss: 2.559418201446533\n",
      "step 4776 loss: 2.593461036682129\n",
      "step 4777 loss: 2.590768814086914\n",
      "step 4778 loss: 2.6157948970794678\n",
      "step 4779 loss: 2.4375104904174805\n",
      "step 4780 loss: 2.5592234134674072\n",
      "step 4781 loss: 2.4070370197296143\n",
      "step 4782 loss: 2.512395143508911\n",
      "step 4783 loss: 2.5493123531341553\n",
      "step 4784 loss: 2.5922024250030518\n",
      "step 4785 loss: 2.545853853225708\n",
      "step 4786 loss: 2.592379331588745\n",
      "step 4787 loss: 2.6038689613342285\n",
      "step 4788 loss: 2.5918962955474854\n",
      "step 4789 loss: 2.530907154083252\n",
      "step 4790 loss: 2.482663631439209\n",
      "step 4791 loss: 2.4949865341186523\n",
      "step 4792 loss: 2.613889217376709\n",
      "step 4793 loss: 2.558881998062134\n",
      "step 4794 loss: 2.6013431549072266\n",
      "step 4795 loss: 2.4096462726593018\n",
      "step 4796 loss: 2.4991581439971924\n",
      "step 4797 loss: 2.4684247970581055\n",
      "step 4798 loss: 2.5249876976013184\n",
      "step 4799 loss: 2.5113985538482666\n",
      "step 4800 loss: 2.5133121013641357\n",
      "step 4801 loss: 2.5324599742889404\n",
      "step 4802 loss: 2.714094400405884\n",
      "step 4803 loss: 2.524174928665161\n",
      "step 4804 loss: 2.6113948822021484\n",
      "step 4805 loss: 2.4687507152557373\n",
      "step 4806 loss: 2.5839216709136963\n",
      "step 4807 loss: 2.6159164905548096\n",
      "step 4808 loss: 2.571721076965332\n",
      "step 4809 loss: 2.6319737434387207\n",
      "step 4810 loss: 2.4293906688690186\n",
      "step 4811 loss: 2.64072585105896\n",
      "step 4812 loss: 2.502399206161499\n",
      "step 4813 loss: 2.599343776702881\n",
      "step 4814 loss: 2.5486133098602295\n",
      "step 4815 loss: 2.638946533203125\n",
      "step 4816 loss: 2.506012201309204\n",
      "step 4817 loss: 2.581725835800171\n",
      "step 4818 loss: 2.606942892074585\n",
      "step 4819 loss: 2.667374610900879\n",
      "step 4820 loss: 2.6295220851898193\n",
      "step 4821 loss: 2.587502956390381\n",
      "step 4822 loss: 2.628932476043701\n",
      "step 4823 loss: 2.5023369789123535\n",
      "step 4824 loss: 2.414081335067749\n",
      "step 4825 loss: 2.6621196269989014\n",
      "step 4826 loss: 2.5928456783294678\n",
      "step 4827 loss: 2.5188305377960205\n",
      "step 4828 loss: 2.503511428833008\n",
      "step 4829 loss: 2.4554436206817627\n",
      "step 4830 loss: 2.443964958190918\n",
      "step 4831 loss: 2.559532642364502\n",
      "step 4832 loss: 2.578946352005005\n",
      "step 4833 loss: 2.5194108486175537\n",
      "step 4834 loss: 2.6590209007263184\n",
      "step 4835 loss: 2.5148539543151855\n",
      "step 4836 loss: 2.517841100692749\n",
      "step 4837 loss: 2.7841243743896484\n",
      "step 4838 loss: 2.5999984741210938\n",
      "step 4839 loss: 2.4744861125946045\n",
      "step 4840 loss: 2.668285608291626\n",
      "step 4841 loss: 2.5589170455932617\n",
      "step 4842 loss: 2.5006484985351562\n",
      "step 4843 loss: 2.6414008140563965\n",
      "step 4844 loss: 2.5708656311035156\n",
      "step 4845 loss: 2.5383927822113037\n",
      "step 4846 loss: 2.425382375717163\n",
      "step 4847 loss: 2.615821361541748\n",
      "step 4848 loss: 2.6028852462768555\n",
      "step 4849 loss: 2.607133388519287\n",
      "step 4850 loss: 2.5173826217651367\n",
      "step 4851 loss: 2.5694873332977295\n",
      "step 4852 loss: 2.589839220046997\n",
      "step 4853 loss: 2.603055238723755\n",
      "step 4854 loss: 2.5933330059051514\n",
      "step 4855 loss: 2.46736741065979\n",
      "step 4856 loss: 2.5997865200042725\n",
      "step 4857 loss: 2.631032705307007\n",
      "step 4858 loss: 2.6282150745391846\n",
      "step 4859 loss: 2.590273141860962\n",
      "step 4860 loss: 2.59189510345459\n",
      "step 4861 loss: 2.5683088302612305\n",
      "step 4862 loss: 2.5618655681610107\n",
      "step 4863 loss: 2.563981771469116\n",
      "step 4864 loss: 2.702889919281006\n",
      "step 4865 loss: 2.5745391845703125\n",
      "step 4866 loss: 2.6348578929901123\n",
      "step 4867 loss: 2.5323586463928223\n",
      "step 4868 loss: 2.6172568798065186\n",
      "step 4869 loss: 2.5514354705810547\n",
      "step 4870 loss: 2.5291500091552734\n",
      "step 4871 loss: 2.561314105987549\n",
      "step 4872 loss: 2.642514944076538\n",
      "step 4873 loss: 2.44539737701416\n",
      "step 4874 loss: 2.473484754562378\n",
      "step 4875 loss: 2.584099054336548\n",
      "step 4876 loss: 2.5210812091827393\n",
      "step 4877 loss: 2.509004592895508\n",
      "step 4878 loss: 2.535658836364746\n",
      "step 4879 loss: 2.557025194168091\n",
      "step 4880 loss: 2.6058709621429443\n",
      "step 4881 loss: 2.417656183242798\n",
      "step 4882 loss: 2.67330002784729\n",
      "step 4883 loss: 2.5199437141418457\n",
      "step 4884 loss: 2.546369791030884\n",
      "step 4885 loss: 2.5039048194885254\n",
      "step 4886 loss: 2.5567800998687744\n",
      "step 4887 loss: 2.5908610820770264\n",
      "step 4888 loss: 2.641878366470337\n",
      "step 4889 loss: 2.630570411682129\n",
      "step 4890 loss: 2.5121898651123047\n",
      "step 4891 loss: 2.609373092651367\n",
      "step 4892 loss: 2.675210952758789\n",
      "step 4893 loss: 2.5848820209503174\n",
      "step 4894 loss: 2.5798609256744385\n",
      "step 4895 loss: 2.510448932647705\n",
      "step 4896 loss: 2.6556670665740967\n",
      "step 4897 loss: 2.5534160137176514\n",
      "step 4898 loss: 2.5283586978912354\n",
      "step 4899 loss: 2.701280117034912\n",
      "step 4900 loss: 2.6088051795959473\n",
      "step 4901 loss: 2.5174267292022705\n",
      "step 4902 loss: 2.60966157913208\n",
      "step 4903 loss: 2.620244026184082\n",
      "step 4904 loss: 2.510568857192993\n",
      "step 4905 loss: 2.6338188648223877\n",
      "step 4906 loss: 2.597832202911377\n",
      "step 4907 loss: 2.636721611022949\n",
      "step 4908 loss: 2.547151565551758\n",
      "step 4909 loss: 2.6360061168670654\n",
      "step 4910 loss: 2.576728105545044\n",
      "step 4911 loss: 2.6119275093078613\n",
      "step 4912 loss: 2.526884078979492\n",
      "step 4913 loss: 2.5393600463867188\n",
      "step 4914 loss: 2.7201642990112305\n",
      "step 4915 loss: 2.598745346069336\n",
      "step 4916 loss: 2.664823532104492\n",
      "step 4917 loss: 2.559201240539551\n",
      "step 4918 loss: 2.4796359539031982\n",
      "step 4919 loss: 2.612287998199463\n",
      "step 4920 loss: 2.5692317485809326\n",
      "step 4921 loss: 2.65315842628479\n",
      "step 4922 loss: 2.5501132011413574\n",
      "step 4923 loss: 2.4352312088012695\n",
      "step 4924 loss: 2.401028871536255\n",
      "step 4925 loss: 2.4620814323425293\n",
      "step 4926 loss: 2.576225996017456\n",
      "step 4927 loss: 2.4789211750030518\n",
      "step 4928 loss: 2.5766143798828125\n",
      "step 4929 loss: 2.5297422409057617\n",
      "step 4930 loss: 2.5722413063049316\n",
      "step 4931 loss: 2.5571587085723877\n",
      "step 4932 loss: 2.6047303676605225\n",
      "step 4933 loss: 2.619344711303711\n",
      "step 4934 loss: 2.540593147277832\n",
      "step 4935 loss: 2.4765090942382812\n",
      "step 4936 loss: 2.547165632247925\n",
      "step 4937 loss: 2.605795383453369\n",
      "step 4938 loss: 2.5876944065093994\n",
      "step 4939 loss: 2.413081645965576\n",
      "step 4940 loss: 2.683912992477417\n",
      "step 4941 loss: 2.564023017883301\n",
      "step 4942 loss: 2.6011393070220947\n",
      "step 4943 loss: 2.53995943069458\n",
      "step 4944 loss: 2.5436527729034424\n",
      "step 4945 loss: 2.475835084915161\n",
      "step 4946 loss: 2.489109754562378\n",
      "step 4947 loss: 2.575126886367798\n",
      "step 4948 loss: 2.666672706604004\n",
      "step 4949 loss: 2.578906297683716\n",
      "step 4950 loss: 2.6474711894989014\n",
      "step 4951 loss: 2.6823978424072266\n",
      "step 4952 loss: 2.665877342224121\n",
      "step 4953 loss: 2.490180253982544\n",
      "step 4954 loss: 2.6244537830352783\n",
      "step 4955 loss: 2.617067337036133\n",
      "step 4956 loss: 2.526066303253174\n",
      "step 4957 loss: 2.5817930698394775\n",
      "step 4958 loss: 2.6085216999053955\n",
      "step 4959 loss: 2.6015987396240234\n",
      "step 4960 loss: 2.555835008621216\n",
      "step 4961 loss: 2.528865337371826\n",
      "step 4962 loss: 2.4434452056884766\n",
      "step 4963 loss: 2.5611345767974854\n",
      "step 4964 loss: 2.573096513748169\n",
      "step 4965 loss: 2.5367789268493652\n",
      "step 4966 loss: 2.5351779460906982\n",
      "step 4967 loss: 2.6158106327056885\n",
      "step 4968 loss: 2.5340256690979004\n",
      "step 4969 loss: 2.594660520553589\n",
      "step 4970 loss: 2.57661771774292\n",
      "step 4971 loss: 2.618072986602783\n",
      "step 4972 loss: 2.556142807006836\n",
      "step 4973 loss: 2.5894200801849365\n",
      "step 4974 loss: 2.532752513885498\n",
      "step 4975 loss: 2.728001356124878\n",
      "step 4976 loss: 2.5276129245758057\n",
      "step 4977 loss: 2.673584461212158\n",
      "step 4978 loss: 2.5770511627197266\n",
      "step 4979 loss: 2.7378783226013184\n",
      "step 4980 loss: 2.61897873878479\n",
      "step 4981 loss: 2.641812324523926\n",
      "step 4982 loss: 2.660709857940674\n",
      "step 4983 loss: 2.6518683433532715\n",
      "step 4984 loss: 2.5004756450653076\n",
      "step 4985 loss: 2.443274974822998\n",
      "step 4986 loss: 2.5014193058013916\n",
      "step 4987 loss: 2.4443438053131104\n",
      "step 4988 loss: 2.567687511444092\n",
      "step 4989 loss: 2.5062825679779053\n",
      "step 4990 loss: 2.4927327632904053\n",
      "step 4991 loss: 2.5369584560394287\n",
      "step 4992 loss: 2.4313454627990723\n",
      "step 4993 loss: 2.6015982627868652\n",
      "step 4994 loss: 2.5404655933380127\n",
      "step 4995 loss: 2.6044280529022217\n",
      "step 4996 loss: 2.6623969078063965\n",
      "step 4997 loss: 2.4921951293945312\n",
      "step 4998 loss: 2.5758981704711914\n",
      "step 4999 loss: 2.6809184551239014\n",
      "step 5000 loss: 2.5105180740356445\n",
      "step 5001 loss: 2.498452663421631\n",
      "step 5002 loss: 2.6411519050598145\n",
      "step 5003 loss: 2.545017957687378\n",
      "step 5004 loss: 2.5822653770446777\n",
      "step 5005 loss: 2.5510265827178955\n",
      "step 5006 loss: 2.5688059329986572\n",
      "step 5007 loss: 2.547714948654175\n",
      "step 5008 loss: 2.502406358718872\n",
      "step 5009 loss: 2.4768574237823486\n",
      "step 5010 loss: 2.514636993408203\n",
      "step 5011 loss: 2.4804952144622803\n",
      "step 5012 loss: 2.5187432765960693\n",
      "step 5013 loss: 2.471015453338623\n",
      "step 5014 loss: 2.5060224533081055\n",
      "step 5015 loss: 2.553105115890503\n",
      "step 5016 loss: 2.6693813800811768\n",
      "step 5017 loss: 2.6579267978668213\n",
      "step 5018 loss: 2.5331437587738037\n",
      "step 5019 loss: 2.5448012351989746\n",
      "step 5020 loss: 2.519066572189331\n",
      "step 5021 loss: 2.494197368621826\n",
      "step 5022 loss: 2.6255569458007812\n",
      "step 5023 loss: 2.6027283668518066\n",
      "step 5024 loss: 2.4937009811401367\n",
      "step 5025 loss: 2.585630178451538\n",
      "step 5026 loss: 2.568652391433716\n",
      "step 5027 loss: 2.54506254196167\n",
      "step 5028 loss: 2.5943119525909424\n",
      "step 5029 loss: 2.4979991912841797\n",
      "step 5030 loss: 2.46372127532959\n",
      "step 5031 loss: 2.5440189838409424\n",
      "step 5032 loss: 2.6040382385253906\n",
      "step 5033 loss: 2.538984537124634\n",
      "step 5034 loss: 2.613231897354126\n",
      "step 5035 loss: 2.6427505016326904\n",
      "step 5036 loss: 2.4055044651031494\n",
      "step 5037 loss: 2.4835338592529297\n",
      "step 5038 loss: 2.556751251220703\n",
      "step 5039 loss: 2.57570219039917\n",
      "step 5040 loss: 2.6242897510528564\n",
      "step 5041 loss: 2.5811281204223633\n",
      "step 5042 loss: 2.531205892562866\n",
      "step 5043 loss: 2.5506672859191895\n",
      "step 5044 loss: 2.49810791015625\n",
      "step 5045 loss: 2.5038764476776123\n",
      "step 5046 loss: 2.588451623916626\n",
      "step 5047 loss: 2.5774760246276855\n",
      "step 5048 loss: 2.493568181991577\n",
      "step 5049 loss: 2.5814244747161865\n",
      "step 5050 loss: 2.589650869369507\n",
      "step 5051 loss: 2.5571136474609375\n",
      "step 5052 loss: 2.544541120529175\n",
      "step 5053 loss: 2.4394993782043457\n",
      "step 5054 loss: 2.51873779296875\n",
      "step 5055 loss: 2.4849045276641846\n",
      "step 5056 loss: 2.5641226768493652\n",
      "step 5057 loss: 2.610833168029785\n",
      "step 5058 loss: 2.4843761920928955\n",
      "step 5059 loss: 2.5071208477020264\n",
      "step 5060 loss: 2.566913366317749\n",
      "step 5061 loss: 2.6530537605285645\n",
      "step 5062 loss: 2.5122063159942627\n",
      "step 5063 loss: 2.5278334617614746\n",
      "step 5064 loss: 2.596686363220215\n",
      "step 5065 loss: 2.4842312335968018\n",
      "step 5066 loss: 2.605236530303955\n",
      "step 5067 loss: 2.590451717376709\n",
      "step 5068 loss: 2.4990017414093018\n",
      "step 5069 loss: 2.5724968910217285\n",
      "step 5070 loss: 2.523258924484253\n",
      "step 5071 loss: 2.4443249702453613\n",
      "step 5072 loss: 2.451639175415039\n",
      "step 5073 loss: 2.606163501739502\n",
      "step 5074 loss: 2.5682101249694824\n",
      "step 5075 loss: 2.659529447555542\n",
      "step 5076 loss: 2.515909194946289\n",
      "step 5077 loss: 2.5406548976898193\n",
      "step 5078 loss: 2.665921449661255\n",
      "step 5079 loss: 2.575064182281494\n",
      "step 5080 loss: 2.6314852237701416\n",
      "step 5081 loss: 2.5496153831481934\n",
      "step 5082 loss: 2.5835368633270264\n",
      "step 5083 loss: 2.5404000282287598\n",
      "step 5084 loss: 2.611112356185913\n",
      "step 5085 loss: 2.5867066383361816\n",
      "step 5086 loss: 2.518859624862671\n",
      "step 5087 loss: 2.63832950592041\n",
      "step 5088 loss: 2.4670276641845703\n",
      "step 5089 loss: 2.570312738418579\n",
      "step 5090 loss: 2.5831820964813232\n",
      "step 5091 loss: 2.5594048500061035\n",
      "step 5092 loss: 2.5140457153320312\n",
      "step 5093 loss: 2.4865517616271973\n",
      "step 5094 loss: 2.5217926502227783\n",
      "step 5095 loss: 2.67004656791687\n",
      "step 5096 loss: 2.6209328174591064\n",
      "step 5097 loss: 2.6477155685424805\n",
      "step 5098 loss: 2.454003095626831\n",
      "step 5099 loss: 2.4290480613708496\n",
      "step 5100 loss: 2.574131727218628\n",
      "step 5101 loss: 2.5872488021850586\n",
      "step 5102 loss: 2.5173752307891846\n",
      "step 5103 loss: 2.6081807613372803\n",
      "step 5104 loss: 2.464362621307373\n",
      "step 5105 loss: 2.5128531455993652\n",
      "step 5106 loss: 2.492542028427124\n",
      "step 5107 loss: 2.4488539695739746\n",
      "step 5108 loss: 2.536376476287842\n",
      "step 5109 loss: 2.6264214515686035\n",
      "step 5110 loss: 2.6667933464050293\n",
      "step 5111 loss: 2.536594867706299\n",
      "step 5112 loss: 2.614116668701172\n",
      "step 5113 loss: 2.6188788414001465\n",
      "step 5114 loss: 2.5631470680236816\n",
      "step 5115 loss: 2.6386520862579346\n",
      "step 5116 loss: 2.5617446899414062\n",
      "step 5117 loss: 2.4422240257263184\n",
      "step 5118 loss: 2.5353965759277344\n",
      "step 5119 loss: 2.5430750846862793\n",
      "step 5120 loss: 2.636831045150757\n",
      "step 5121 loss: 2.445570230484009\n",
      "step 5122 loss: 2.544557571411133\n",
      "step 5123 loss: 2.5831053256988525\n",
      "step 5124 loss: 2.52988338470459\n",
      "step 5125 loss: 2.5580358505249023\n",
      "step 5126 loss: 2.544551372528076\n",
      "step 5127 loss: 2.5180671215057373\n",
      "step 5128 loss: 2.544677257537842\n",
      "step 5129 loss: 2.6819140911102295\n",
      "step 5130 loss: 2.5057950019836426\n",
      "step 5131 loss: 2.5921998023986816\n",
      "step 5132 loss: 2.7357468605041504\n",
      "step 5133 loss: 2.6529622077941895\n",
      "step 5134 loss: 2.5386557579040527\n",
      "step 5135 loss: 2.5380780696868896\n",
      "step 5136 loss: 2.5839710235595703\n",
      "step 5137 loss: 2.468766212463379\n",
      "step 5138 loss: 2.367617607116699\n",
      "step 5139 loss: 2.5751492977142334\n",
      "step 5140 loss: 2.5377039909362793\n",
      "step 5141 loss: 2.476982593536377\n",
      "step 5142 loss: 2.4861485958099365\n",
      "step 5143 loss: 2.443341016769409\n",
      "step 5144 loss: 2.6616098880767822\n",
      "step 5145 loss: 2.4858558177948\n",
      "step 5146 loss: 2.5898988246917725\n",
      "step 5147 loss: 2.513141393661499\n",
      "step 5148 loss: 2.5817770957946777\n",
      "step 5149 loss: 2.692006826400757\n",
      "step 5150 loss: 2.596784830093384\n",
      "step 5151 loss: 2.5480728149414062\n",
      "step 5152 loss: 2.510852813720703\n",
      "step 5153 loss: 2.5072109699249268\n",
      "step 5154 loss: 2.5970261096954346\n",
      "step 5155 loss: 2.535335063934326\n",
      "step 5156 loss: 2.5447804927825928\n",
      "step 5157 loss: 2.6418869495391846\n",
      "step 5158 loss: 2.6041224002838135\n",
      "step 5159 loss: 2.618035078048706\n",
      "step 5160 loss: 2.5543923377990723\n",
      "step 5161 loss: 2.574083089828491\n",
      "step 5162 loss: 2.5503687858581543\n",
      "step 5163 loss: 2.5052225589752197\n",
      "step 5164 loss: 2.530527114868164\n",
      "step 5165 loss: 2.4700329303741455\n",
      "step 5166 loss: 2.54093599319458\n",
      "step 5167 loss: 2.4999377727508545\n",
      "step 5168 loss: 2.6397008895874023\n",
      "step 5169 loss: 2.5584909915924072\n",
      "step 5170 loss: 2.556008815765381\n",
      "step 5171 loss: 2.4602720737457275\n",
      "step 5172 loss: 2.5480000972747803\n",
      "step 5173 loss: 2.5743093490600586\n",
      "step 5174 loss: 2.684633255004883\n",
      "step 5175 loss: 2.5622856616973877\n",
      "step 5176 loss: 2.545844793319702\n",
      "step 5177 loss: 2.648836851119995\n",
      "step 5178 loss: 2.4868781566619873\n",
      "step 5179 loss: 2.557406425476074\n",
      "step 5180 loss: 2.52949595451355\n",
      "step 5181 loss: 2.641604423522949\n",
      "step 5182 loss: 2.498749017715454\n",
      "step 5183 loss: 2.535249710083008\n",
      "step 5184 loss: 2.5313849449157715\n",
      "step 5185 loss: 2.3813395500183105\n",
      "step 5186 loss: 2.559483051300049\n",
      "step 5187 loss: 2.4988062381744385\n",
      "step 5188 loss: 2.532940149307251\n",
      "step 5189 loss: 2.470513343811035\n",
      "step 5190 loss: 2.618847370147705\n",
      "step 5191 loss: 2.6464743614196777\n",
      "step 5192 loss: 2.5835914611816406\n",
      "step 5193 loss: 2.636049509048462\n",
      "step 5194 loss: 2.5653390884399414\n",
      "step 5195 loss: 2.459688663482666\n",
      "step 5196 loss: 2.6262311935424805\n",
      "step 5197 loss: 2.6231820583343506\n",
      "step 5198 loss: 2.6318585872650146\n",
      "step 5199 loss: 2.617816686630249\n",
      "step 5200 loss: 2.4975621700286865\n",
      "step 5201 loss: 2.5004265308380127\n",
      "step 5202 loss: 2.5004003047943115\n",
      "step 5203 loss: 2.579477310180664\n",
      "step 5204 loss: 2.5395238399505615\n",
      "step 5205 loss: 2.478151321411133\n",
      "step 5206 loss: 2.3630363941192627\n",
      "step 5207 loss: 2.5130960941314697\n",
      "step 5208 loss: 2.5637545585632324\n",
      "step 5209 loss: 2.443134069442749\n",
      "step 5210 loss: 2.493852138519287\n",
      "step 5211 loss: 2.6111488342285156\n",
      "step 5212 loss: 2.6492979526519775\n",
      "step 5213 loss: 2.5451149940490723\n",
      "step 5214 loss: 2.547886610031128\n",
      "step 5215 loss: 2.4752821922302246\n",
      "step 5216 loss: 2.586763858795166\n",
      "step 5217 loss: 2.6902430057525635\n",
      "step 5218 loss: 2.5697669982910156\n",
      "step 5219 loss: 2.510667324066162\n",
      "step 5220 loss: 2.6059834957122803\n",
      "step 5221 loss: 2.5360007286071777\n",
      "step 5222 loss: 2.5725278854370117\n",
      "step 5223 loss: 2.475450038909912\n",
      "step 5224 loss: 2.5262091159820557\n",
      "step 5225 loss: 2.6663174629211426\n",
      "step 5226 loss: 2.664470911026001\n",
      "step 5227 loss: 2.4937682151794434\n",
      "step 5228 loss: 2.547166109085083\n",
      "step 5229 loss: 2.5396313667297363\n",
      "step 5230 loss: 2.573148012161255\n",
      "step 5231 loss: 2.5845248699188232\n",
      "step 5232 loss: 2.630953311920166\n",
      "step 5233 loss: 2.4714298248291016\n",
      "step 5234 loss: 2.540879487991333\n",
      "step 5235 loss: 2.5794882774353027\n",
      "step 5236 loss: 2.522230386734009\n",
      "step 5237 loss: 2.511352777481079\n",
      "step 5238 loss: 2.4736056327819824\n",
      "step 5239 loss: 2.6156165599823\n",
      "step 5240 loss: 2.5773613452911377\n",
      "step 5241 loss: 2.4468576908111572\n",
      "step 5242 loss: 2.5876851081848145\n",
      "step 5243 loss: 2.566420316696167\n",
      "step 5244 loss: 2.6148295402526855\n",
      "step 5245 loss: 2.5858757495880127\n",
      "step 5246 loss: 2.5606958866119385\n",
      "step 5247 loss: 2.5088117122650146\n",
      "step 5248 loss: 2.4476733207702637\n",
      "step 5249 loss: 2.451824426651001\n",
      "step 5250 loss: 2.609483242034912\n",
      "step 5251 loss: 2.540860414505005\n",
      "step 5252 loss: 2.5083658695220947\n",
      "step 5253 loss: 2.5524027347564697\n",
      "step 5254 loss: 2.5931594371795654\n",
      "step 5255 loss: 2.6175482273101807\n",
      "step 5256 loss: 2.509608507156372\n",
      "step 5257 loss: 2.465851306915283\n",
      "step 5258 loss: 2.5832371711730957\n",
      "step 5259 loss: 2.604107618331909\n",
      "step 5260 loss: 2.552011013031006\n",
      "step 5261 loss: 2.4164483547210693\n",
      "step 5262 loss: 2.5351643562316895\n",
      "step 5263 loss: 2.488983392715454\n",
      "step 5264 loss: 2.5474750995635986\n",
      "step 5265 loss: 2.4523658752441406\n",
      "step 5266 loss: 2.604783535003662\n",
      "step 5267 loss: 2.5883278846740723\n",
      "step 5268 loss: 2.600639820098877\n",
      "step 5269 loss: 2.4951746463775635\n",
      "step 5270 loss: 2.5147318840026855\n",
      "step 5271 loss: 2.4509165287017822\n",
      "step 5272 loss: 2.503504753112793\n",
      "step 5273 loss: 2.5782315731048584\n",
      "step 5274 loss: 2.513148784637451\n",
      "step 5275 loss: 2.5748109817504883\n",
      "step 5276 loss: 2.5571703910827637\n",
      "step 5277 loss: 2.556985378265381\n",
      "step 5278 loss: 2.556049108505249\n",
      "step 5279 loss: 2.6078007221221924\n",
      "step 5280 loss: 2.703986883163452\n",
      "step 5281 loss: 2.5056729316711426\n",
      "step 5282 loss: 2.4801626205444336\n",
      "step 5283 loss: 2.5753769874572754\n",
      "step 5284 loss: 2.4650909900665283\n",
      "step 5285 loss: 2.4870102405548096\n",
      "step 5286 loss: 2.521710157394409\n",
      "step 5287 loss: 2.521059274673462\n",
      "step 5288 loss: 2.621356725692749\n",
      "step 5289 loss: 2.512791633605957\n",
      "step 5290 loss: 2.595227003097534\n",
      "step 5291 loss: 2.4313933849334717\n",
      "step 5292 loss: 2.4834389686584473\n",
      "step 5293 loss: 2.5701792240142822\n",
      "step 5294 loss: 2.620485305786133\n",
      "step 5295 loss: 2.5630054473876953\n",
      "step 5296 loss: 2.44266939163208\n",
      "step 5297 loss: 2.5373010635375977\n",
      "step 5298 loss: 2.525014877319336\n",
      "step 5299 loss: 2.5552241802215576\n",
      "step 5300 loss: 2.525254487991333\n",
      "step 5301 loss: 2.469564437866211\n",
      "step 5302 loss: 2.685802698135376\n",
      "step 5303 loss: 2.493011713027954\n",
      "step 5304 loss: 2.4978418350219727\n",
      "step 5305 loss: 2.6347217559814453\n",
      "step 5306 loss: 2.582186460494995\n",
      "step 5307 loss: 2.592202663421631\n",
      "step 5308 loss: 2.5114099979400635\n",
      "step 5309 loss: 2.4385392665863037\n",
      "step 5310 loss: 2.576220989227295\n",
      "step 5311 loss: 2.564939260482788\n",
      "step 5312 loss: 2.473996877670288\n",
      "step 5313 loss: 2.4351654052734375\n",
      "step 5314 loss: 2.523545026779175\n",
      "step 5315 loss: 2.5884740352630615\n",
      "step 5316 loss: 2.6196727752685547\n",
      "step 5317 loss: 2.634235382080078\n",
      "step 5318 loss: 2.6933140754699707\n",
      "step 5319 loss: 2.545815944671631\n",
      "step 5320 loss: 2.5616931915283203\n",
      "step 5321 loss: 2.557035446166992\n",
      "step 5322 loss: 2.522796392440796\n",
      "step 5323 loss: 2.5805394649505615\n",
      "step 5324 loss: 2.480056047439575\n",
      "step 5325 loss: 2.572932004928589\n",
      "step 5326 loss: 2.4411699771881104\n",
      "step 5327 loss: 2.4330437183380127\n",
      "step 5328 loss: 2.458176851272583\n",
      "step 5329 loss: 2.579942464828491\n",
      "step 5330 loss: 2.558119535446167\n",
      "step 5331 loss: 2.5871362686157227\n",
      "step 5332 loss: 2.5489227771759033\n",
      "step 5333 loss: 2.5596983432769775\n",
      "step 5334 loss: 2.5839478969573975\n",
      "step 5335 loss: 2.518948554992676\n",
      "step 5336 loss: 2.592423439025879\n",
      "step 5337 loss: 2.5623726844787598\n",
      "step 5338 loss: 2.5577914714813232\n",
      "step 5339 loss: 2.5395348072052\n",
      "step 5340 loss: 2.5424017906188965\n",
      "step 5341 loss: 2.580735921859741\n",
      "step 5342 loss: 2.5949833393096924\n",
      "step 5343 loss: 2.4718940258026123\n",
      "step 5344 loss: 2.521512031555176\n",
      "step 5345 loss: 2.461127996444702\n",
      "step 5346 loss: 2.575921058654785\n",
      "step 5347 loss: 2.5200023651123047\n",
      "step 5348 loss: 2.546973943710327\n",
      "step 5349 loss: 2.460329532623291\n",
      "step 5350 loss: 2.488356828689575\n",
      "step 5351 loss: 2.4885997772216797\n",
      "step 5352 loss: 2.566920518875122\n",
      "step 5353 loss: 2.599087715148926\n",
      "step 5354 loss: 2.461591958999634\n",
      "step 5355 loss: 2.5091421604156494\n",
      "step 5356 loss: 2.5436887741088867\n",
      "step 5357 loss: 2.532276153564453\n",
      "step 5358 loss: 2.618992567062378\n",
      "step 5359 loss: 2.536531448364258\n",
      "step 5360 loss: 2.4167065620422363\n",
      "step 5361 loss: 2.4583687782287598\n",
      "step 5362 loss: 2.5183427333831787\n",
      "step 5363 loss: 2.6021931171417236\n",
      "step 5364 loss: 2.4992172718048096\n",
      "step 5365 loss: 2.5219454765319824\n",
      "step 5366 loss: 2.5043978691101074\n",
      "step 5367 loss: 2.4809815883636475\n",
      "step 5368 loss: 2.54227876663208\n",
      "step 5369 loss: 2.5874898433685303\n",
      "step 5370 loss: 2.760570526123047\n",
      "step 5371 loss: 2.660414695739746\n",
      "step 5372 loss: 2.647026300430298\n",
      "step 5373 loss: 2.4016237258911133\n",
      "step 5374 loss: 2.5596354007720947\n",
      "step 5375 loss: 2.3763561248779297\n",
      "step 5376 loss: 2.420936107635498\n",
      "step 5377 loss: 2.3804478645324707\n",
      "step 5378 loss: 2.650790214538574\n",
      "step 5379 loss: 2.5360841751098633\n",
      "step 5380 loss: 2.5106050968170166\n",
      "step 5381 loss: 2.5259501934051514\n",
      "step 5382 loss: 2.488806962966919\n",
      "step 5383 loss: 2.619882583618164\n",
      "step 5384 loss: 2.653773784637451\n",
      "step 5385 loss: 2.535480260848999\n",
      "step 5386 loss: 2.4622745513916016\n",
      "step 5387 loss: 2.584929943084717\n",
      "step 5388 loss: 2.47324275970459\n",
      "step 5389 loss: 2.546670436859131\n",
      "step 5390 loss: 2.5379109382629395\n",
      "step 5391 loss: 2.585702419281006\n",
      "step 5392 loss: 2.5551934242248535\n",
      "step 5393 loss: 2.5130841732025146\n",
      "step 5394 loss: 2.440106153488159\n",
      "step 5395 loss: 2.5772385597229004\n",
      "step 5396 loss: 2.400325059890747\n",
      "step 5397 loss: 2.5292856693267822\n",
      "step 5398 loss: 2.4445641040802\n",
      "step 5399 loss: 2.6122937202453613\n",
      "step 5400 loss: 2.4852945804595947\n",
      "step 5401 loss: 2.5752198696136475\n",
      "step 5402 loss: 2.512342929840088\n",
      "step 5403 loss: 2.5072576999664307\n",
      "step 5404 loss: 2.4121689796447754\n",
      "step 5405 loss: 2.5614445209503174\n",
      "step 5406 loss: 2.7137770652770996\n",
      "step 5407 loss: 2.433406352996826\n",
      "step 5408 loss: 2.465740442276001\n",
      "step 5409 loss: 2.581660747528076\n",
      "step 5410 loss: 2.4969358444213867\n",
      "step 5411 loss: 2.552773952484131\n",
      "step 5412 loss: 2.496720790863037\n",
      "step 5413 loss: 2.5893447399139404\n",
      "step 5414 loss: 2.3601930141448975\n",
      "step 5415 loss: 2.503239154815674\n",
      "step 5416 loss: 2.6057333946228027\n",
      "step 5417 loss: 2.5694801807403564\n",
      "step 5418 loss: 2.5842857360839844\n",
      "step 5419 loss: 2.542264938354492\n",
      "step 5420 loss: 2.4818203449249268\n",
      "step 5421 loss: 2.5769898891448975\n",
      "step 5422 loss: 2.45965313911438\n",
      "step 5423 loss: 2.548203706741333\n",
      "step 5424 loss: 2.5591189861297607\n",
      "step 5425 loss: 2.505568265914917\n",
      "step 5426 loss: 2.5123403072357178\n",
      "step 5427 loss: 2.5332534313201904\n",
      "step 5428 loss: 2.5408005714416504\n",
      "step 5429 loss: 2.578321695327759\n",
      "step 5430 loss: 2.5858359336853027\n",
      "step 5431 loss: 2.677086591720581\n",
      "step 5432 loss: 2.569902181625366\n",
      "step 5433 loss: 2.5367226600646973\n",
      "step 5434 loss: 2.511995553970337\n",
      "step 5435 loss: 2.516643762588501\n",
      "step 5436 loss: 2.4758458137512207\n",
      "step 5437 loss: 2.6719160079956055\n",
      "step 5438 loss: 2.5392963886260986\n",
      "step 5439 loss: 2.499835968017578\n",
      "step 5440 loss: 2.662271022796631\n",
      "step 5441 loss: 2.527116060256958\n",
      "step 5442 loss: 2.43109393119812\n",
      "step 5443 loss: 2.519507646560669\n",
      "step 5444 loss: 2.6591506004333496\n",
      "step 5445 loss: 2.585989236831665\n",
      "step 5446 loss: 2.496803045272827\n",
      "step 5447 loss: 2.676234483718872\n",
      "step 5448 loss: 2.548215389251709\n",
      "step 5449 loss: 2.6210784912109375\n",
      "step 5450 loss: 2.5067138671875\n",
      "step 5451 loss: 2.5323827266693115\n",
      "step 5452 loss: 2.4911417961120605\n",
      "step 5453 loss: 2.5360610485076904\n",
      "step 5454 loss: 2.5938570499420166\n",
      "step 5455 loss: 2.6612942218780518\n",
      "step 5456 loss: 2.5653181076049805\n",
      "step 5457 loss: 2.4422292709350586\n",
      "step 5458 loss: 2.5031251907348633\n",
      "step 5459 loss: 2.622713565826416\n",
      "step 5460 loss: 2.552159070968628\n",
      "step 5461 loss: 2.59814190864563\n",
      "step 5462 loss: 2.424873113632202\n",
      "step 5463 loss: 2.4985601902008057\n",
      "step 5464 loss: 2.5409271717071533\n",
      "step 5465 loss: 2.5418639183044434\n",
      "step 5466 loss: 2.4282710552215576\n",
      "step 5467 loss: 2.5786139965057373\n",
      "step 5468 loss: 2.5457863807678223\n",
      "step 5469 loss: 2.449205160140991\n",
      "step 5470 loss: 2.558091402053833\n",
      "step 5471 loss: 2.523341655731201\n",
      "step 5472 loss: 2.6279051303863525\n",
      "step 5473 loss: 2.6073126792907715\n",
      "step 5474 loss: 2.5236687660217285\n",
      "step 5475 loss: 2.4780166149139404\n",
      "step 5476 loss: 2.7081940174102783\n",
      "step 5477 loss: 2.4668633937835693\n",
      "step 5478 loss: 2.6269092559814453\n",
      "step 5479 loss: 2.65512752532959\n",
      "step 5480 loss: 2.4679343700408936\n",
      "step 5481 loss: 2.4683876037597656\n",
      "step 5482 loss: 2.5553576946258545\n",
      "step 5483 loss: 2.4802775382995605\n",
      "step 5484 loss: 2.540595769882202\n",
      "step 5485 loss: 2.532228946685791\n",
      "step 5486 loss: 2.570178508758545\n",
      "step 5487 loss: 2.6604504585266113\n",
      "step 5488 loss: 2.4867749214172363\n",
      "step 5489 loss: 2.4587581157684326\n",
      "step 5490 loss: 2.4742276668548584\n",
      "step 5491 loss: 2.589735746383667\n",
      "step 5492 loss: 2.6544501781463623\n",
      "step 5493 loss: 2.6739139556884766\n",
      "step 5494 loss: 2.505615711212158\n",
      "step 5495 loss: 2.5251331329345703\n",
      "step 5496 loss: 2.583038091659546\n",
      "step 5497 loss: 2.5497186183929443\n",
      "step 5498 loss: 2.637336254119873\n",
      "step 5499 loss: 2.5408599376678467\n",
      "step 5500 loss: 2.548015594482422\n",
      "step 5501 loss: 2.399907350540161\n",
      "step 5502 loss: 2.451047658920288\n",
      "step 5503 loss: 2.492816686630249\n",
      "step 5504 loss: 2.4310450553894043\n",
      "step 5505 loss: 2.5221376419067383\n",
      "step 5506 loss: 2.5141103267669678\n",
      "step 5507 loss: 2.5878372192382812\n",
      "step 5508 loss: 2.547549247741699\n",
      "step 5509 loss: 2.411020517349243\n",
      "step 5510 loss: 2.527316093444824\n",
      "step 5511 loss: 2.557145833969116\n",
      "step 5512 loss: 2.3873393535614014\n",
      "step 5513 loss: 2.491694688796997\n",
      "step 5514 loss: 2.4570202827453613\n",
      "step 5515 loss: 2.5130834579467773\n",
      "step 5516 loss: 2.540710687637329\n",
      "step 5517 loss: 2.4636335372924805\n",
      "step 5518 loss: 2.507725954055786\n",
      "step 5519 loss: 2.446361541748047\n",
      "step 5520 loss: 2.4424102306365967\n",
      "step 5521 loss: 2.4751944541931152\n",
      "step 5522 loss: 2.5563435554504395\n",
      "step 5523 loss: 2.556971549987793\n",
      "step 5524 loss: 2.3987624645233154\n",
      "step 5525 loss: 2.506708860397339\n",
      "step 5526 loss: 2.510694742202759\n",
      "step 5527 loss: 2.704155206680298\n",
      "step 5528 loss: 2.593503952026367\n",
      "step 5529 loss: 2.6318247318267822\n",
      "step 5530 loss: 2.5408310890197754\n",
      "step 5531 loss: 2.5750184059143066\n",
      "step 5532 loss: 2.5498595237731934\n",
      "step 5533 loss: 2.4871063232421875\n",
      "step 5534 loss: 2.435981273651123\n",
      "step 5535 loss: 2.5359885692596436\n",
      "step 5536 loss: 2.5040905475616455\n",
      "step 5537 loss: 2.5167832374572754\n",
      "step 5538 loss: 2.594238519668579\n",
      "step 5539 loss: 2.6294102668762207\n",
      "step 5540 loss: 2.6159205436706543\n",
      "step 5541 loss: 2.62330961227417\n",
      "step 5542 loss: 2.3592686653137207\n",
      "step 5543 loss: 2.600346088409424\n",
      "step 5544 loss: 2.3936259746551514\n",
      "step 5545 loss: 2.4993321895599365\n",
      "step 5546 loss: 2.572932720184326\n",
      "step 5547 loss: 2.672729015350342\n",
      "step 5548 loss: 2.4882359504699707\n",
      "step 5549 loss: 2.4943881034851074\n",
      "step 5550 loss: 2.521092414855957\n",
      "step 5551 loss: 2.4905993938446045\n",
      "step 5552 loss: 2.507312059402466\n",
      "step 5553 loss: 2.583264112472534\n",
      "step 5554 loss: 2.5864179134368896\n",
      "step 5555 loss: 2.5219709873199463\n",
      "step 5556 loss: 2.5422914028167725\n",
      "step 5557 loss: 2.54952073097229\n",
      "step 5558 loss: 2.5868186950683594\n",
      "step 5559 loss: 2.5259575843811035\n",
      "step 5560 loss: 2.698676824569702\n",
      "step 5561 loss: 2.4793410301208496\n",
      "step 5562 loss: 2.376763343811035\n",
      "step 5563 loss: 2.494994878768921\n",
      "step 5564 loss: 2.5004265308380127\n",
      "step 5565 loss: 2.539898157119751\n",
      "step 5566 loss: 2.6183853149414062\n",
      "step 5567 loss: 2.578822374343872\n",
      "step 5568 loss: 2.571141481399536\n",
      "step 5569 loss: 2.416214942932129\n",
      "step 5570 loss: 2.5020828247070312\n",
      "step 5571 loss: 2.558281898498535\n",
      "step 5572 loss: 2.5164942741394043\n",
      "step 5573 loss: 2.5634937286376953\n",
      "step 5574 loss: 2.614447593688965\n",
      "step 5575 loss: 2.422175168991089\n",
      "step 5576 loss: 2.588144063949585\n",
      "step 5577 loss: 2.6253185272216797\n",
      "step 5578 loss: 2.6159801483154297\n",
      "step 5579 loss: 2.639310121536255\n",
      "step 5580 loss: 2.4752306938171387\n",
      "step 5581 loss: 2.5745675563812256\n",
      "step 5582 loss: 2.416260004043579\n",
      "step 5583 loss: 2.4807658195495605\n",
      "step 5584 loss: 2.5426926612854004\n",
      "step 5585 loss: 2.517176389694214\n",
      "step 5586 loss: 2.5850119590759277\n",
      "step 5587 loss: 2.65065336227417\n",
      "step 5588 loss: 2.521132469177246\n",
      "step 5589 loss: 2.635329008102417\n",
      "step 5590 loss: 2.567450523376465\n",
      "step 5591 loss: 2.5091359615325928\n",
      "step 5592 loss: 2.574587345123291\n",
      "step 5593 loss: 2.6886606216430664\n",
      "step 5594 loss: 2.5899503231048584\n",
      "step 5595 loss: 2.483891725540161\n",
      "step 5596 loss: 2.4588496685028076\n",
      "step 5597 loss: 2.531101703643799\n",
      "step 5598 loss: 2.5305635929107666\n",
      "step 5599 loss: 2.56628680229187\n",
      "step 5600 loss: 2.577272415161133\n",
      "step 5601 loss: 2.5071799755096436\n",
      "step 5602 loss: 2.5118680000305176\n",
      "step 5603 loss: 2.577237606048584\n",
      "step 5604 loss: 2.6189773082733154\n",
      "step 5605 loss: 2.5022175312042236\n",
      "step 5606 loss: 2.575685977935791\n",
      "step 5607 loss: 2.5076892375946045\n",
      "step 5608 loss: 2.445674419403076\n",
      "step 5609 loss: 2.549370288848877\n",
      "step 5610 loss: 2.489058017730713\n",
      "step 5611 loss: 2.485048770904541\n",
      "step 5612 loss: 2.602752447128296\n",
      "step 5613 loss: 2.6043999195098877\n",
      "step 5614 loss: 2.490406036376953\n",
      "step 5615 loss: 2.4444737434387207\n",
      "step 5616 loss: 2.5267393589019775\n",
      "step 5617 loss: 2.502812147140503\n",
      "step 5618 loss: 2.5511093139648438\n",
      "step 5619 loss: 2.5259110927581787\n",
      "step 5620 loss: 2.455442428588867\n",
      "step 5621 loss: 2.4937877655029297\n",
      "step 5622 loss: 2.4197323322296143\n",
      "step 5623 loss: 2.434689998626709\n",
      "step 5624 loss: 2.523904800415039\n",
      "step 5625 loss: 2.536252975463867\n",
      "step 5626 loss: 2.4803848266601562\n",
      "step 5627 loss: 2.5480434894561768\n",
      "step 5628 loss: 2.529283046722412\n",
      "step 5629 loss: 2.5749940872192383\n",
      "step 5630 loss: 2.532456636428833\n",
      "step 5631 loss: 2.5403525829315186\n",
      "step 5632 loss: 2.555398464202881\n",
      "step 5633 loss: 2.6041316986083984\n",
      "step 5634 loss: 2.526394844055176\n",
      "step 5635 loss: 2.4535372257232666\n",
      "step 5636 loss: 2.629960298538208\n",
      "step 5637 loss: 2.425004005432129\n",
      "step 5638 loss: 2.6296932697296143\n",
      "step 5639 loss: 2.4715466499328613\n",
      "step 5640 loss: 2.4789681434631348\n",
      "step 5641 loss: 2.5711138248443604\n",
      "step 5642 loss: 2.56543231010437\n",
      "step 5643 loss: 2.525472640991211\n",
      "step 5644 loss: 2.5054056644439697\n",
      "step 5645 loss: 2.4615654945373535\n",
      "step 5646 loss: 2.4683990478515625\n",
      "step 5647 loss: 2.5428459644317627\n",
      "step 5648 loss: 2.473356008529663\n",
      "step 5649 loss: 2.5976943969726562\n",
      "step 5650 loss: 2.54988169670105\n",
      "step 5651 loss: 2.4874470233917236\n",
      "step 5652 loss: 2.5025134086608887\n",
      "step 5653 loss: 2.4275693893432617\n",
      "step 5654 loss: 2.5406477451324463\n",
      "step 5655 loss: 2.5413198471069336\n",
      "step 5656 loss: 2.510996103286743\n",
      "step 5657 loss: 2.4721269607543945\n",
      "step 5658 loss: 2.581763505935669\n",
      "step 5659 loss: 2.636709690093994\n",
      "step 5660 loss: 2.6428275108337402\n",
      "step 5661 loss: 2.5531694889068604\n",
      "step 5662 loss: 2.583686590194702\n",
      "step 5663 loss: 2.5732264518737793\n",
      "step 5664 loss: 2.557065963745117\n",
      "step 5665 loss: 2.586060047149658\n",
      "step 5666 loss: 2.476505756378174\n",
      "step 5667 loss: 2.5161550045013428\n",
      "step 5668 loss: 2.38249135017395\n",
      "step 5669 loss: 2.540478229522705\n",
      "step 5670 loss: 2.4364278316497803\n",
      "step 5671 loss: 2.58392596244812\n",
      "step 5672 loss: 2.572763442993164\n",
      "step 5673 loss: 2.6989188194274902\n",
      "step 5674 loss: 2.508464813232422\n",
      "step 5675 loss: 2.4837145805358887\n",
      "step 5676 loss: 2.541036605834961\n",
      "step 5677 loss: 2.4813528060913086\n",
      "step 5678 loss: 2.5744917392730713\n",
      "step 5679 loss: 2.5511186122894287\n",
      "step 5680 loss: 2.641432046890259\n",
      "step 5681 loss: 2.6536362171173096\n",
      "step 5682 loss: 2.6000726222991943\n",
      "step 5683 loss: 2.5796868801116943\n",
      "step 5684 loss: 2.5877530574798584\n",
      "step 5685 loss: 2.6199982166290283\n",
      "step 5686 loss: 2.5131354331970215\n",
      "step 5687 loss: 2.671710252761841\n",
      "step 5688 loss: 2.5364930629730225\n",
      "step 5689 loss: 2.42337965965271\n",
      "step 5690 loss: 2.58237886428833\n",
      "step 5691 loss: 2.4602200984954834\n",
      "step 5692 loss: 2.4773108959198\n",
      "step 5693 loss: 2.4605021476745605\n",
      "step 5694 loss: 2.52956485748291\n",
      "step 5695 loss: 2.4894397258758545\n",
      "step 5696 loss: 2.6059165000915527\n",
      "step 5697 loss: 2.4891793727874756\n",
      "step 5698 loss: 2.405081272125244\n",
      "step 5699 loss: 2.5541906356811523\n",
      "step 5700 loss: 2.602153778076172\n",
      "step 5701 loss: 2.578528881072998\n",
      "step 5702 loss: 2.6093082427978516\n",
      "step 5703 loss: 2.6157000064849854\n",
      "step 5704 loss: 2.5453507900238037\n",
      "step 5705 loss: 2.3880317211151123\n",
      "step 5706 loss: 2.633728504180908\n",
      "step 5707 loss: 2.474879026412964\n",
      "step 5708 loss: 2.548191785812378\n",
      "step 5709 loss: 2.49141526222229\n",
      "step 5710 loss: 2.5504722595214844\n",
      "step 5711 loss: 2.6128056049346924\n",
      "step 5712 loss: 2.5876119136810303\n",
      "step 5713 loss: 2.603137969970703\n",
      "step 5714 loss: 2.4592010974884033\n",
      "step 5715 loss: 2.4683728218078613\n",
      "step 5716 loss: 2.52217960357666\n",
      "step 5717 loss: 2.539232015609741\n",
      "step 5718 loss: 2.4410581588745117\n",
      "step 5719 loss: 2.548410415649414\n",
      "step 5720 loss: 2.4533884525299072\n",
      "step 5721 loss: 2.5674707889556885\n",
      "step 5722 loss: 2.5785739421844482\n",
      "step 5723 loss: 2.655818223953247\n",
      "step 5724 loss: 2.525515079498291\n",
      "step 5725 loss: 2.4909069538116455\n",
      "step 5726 loss: 2.5344600677490234\n",
      "step 5727 loss: 2.451970338821411\n",
      "step 5728 loss: 2.5299899578094482\n",
      "step 5729 loss: 2.504930257797241\n",
      "step 5730 loss: 2.5552866458892822\n",
      "step 5731 loss: 2.47666072845459\n",
      "step 5732 loss: 2.4803218841552734\n",
      "step 5733 loss: 2.480027914047241\n",
      "step 5734 loss: 2.545408010482788\n",
      "step 5735 loss: 2.447816848754883\n",
      "step 5736 loss: 2.4448580741882324\n",
      "step 5737 loss: 2.5404601097106934\n",
      "step 5738 loss: 2.6308822631835938\n",
      "step 5739 loss: 2.6484251022338867\n",
      "step 5740 loss: 2.469601631164551\n",
      "step 5741 loss: 2.466616630554199\n",
      "step 5742 loss: 2.502131700515747\n",
      "step 5743 loss: 2.632669448852539\n",
      "step 5744 loss: 2.4738242626190186\n",
      "step 5745 loss: 2.500235080718994\n",
      "step 5746 loss: 2.5040767192840576\n",
      "step 5747 loss: 2.508126735687256\n",
      "step 5748 loss: 2.5839552879333496\n",
      "step 5749 loss: 2.6156327724456787\n",
      "step 5750 loss: 2.5141122341156006\n",
      "step 5751 loss: 2.514335870742798\n",
      "step 5752 loss: 2.4622457027435303\n",
      "step 5753 loss: 2.4831626415252686\n",
      "step 5754 loss: 2.533039093017578\n",
      "step 5755 loss: 2.611842393875122\n",
      "step 5756 loss: 2.468813180923462\n",
      "step 5757 loss: 2.5863840579986572\n",
      "step 5758 loss: 2.5321221351623535\n",
      "step 5759 loss: 2.422677755355835\n",
      "step 5760 loss: 2.390599489212036\n",
      "step 5761 loss: 2.4458374977111816\n",
      "step 5762 loss: 2.518561363220215\n",
      "step 5763 loss: 2.509507656097412\n",
      "step 5764 loss: 2.521287679672241\n",
      "step 5765 loss: 2.4744436740875244\n",
      "step 5766 loss: 2.607677459716797\n",
      "step 5767 loss: 2.3883554935455322\n",
      "step 5768 loss: 2.47403621673584\n",
      "step 5769 loss: 2.5211496353149414\n",
      "step 5770 loss: 2.5672407150268555\n",
      "step 5771 loss: 2.4916832447052\n",
      "step 5772 loss: 2.4658844470977783\n",
      "step 5773 loss: 2.5703651905059814\n",
      "step 5774 loss: 2.518620252609253\n",
      "step 5775 loss: 2.540424108505249\n",
      "step 5776 loss: 2.5845930576324463\n",
      "step 5777 loss: 2.6079914569854736\n",
      "step 5778 loss: 2.572120189666748\n",
      "step 5779 loss: 2.4505650997161865\n",
      "step 5780 loss: 2.6025164127349854\n",
      "step 5781 loss: 2.4939987659454346\n",
      "step 5782 loss: 2.416959762573242\n",
      "step 5783 loss: 2.5148239135742188\n",
      "step 5784 loss: 2.6298394203186035\n",
      "step 5785 loss: 2.4890685081481934\n",
      "step 5786 loss: 2.5503225326538086\n",
      "step 5787 loss: 2.515427827835083\n",
      "step 5788 loss: 2.5012102127075195\n",
      "step 5789 loss: 2.5371897220611572\n",
      "step 5790 loss: 2.547896146774292\n",
      "step 5791 loss: 2.6078176498413086\n",
      "step 5792 loss: 2.492647171020508\n",
      "step 5793 loss: 2.561708450317383\n",
      "step 5794 loss: 2.492196559906006\n",
      "step 5795 loss: 2.440471887588501\n",
      "step 5796 loss: 2.4741504192352295\n",
      "step 5797 loss: 2.531564712524414\n",
      "step 5798 loss: 2.545849561691284\n",
      "step 5799 loss: 2.6534717082977295\n",
      "step 5800 loss: 2.3584301471710205\n",
      "step 5801 loss: 2.446800470352173\n",
      "step 5802 loss: 2.6662497520446777\n",
      "step 5803 loss: 2.5635695457458496\n",
      "step 5804 loss: 2.6394331455230713\n",
      "step 5805 loss: 2.4754393100738525\n",
      "step 5806 loss: 2.5233287811279297\n",
      "step 5807 loss: 2.4199273586273193\n",
      "step 5808 loss: 2.4971795082092285\n",
      "step 5809 loss: 2.6776680946350098\n",
      "step 5810 loss: 2.4942784309387207\n",
      "step 5811 loss: 2.5013177394866943\n",
      "step 5812 loss: 2.4769346714019775\n",
      "step 5813 loss: 2.350642204284668\n",
      "step 5814 loss: 2.500253915786743\n",
      "step 5815 loss: 2.449597120285034\n",
      "step 5816 loss: 2.6344919204711914\n",
      "step 5817 loss: 2.434282064437866\n",
      "step 5818 loss: 2.3775131702423096\n",
      "step 5819 loss: 2.48677396774292\n",
      "step 5820 loss: 2.5228283405303955\n",
      "step 5821 loss: 2.447870969772339\n",
      "step 5822 loss: 2.5846199989318848\n",
      "step 5823 loss: 2.5039665699005127\n",
      "step 5824 loss: 2.483351945877075\n",
      "step 5825 loss: 2.5501201152801514\n",
      "step 5826 loss: 2.6472954750061035\n",
      "step 5827 loss: 2.417931079864502\n",
      "step 5828 loss: 2.6071579456329346\n",
      "step 5829 loss: 2.4712436199188232\n",
      "step 5830 loss: 2.547276735305786\n",
      "step 5831 loss: 2.464221954345703\n",
      "step 5832 loss: 2.7058298587799072\n",
      "step 5833 loss: 2.5935349464416504\n",
      "step 5834 loss: 2.4998300075531006\n",
      "step 5835 loss: 2.500290632247925\n",
      "step 5836 loss: 2.5407938957214355\n",
      "step 5837 loss: 2.587738513946533\n",
      "step 5838 loss: 2.5887601375579834\n",
      "step 5839 loss: 2.4826247692108154\n",
      "step 5840 loss: 2.4898314476013184\n",
      "step 5841 loss: 2.4340455532073975\n",
      "step 5842 loss: 2.486497640609741\n",
      "step 5843 loss: 2.7022969722747803\n",
      "step 5844 loss: 2.402813673019409\n",
      "step 5845 loss: 2.6172754764556885\n",
      "step 5846 loss: 2.4145736694335938\n",
      "step 5847 loss: 2.543687343597412\n",
      "step 5848 loss: 2.399852991104126\n",
      "step 5849 loss: 2.5484113693237305\n",
      "step 5850 loss: 2.469301223754883\n",
      "step 5851 loss: 2.569380283355713\n",
      "step 5852 loss: 2.534757614135742\n",
      "step 5853 loss: 2.3909285068511963\n",
      "step 5854 loss: 2.585456371307373\n",
      "step 5855 loss: 2.554534673690796\n",
      "step 5856 loss: 2.5140151977539062\n",
      "step 5857 loss: 2.5773348808288574\n",
      "step 5858 loss: 2.5707902908325195\n",
      "step 5859 loss: 2.457484245300293\n",
      "step 5860 loss: 2.4926183223724365\n",
      "step 5861 loss: 2.457507848739624\n",
      "step 5862 loss: 2.614036798477173\n",
      "step 5863 loss: 2.5974087715148926\n",
      "step 5864 loss: 2.4543943405151367\n",
      "step 5865 loss: 2.523099899291992\n",
      "step 5866 loss: 2.5614981651306152\n",
      "step 5867 loss: 2.4213709831237793\n",
      "step 5868 loss: 2.4844815731048584\n",
      "step 5869 loss: 2.6092960834503174\n",
      "step 5870 loss: 2.4155967235565186\n",
      "step 5871 loss: 2.5842926502227783\n",
      "step 5872 loss: 2.633578300476074\n",
      "step 5873 loss: 2.541184902191162\n",
      "step 5874 loss: 2.4447388648986816\n",
      "step 5875 loss: 2.4996767044067383\n",
      "step 5876 loss: 2.5611605644226074\n",
      "step 5877 loss: 2.3629302978515625\n",
      "step 5878 loss: 2.426567554473877\n",
      "step 5879 loss: 2.512305736541748\n",
      "step 5880 loss: 2.3826911449432373\n",
      "step 5881 loss: 2.463690757751465\n",
      "step 5882 loss: 2.6369082927703857\n",
      "step 5883 loss: 2.3971569538116455\n",
      "step 5884 loss: 2.653550386428833\n",
      "step 5885 loss: 2.436361312866211\n",
      "step 5886 loss: 2.5126450061798096\n",
      "step 5887 loss: 2.627281665802002\n",
      "step 5888 loss: 2.516957998275757\n",
      "step 5889 loss: 2.501882553100586\n",
      "step 5890 loss: 2.540884017944336\n",
      "step 5891 loss: 2.468283176422119\n",
      "step 5892 loss: 2.537039041519165\n",
      "step 5893 loss: 2.5267984867095947\n",
      "step 5894 loss: 2.499945878982544\n",
      "step 5895 loss: 2.5403201580047607\n",
      "step 5896 loss: 2.4675841331481934\n",
      "step 5897 loss: 2.610168218612671\n",
      "step 5898 loss: 2.5379092693328857\n",
      "step 5899 loss: 2.5148637294769287\n",
      "step 5900 loss: 2.448211908340454\n",
      "step 5901 loss: 2.5903372764587402\n",
      "step 5902 loss: 2.40061092376709\n",
      "step 5903 loss: 2.5865182876586914\n",
      "step 5904 loss: 2.408714771270752\n",
      "step 5905 loss: 2.5135393142700195\n",
      "step 5906 loss: 2.4947633743286133\n",
      "step 5907 loss: 2.5465760231018066\n",
      "step 5908 loss: 2.536703586578369\n",
      "step 5909 loss: 2.495608329772949\n",
      "step 5910 loss: 2.4478673934936523\n",
      "step 5911 loss: 2.5772643089294434\n",
      "step 5912 loss: 2.5120480060577393\n",
      "step 5913 loss: 2.462101697921753\n",
      "step 5914 loss: 2.6213598251342773\n",
      "step 5915 loss: 2.5670299530029297\n",
      "step 5916 loss: 2.5892834663391113\n",
      "step 5917 loss: 2.520890951156616\n",
      "step 5918 loss: 2.4906511306762695\n",
      "step 5919 loss: 2.6219236850738525\n",
      "step 5920 loss: 2.5420854091644287\n",
      "step 5921 loss: 2.629812479019165\n",
      "step 5922 loss: 2.5886025428771973\n",
      "step 5923 loss: 2.4898297786712646\n",
      "step 5924 loss: 2.4762556552886963\n",
      "step 5925 loss: 2.5833587646484375\n",
      "step 5926 loss: 2.596904993057251\n",
      "step 5927 loss: 2.4930200576782227\n",
      "step 5928 loss: 2.467092990875244\n",
      "step 5929 loss: 2.5110530853271484\n",
      "step 5930 loss: 2.3863401412963867\n",
      "step 5931 loss: 2.6304991245269775\n",
      "step 5932 loss: 2.585491180419922\n",
      "step 5933 loss: 2.4833803176879883\n",
      "step 5934 loss: 2.4134700298309326\n",
      "step 5935 loss: 2.695767402648926\n",
      "step 5936 loss: 2.5035088062286377\n",
      "step 5937 loss: 2.566918134689331\n",
      "step 5938 loss: 2.4036972522735596\n",
      "step 5939 loss: 2.3811912536621094\n",
      "step 5940 loss: 2.4599149227142334\n",
      "step 5941 loss: 2.5137696266174316\n",
      "step 5942 loss: 2.571152687072754\n",
      "step 5943 loss: 2.4185070991516113\n",
      "step 5944 loss: 2.4493722915649414\n",
      "step 5945 loss: 2.5173094272613525\n",
      "step 5946 loss: 2.4018733501434326\n",
      "step 5947 loss: 2.4736037254333496\n",
      "step 5948 loss: 2.467217206954956\n",
      "step 5949 loss: 2.7061352729797363\n",
      "step 5950 loss: 2.399477005004883\n",
      "step 5951 loss: 2.484802722930908\n",
      "step 5952 loss: 2.486593246459961\n",
      "step 5953 loss: 2.4445154666900635\n",
      "step 5954 loss: 2.529372215270996\n",
      "step 5955 loss: 2.5113210678100586\n",
      "step 5956 loss: 2.4841971397399902\n",
      "step 5957 loss: 2.5601367950439453\n",
      "step 5958 loss: 2.5157790184020996\n",
      "step 5959 loss: 2.4652528762817383\n",
      "step 5960 loss: 2.4417688846588135\n",
      "step 5961 loss: 2.4392738342285156\n",
      "step 5962 loss: 2.5726733207702637\n",
      "step 5963 loss: 2.5946450233459473\n",
      "step 5964 loss: 2.4376180171966553\n",
      "step 5965 loss: 2.5433475971221924\n",
      "step 5966 loss: 2.622746229171753\n",
      "step 5967 loss: 2.4424643516540527\n",
      "step 5968 loss: 2.5422332286834717\n",
      "step 5969 loss: 2.500704526901245\n",
      "step 5970 loss: 2.605180501937866\n",
      "step 5971 loss: 2.416398286819458\n",
      "step 5972 loss: 2.58687424659729\n",
      "step 5973 loss: 2.4851832389831543\n",
      "step 5974 loss: 2.3960297107696533\n",
      "step 5975 loss: 2.4678118228912354\n",
      "step 5976 loss: 2.5135176181793213\n",
      "step 5977 loss: 2.382983684539795\n",
      "step 5978 loss: 2.581714153289795\n",
      "step 5979 loss: 2.6042826175689697\n",
      "step 5980 loss: 2.5004608631134033\n",
      "step 5981 loss: 2.423646926879883\n",
      "step 5982 loss: 2.4793663024902344\n",
      "step 5983 loss: 2.5010228157043457\n",
      "step 5984 loss: 2.5435659885406494\n",
      "step 5985 loss: 2.537321090698242\n",
      "step 5986 loss: 2.4466333389282227\n",
      "step 5987 loss: 2.588449001312256\n",
      "step 5988 loss: 2.4377286434173584\n",
      "step 5989 loss: 2.4051082134246826\n",
      "step 5990 loss: 2.5950801372528076\n",
      "step 5991 loss: 2.437718629837036\n",
      "step 5992 loss: 2.602156639099121\n",
      "step 5993 loss: 2.487889528274536\n",
      "step 5994 loss: 2.431506872177124\n",
      "step 5995 loss: 2.451504707336426\n",
      "step 5996 loss: 2.5232253074645996\n",
      "step 5997 loss: 2.4831438064575195\n",
      "step 5998 loss: 2.579334020614624\n",
      "step 5999 loss: 2.505293130874634\n",
      "step 6000 loss: 2.531585931777954\n",
      "step 6001 loss: 2.4789116382598877\n",
      "step 6002 loss: 2.495093584060669\n",
      "step 6003 loss: 2.484029769897461\n",
      "step 6004 loss: 2.595329999923706\n",
      "step 6005 loss: 2.5634779930114746\n",
      "step 6006 loss: 2.4396424293518066\n",
      "step 6007 loss: 2.4812169075012207\n",
      "step 6008 loss: 2.51702618598938\n",
      "step 6009 loss: 2.4927875995635986\n",
      "step 6010 loss: 2.546041488647461\n",
      "step 6011 loss: 2.625469207763672\n",
      "step 6012 loss: 2.4864070415496826\n",
      "step 6013 loss: 2.4013326168060303\n",
      "step 6014 loss: 2.4127681255340576\n",
      "step 6015 loss: 2.4904556274414062\n",
      "step 6016 loss: 2.4443862438201904\n",
      "step 6017 loss: 2.473536491394043\n",
      "step 6018 loss: 2.5318706035614014\n",
      "step 6019 loss: 2.501194953918457\n",
      "step 6020 loss: 2.6093742847442627\n",
      "step 6021 loss: 2.483757257461548\n",
      "step 6022 loss: 2.418365001678467\n",
      "step 6023 loss: 2.5380501747131348\n",
      "step 6024 loss: 2.4427647590637207\n",
      "step 6025 loss: 2.5958828926086426\n",
      "step 6026 loss: 2.538562536239624\n",
      "step 6027 loss: 2.496269464492798\n",
      "step 6028 loss: 2.4540584087371826\n",
      "step 6029 loss: 2.520857095718384\n",
      "step 6030 loss: 2.5533814430236816\n",
      "step 6031 loss: 2.5015103816986084\n",
      "step 6032 loss: 2.474217176437378\n",
      "step 6033 loss: 2.5621297359466553\n",
      "step 6034 loss: 2.6116831302642822\n",
      "step 6035 loss: 2.521357536315918\n",
      "step 6036 loss: 2.6571407318115234\n",
      "step 6037 loss: 2.5438919067382812\n",
      "step 6038 loss: 2.578200101852417\n",
      "step 6039 loss: 2.411607265472412\n",
      "step 6040 loss: 2.605712413787842\n",
      "step 6041 loss: 2.4381489753723145\n",
      "step 6042 loss: 2.4886960983276367\n",
      "step 6043 loss: 2.6023244857788086\n",
      "step 6044 loss: 2.4977023601531982\n",
      "step 6045 loss: 2.5380189418792725\n",
      "step 6046 loss: 2.5883026123046875\n",
      "step 6047 loss: 2.526499032974243\n",
      "step 6048 loss: 2.443692922592163\n",
      "step 6049 loss: 2.5992963314056396\n",
      "step 6050 loss: 2.548278570175171\n",
      "step 6051 loss: 2.4656872749328613\n",
      "step 6052 loss: 2.400663375854492\n",
      "step 6053 loss: 2.512023448944092\n",
      "step 6054 loss: 2.535372018814087\n",
      "step 6055 loss: 2.4654195308685303\n",
      "step 6056 loss: 2.5645782947540283\n",
      "step 6057 loss: 2.539933681488037\n",
      "step 6058 loss: 2.505706310272217\n",
      "step 6059 loss: 2.417569398880005\n",
      "step 6060 loss: 2.4653480052948\n",
      "step 6061 loss: 2.4613053798675537\n",
      "step 6062 loss: 2.4155919551849365\n",
      "step 6063 loss: 2.4972541332244873\n",
      "step 6064 loss: 2.4988021850585938\n",
      "step 6065 loss: 2.4524450302124023\n",
      "step 6066 loss: 2.3444089889526367\n",
      "step 6067 loss: 2.5738964080810547\n",
      "step 6068 loss: 2.4891281127929688\n",
      "step 6069 loss: 2.546799898147583\n",
      "step 6070 loss: 2.6240482330322266\n",
      "step 6071 loss: 2.5750529766082764\n",
      "step 6072 loss: 2.549633502960205\n",
      "step 6073 loss: 2.5177619457244873\n",
      "step 6074 loss: 2.5809881687164307\n",
      "step 6075 loss: 2.481738328933716\n",
      "step 6076 loss: 2.6443402767181396\n",
      "step 6077 loss: 2.4727728366851807\n",
      "step 6078 loss: 2.512693166732788\n",
      "step 6079 loss: 2.6530916690826416\n",
      "step 6080 loss: 2.5795419216156006\n",
      "step 6081 loss: 2.3849618434906006\n",
      "step 6082 loss: 2.5454938411712646\n",
      "step 6083 loss: 2.404278039932251\n",
      "step 6084 loss: 2.4825806617736816\n",
      "step 6085 loss: 2.470611095428467\n",
      "step 6086 loss: 2.3687405586242676\n",
      "step 6087 loss: 2.487921953201294\n",
      "step 6088 loss: 2.504244565963745\n",
      "step 6089 loss: 2.537994146347046\n",
      "step 6090 loss: 2.648005485534668\n",
      "step 6091 loss: 2.389371156692505\n",
      "step 6092 loss: 2.446516513824463\n",
      "step 6093 loss: 2.4917492866516113\n",
      "step 6094 loss: 2.5121519565582275\n",
      "step 6095 loss: 2.4854893684387207\n",
      "step 6096 loss: 2.618704080581665\n",
      "step 6097 loss: 2.4385874271392822\n",
      "step 6098 loss: 2.5636138916015625\n",
      "step 6099 loss: 2.385906219482422\n",
      "step 6100 loss: 2.497115135192871\n",
      "step 6101 loss: 2.5015971660614014\n",
      "step 6102 loss: 2.4890453815460205\n",
      "step 6103 loss: 2.5379765033721924\n",
      "step 6104 loss: 2.5020031929016113\n",
      "step 6105 loss: 2.4986400604248047\n",
      "step 6106 loss: 2.594099998474121\n",
      "step 6107 loss: 2.463707208633423\n",
      "step 6108 loss: 2.530423164367676\n",
      "step 6109 loss: 2.447152853012085\n",
      "step 6110 loss: 2.541973352432251\n",
      "step 6111 loss: 2.5078999996185303\n",
      "step 6112 loss: 2.4908547401428223\n",
      "step 6113 loss: 2.553884506225586\n",
      "step 6114 loss: 2.4415524005889893\n",
      "step 6115 loss: 2.5725460052490234\n",
      "step 6116 loss: 2.4964451789855957\n",
      "step 6117 loss: 2.5759899616241455\n",
      "step 6118 loss: 2.4043734073638916\n",
      "step 6119 loss: 2.4372971057891846\n",
      "step 6120 loss: 2.49367618560791\n",
      "step 6121 loss: 2.5636627674102783\n",
      "step 6122 loss: 2.6522274017333984\n",
      "step 6123 loss: 2.5363547801971436\n",
      "step 6124 loss: 2.51328182220459\n",
      "step 6125 loss: 2.4517745971679688\n",
      "step 6126 loss: 2.6142184734344482\n",
      "step 6127 loss: 2.5286777019500732\n",
      "step 6128 loss: 2.495919942855835\n",
      "step 6129 loss: 2.4752166271209717\n",
      "step 6130 loss: 2.583909749984741\n",
      "step 6131 loss: 2.4892287254333496\n",
      "step 6132 loss: 2.59078311920166\n",
      "step 6133 loss: 2.4798028469085693\n",
      "step 6134 loss: 2.535285234451294\n",
      "step 6135 loss: 2.549607515335083\n",
      "step 6136 loss: 2.4786040782928467\n",
      "step 6137 loss: 2.5695266723632812\n",
      "step 6138 loss: 2.4407315254211426\n",
      "step 6139 loss: 2.568624973297119\n",
      "step 6140 loss: 2.619753122329712\n",
      "step 6141 loss: 2.525174617767334\n",
      "step 6142 loss: 2.4956963062286377\n",
      "step 6143 loss: 2.5046839714050293\n",
      "step 6144 loss: 2.580488443374634\n",
      "step 6145 loss: 2.5932745933532715\n",
      "step 6146 loss: 2.4531948566436768\n",
      "step 6147 loss: 2.4918100833892822\n",
      "step 6148 loss: 2.5144095420837402\n",
      "step 6149 loss: 2.482522487640381\n",
      "step 6150 loss: 2.466801643371582\n",
      "step 6151 loss: 2.4905247688293457\n",
      "step 6152 loss: 2.508281707763672\n",
      "step 6153 loss: 2.5731775760650635\n",
      "step 6154 loss: 2.539555072784424\n",
      "step 6155 loss: 2.562147617340088\n",
      "step 6156 loss: 2.524258613586426\n",
      "step 6157 loss: 2.431222438812256\n",
      "step 6158 loss: 2.4574830532073975\n",
      "step 6159 loss: 2.5733513832092285\n",
      "step 6160 loss: 2.524704694747925\n",
      "step 6161 loss: 2.4619975090026855\n",
      "step 6162 loss: 2.4514994621276855\n",
      "step 6163 loss: 2.4593987464904785\n",
      "step 6164 loss: 2.4547271728515625\n",
      "step 6165 loss: 2.462938070297241\n",
      "step 6166 loss: 2.445021152496338\n",
      "step 6167 loss: 2.56019926071167\n",
      "step 6168 loss: 2.4855408668518066\n",
      "step 6169 loss: 2.582310914993286\n",
      "step 6170 loss: 2.5062434673309326\n",
      "step 6171 loss: 2.4554104804992676\n",
      "step 6172 loss: 2.481609582901001\n",
      "step 6173 loss: 2.5779881477355957\n",
      "step 6174 loss: 2.5454533100128174\n",
      "step 6175 loss: 2.5738344192504883\n",
      "step 6176 loss: 2.56194806098938\n",
      "step 6177 loss: 2.639073371887207\n",
      "step 6178 loss: 2.4777424335479736\n",
      "step 6179 loss: 2.600703239440918\n",
      "step 6180 loss: 2.6011829376220703\n",
      "step 6181 loss: 2.4424843788146973\n",
      "step 6182 loss: 2.5019032955169678\n",
      "step 6183 loss: 2.494432210922241\n",
      "step 6184 loss: 2.4222335815429688\n",
      "step 6185 loss: 2.3565618991851807\n",
      "step 6186 loss: 2.3846333026885986\n",
      "step 6187 loss: 2.582974433898926\n",
      "step 6188 loss: 2.49389386177063\n",
      "step 6189 loss: 2.399226427078247\n",
      "step 6190 loss: 2.4774374961853027\n",
      "step 6191 loss: 2.4245243072509766\n",
      "step 6192 loss: 2.469053268432617\n",
      "step 6193 loss: 2.4073257446289062\n",
      "step 6194 loss: 2.5266637802124023\n",
      "step 6195 loss: 2.3543848991394043\n",
      "step 6196 loss: 2.546213388442993\n",
      "step 6197 loss: 2.534367322921753\n",
      "step 6198 loss: 2.4838058948516846\n",
      "step 6199 loss: 2.5543012619018555\n",
      "step 6200 loss: 2.446481943130493\n",
      "step 6201 loss: 2.4645113945007324\n",
      "step 6202 loss: 2.4579994678497314\n",
      "step 6203 loss: 2.4687411785125732\n",
      "step 6204 loss: 2.5481245517730713\n",
      "step 6205 loss: 2.528428554534912\n",
      "step 6206 loss: 2.563728094100952\n",
      "step 6207 loss: 2.4614245891571045\n",
      "step 6208 loss: 2.3676300048828125\n",
      "step 6209 loss: 2.5137836933135986\n",
      "step 6210 loss: 2.4560019969940186\n",
      "step 6211 loss: 2.583265542984009\n",
      "step 6212 loss: 2.444733142852783\n",
      "step 6213 loss: 2.374422788619995\n",
      "step 6214 loss: 2.5387604236602783\n",
      "step 6215 loss: 2.37197208404541\n",
      "step 6216 loss: 2.475545644760132\n",
      "step 6217 loss: 2.5609445571899414\n",
      "step 6218 loss: 2.4388656616210938\n",
      "step 6219 loss: 2.497960090637207\n",
      "step 6220 loss: 2.4205482006073\n",
      "step 6221 loss: 2.6006274223327637\n",
      "step 6222 loss: 2.5231213569641113\n",
      "step 6223 loss: 2.483795166015625\n",
      "step 6224 loss: 2.4685864448547363\n",
      "step 6225 loss: 2.523326873779297\n",
      "step 6226 loss: 2.503075361251831\n",
      "step 6227 loss: 2.512646436691284\n",
      "step 6228 loss: 2.509392499923706\n",
      "step 6229 loss: 2.3927276134490967\n",
      "step 6230 loss: 2.5388476848602295\n",
      "step 6231 loss: 2.4668822288513184\n",
      "step 6232 loss: 2.500092029571533\n",
      "step 6233 loss: 2.4742352962493896\n",
      "step 6234 loss: 2.6064822673797607\n",
      "step 6235 loss: 2.384180784225464\n",
      "step 6236 loss: 2.4697492122650146\n",
      "step 6237 loss: 2.540992259979248\n",
      "step 6238 loss: 2.430988311767578\n",
      "step 6239 loss: 2.4454169273376465\n",
      "step 6240 loss: 2.4040446281433105\n",
      "step 6241 loss: 2.438958168029785\n",
      "step 6242 loss: 2.617788791656494\n",
      "step 6243 loss: 2.6258554458618164\n",
      "step 6244 loss: 2.502647876739502\n",
      "step 6245 loss: 2.53259539604187\n",
      "step 6246 loss: 2.499796152114868\n",
      "step 6247 loss: 2.5297675132751465\n",
      "step 6248 loss: 2.606624126434326\n",
      "step 6249 loss: 2.4397802352905273\n",
      "step 6250 loss: 2.6416501998901367\n",
      "step 6251 loss: 2.459563970565796\n",
      "step 6252 loss: 2.5379974842071533\n",
      "step 6253 loss: 2.473785400390625\n",
      "step 6254 loss: 2.521824836730957\n",
      "step 6255 loss: 2.3916144371032715\n",
      "step 6256 loss: 2.5263383388519287\n",
      "step 6257 loss: 2.7764029502868652\n",
      "step 6258 loss: 2.5405452251434326\n",
      "step 6259 loss: 2.456369161605835\n",
      "step 6260 loss: 2.580510139465332\n",
      "step 6261 loss: 2.3811140060424805\n",
      "step 6262 loss: 2.4892845153808594\n",
      "step 6263 loss: 2.522274971008301\n",
      "step 6264 loss: 2.418609857559204\n",
      "step 6265 loss: 2.5269534587860107\n",
      "step 6266 loss: 2.4528188705444336\n",
      "step 6267 loss: 2.5217349529266357\n",
      "step 6268 loss: 2.5472655296325684\n",
      "step 6269 loss: 2.4975907802581787\n",
      "step 6270 loss: 2.540919542312622\n",
      "step 6271 loss: 2.3995964527130127\n",
      "step 6272 loss: 2.441305160522461\n",
      "step 6273 loss: 2.5060596466064453\n",
      "step 6274 loss: 2.617685079574585\n",
      "step 6275 loss: 2.5374999046325684\n",
      "step 6276 loss: 2.437051296234131\n",
      "step 6277 loss: 2.4620399475097656\n",
      "step 6278 loss: 2.452070951461792\n",
      "step 6279 loss: 2.546229600906372\n",
      "step 6280 loss: 2.6262362003326416\n",
      "step 6281 loss: 2.3696815967559814\n",
      "step 6282 loss: 2.4659061431884766\n",
      "step 6283 loss: 2.602029800415039\n",
      "step 6284 loss: 2.5415291786193848\n",
      "step 6285 loss: 2.510385513305664\n",
      "step 6286 loss: 2.4727156162261963\n",
      "step 6287 loss: 2.5400428771972656\n",
      "step 6288 loss: 2.534557342529297\n",
      "step 6289 loss: 2.4036736488342285\n",
      "step 6290 loss: 2.5195822715759277\n",
      "step 6291 loss: 2.367164134979248\n",
      "step 6292 loss: 2.4802613258361816\n",
      "step 6293 loss: 2.3443331718444824\n",
      "step 6294 loss: 2.547752618789673\n",
      "step 6295 loss: 2.376587390899658\n",
      "step 6296 loss: 2.543225049972534\n",
      "step 6297 loss: 2.557220458984375\n",
      "step 6298 loss: 2.4559574127197266\n",
      "step 6299 loss: 2.535679578781128\n",
      "step 6300 loss: 2.4474494457244873\n",
      "step 6301 loss: 2.5779426097869873\n",
      "step 6302 loss: 2.57519268989563\n",
      "step 6303 loss: 2.4895200729370117\n",
      "step 6304 loss: 2.5605413913726807\n",
      "step 6305 loss: 2.638579845428467\n",
      "step 6306 loss: 2.538482189178467\n",
      "step 6307 loss: 2.5238585472106934\n",
      "step 6308 loss: 2.5484890937805176\n",
      "step 6309 loss: 2.44039249420166\n",
      "step 6310 loss: 2.5682284832000732\n",
      "step 6311 loss: 2.5263407230377197\n",
      "step 6312 loss: 2.588434934616089\n",
      "step 6313 loss: 2.5119378566741943\n",
      "step 6314 loss: 2.4681780338287354\n",
      "step 6315 loss: 2.476588249206543\n",
      "step 6316 loss: 2.4487714767456055\n",
      "step 6317 loss: 2.5165319442749023\n",
      "step 6318 loss: 2.6115100383758545\n",
      "step 6319 loss: 2.600294351577759\n",
      "step 6320 loss: 2.4757838249206543\n",
      "step 6321 loss: 2.565847873687744\n",
      "step 6322 loss: 2.5506603717803955\n",
      "step 6323 loss: 2.580335855484009\n",
      "step 6324 loss: 2.4517767429351807\n",
      "step 6325 loss: 2.4314804077148438\n",
      "step 6326 loss: 2.6461689472198486\n",
      "step 6327 loss: 2.53481125831604\n",
      "step 6328 loss: 2.4966881275177\n",
      "step 6329 loss: 2.4452457427978516\n",
      "step 6330 loss: 2.438798427581787\n",
      "step 6331 loss: 2.435978651046753\n",
      "step 6332 loss: 2.502774238586426\n",
      "step 6333 loss: 2.673780679702759\n",
      "step 6334 loss: 2.5219168663024902\n",
      "step 6335 loss: 2.557939052581787\n",
      "step 6336 loss: 2.545480966567993\n",
      "step 6337 loss: 2.5027685165405273\n",
      "step 6338 loss: 2.601410150527954\n",
      "step 6339 loss: 2.557213068008423\n",
      "step 6340 loss: 2.515730857849121\n",
      "step 6341 loss: 2.4885661602020264\n",
      "step 6342 loss: 2.5633134841918945\n",
      "step 6343 loss: 2.3859925270080566\n",
      "step 6344 loss: 2.6071407794952393\n",
      "step 6345 loss: 2.5758962631225586\n",
      "step 6346 loss: 2.4833719730377197\n",
      "step 6347 loss: 2.436847686767578\n",
      "step 6348 loss: 2.487257719039917\n",
      "step 6349 loss: 2.4625303745269775\n",
      "step 6350 loss: 2.566441774368286\n",
      "step 6351 loss: 2.6203255653381348\n",
      "step 6352 loss: 2.4705803394317627\n",
      "step 6353 loss: 2.477260112762451\n",
      "step 6354 loss: 2.6162843704223633\n",
      "step 6355 loss: 2.5111629962921143\n",
      "step 6356 loss: 2.495535373687744\n",
      "step 6357 loss: 2.4431064128875732\n",
      "step 6358 loss: 2.5025887489318848\n",
      "step 6359 loss: 2.5444490909576416\n",
      "step 6360 loss: 2.5729868412017822\n",
      "step 6361 loss: 2.561760187149048\n",
      "step 6362 loss: 2.509540557861328\n",
      "step 6363 loss: 2.6183888912200928\n",
      "step 6364 loss: 2.5524654388427734\n",
      "step 6365 loss: 2.6008589267730713\n",
      "step 6366 loss: 2.452413558959961\n",
      "step 6367 loss: 2.41599440574646\n",
      "step 6368 loss: 2.5800654888153076\n",
      "step 6369 loss: 2.5292375087738037\n",
      "step 6370 loss: 2.5867819786071777\n",
      "step 6371 loss: 2.449934959411621\n",
      "step 6372 loss: 2.747621536254883\n",
      "step 6373 loss: 2.4889955520629883\n",
      "step 6374 loss: 2.469998359680176\n",
      "step 6375 loss: 2.474349021911621\n",
      "step 6376 loss: 2.5219340324401855\n",
      "step 6377 loss: 2.562906503677368\n",
      "step 6378 loss: 2.431441068649292\n",
      "step 6379 loss: 2.4993820190429688\n",
      "step 6380 loss: 2.627732992172241\n",
      "step 6381 loss: 2.5569686889648438\n",
      "step 6382 loss: 2.4679172039031982\n",
      "step 6383 loss: 2.4964826107025146\n",
      "step 6384 loss: 2.5472354888916016\n",
      "step 6385 loss: 2.6759324073791504\n",
      "step 6386 loss: 2.4398534297943115\n",
      "step 6387 loss: 2.506436586380005\n",
      "step 6388 loss: 2.470038890838623\n",
      "step 6389 loss: 2.4667932987213135\n",
      "step 6390 loss: 2.5114243030548096\n",
      "step 6391 loss: 2.509855031967163\n",
      "step 6392 loss: 2.635382652282715\n",
      "step 6393 loss: 2.5887372493743896\n",
      "step 6394 loss: 2.4692723751068115\n",
      "step 6395 loss: 2.5477499961853027\n",
      "step 6396 loss: 2.5392403602600098\n",
      "step 6397 loss: 2.4802424907684326\n",
      "step 6398 loss: 2.433151960372925\n",
      "step 6399 loss: 2.591236114501953\n",
      "step 6400 loss: 2.5307929515838623\n",
      "step 6401 loss: 2.4452476501464844\n",
      "step 6402 loss: 2.606112003326416\n",
      "step 6403 loss: 2.5306038856506348\n",
      "step 6404 loss: 2.4134974479675293\n",
      "step 6405 loss: 2.494288682937622\n",
      "step 6406 loss: 2.639904737472534\n",
      "step 6407 loss: 2.570326328277588\n",
      "step 6408 loss: 2.486065626144409\n",
      "step 6409 loss: 2.5242509841918945\n",
      "step 6410 loss: 2.5446012020111084\n",
      "step 6411 loss: 2.5099260807037354\n",
      "step 6412 loss: 2.4895410537719727\n",
      "step 6413 loss: 2.550481081008911\n",
      "step 6414 loss: 2.5141828060150146\n",
      "step 6415 loss: 2.4547338485717773\n",
      "step 6416 loss: 2.4898085594177246\n",
      "step 6417 loss: 2.5187506675720215\n",
      "step 6418 loss: 2.5065321922302246\n",
      "step 6419 loss: 2.574289560317993\n",
      "step 6420 loss: 2.5279176235198975\n",
      "step 6421 loss: 2.534593343734741\n",
      "step 6422 loss: 2.5081591606140137\n",
      "step 6423 loss: 2.4430880546569824\n",
      "step 6424 loss: 2.4669153690338135\n",
      "step 6425 loss: 2.5641164779663086\n",
      "step 6426 loss: 2.452625036239624\n",
      "step 6427 loss: 2.592620611190796\n",
      "step 6428 loss: 2.452592611312866\n",
      "step 6429 loss: 2.502631425857544\n",
      "step 6430 loss: 2.4088923931121826\n",
      "step 6431 loss: 2.5192818641662598\n",
      "step 6432 loss: 2.503894090652466\n",
      "step 6433 loss: 2.5410313606262207\n",
      "step 6434 loss: 2.531742572784424\n",
      "step 6435 loss: 2.5512211322784424\n",
      "step 6436 loss: 2.5896682739257812\n",
      "step 6437 loss: 2.5115928649902344\n",
      "step 6438 loss: 2.4780495166778564\n",
      "step 6439 loss: 2.538646936416626\n",
      "step 6440 loss: 2.540738344192505\n",
      "step 6441 loss: 2.520895004272461\n",
      "step 6442 loss: 2.547102451324463\n",
      "step 6443 loss: 2.5770561695098877\n",
      "step 6444 loss: 2.4818785190582275\n",
      "step 6445 loss: 2.5252952575683594\n",
      "step 6446 loss: 2.519280195236206\n",
      "step 6447 loss: 2.4505510330200195\n",
      "step 6448 loss: 2.4029855728149414\n",
      "step 6449 loss: 2.5124781131744385\n",
      "step 6450 loss: 2.4851810932159424\n",
      "step 6451 loss: 2.5589182376861572\n",
      "step 6452 loss: 2.4944686889648438\n",
      "step 6453 loss: 2.5375044345855713\n",
      "step 6454 loss: 2.4567856788635254\n",
      "step 6455 loss: 2.514061212539673\n",
      "step 6456 loss: 2.493413209915161\n",
      "step 6457 loss: 2.4183998107910156\n",
      "step 6458 loss: 2.396291494369507\n",
      "step 6459 loss: 2.5803802013397217\n",
      "step 6460 loss: 2.646249771118164\n",
      "step 6461 loss: 2.507831573486328\n",
      "step 6462 loss: 2.498081684112549\n",
      "step 6463 loss: 2.59572696685791\n",
      "step 6464 loss: 2.4952306747436523\n",
      "step 6465 loss: 2.444276809692383\n",
      "step 6466 loss: 2.438974380493164\n",
      "step 6467 loss: 2.34375\n",
      "step 6468 loss: 2.4089016914367676\n",
      "step 6469 loss: 2.5009357929229736\n",
      "step 6470 loss: 2.6037697792053223\n",
      "step 6471 loss: 2.5261683464050293\n",
      "step 6472 loss: 2.464019298553467\n",
      "step 6473 loss: 2.534707546234131\n",
      "step 6474 loss: 2.4880993366241455\n",
      "step 6475 loss: 2.521677255630493\n",
      "step 6476 loss: 2.476473808288574\n",
      "step 6477 loss: 2.489508628845215\n",
      "step 6478 loss: 2.633209705352783\n",
      "step 6479 loss: 2.4872019290924072\n",
      "step 6480 loss: 2.502048969268799\n",
      "step 6481 loss: 2.381852626800537\n",
      "step 6482 loss: 2.669980764389038\n",
      "step 6483 loss: 2.397167205810547\n",
      "step 6484 loss: 2.5380702018737793\n",
      "step 6485 loss: 2.4593772888183594\n",
      "step 6486 loss: 2.6119742393493652\n",
      "step 6487 loss: 2.5743260383605957\n",
      "step 6488 loss: 2.4709861278533936\n",
      "step 6489 loss: 2.438631534576416\n",
      "step 6490 loss: 2.5653862953186035\n",
      "step 6491 loss: 2.4875504970550537\n",
      "step 6492 loss: 2.4340665340423584\n",
      "step 6493 loss: 2.4141809940338135\n",
      "step 6494 loss: 2.642427682876587\n",
      "step 6495 loss: 2.510908603668213\n",
      "step 6496 loss: 2.497945547103882\n",
      "step 6497 loss: 2.430443525314331\n",
      "step 6498 loss: 2.4686219692230225\n",
      "step 6499 loss: 2.5608952045440674\n",
      "step 6500 loss: 2.4708240032196045\n",
      "step 6501 loss: 2.6671862602233887\n",
      "step 6502 loss: 2.4082555770874023\n",
      "step 6503 loss: 2.509347677230835\n",
      "step 6504 loss: 2.5650994777679443\n",
      "step 6505 loss: 2.5582690238952637\n",
      "step 6506 loss: 2.4935550689697266\n",
      "step 6507 loss: 2.5503089427948\n",
      "step 6508 loss: 2.5310497283935547\n",
      "step 6509 loss: 2.430420398712158\n",
      "step 6510 loss: 2.541013240814209\n",
      "step 6511 loss: 2.414020299911499\n",
      "step 6512 loss: 2.4833486080169678\n",
      "step 6513 loss: 2.4637832641601562\n",
      "step 6514 loss: 2.4867606163024902\n",
      "step 6515 loss: 2.4788761138916016\n",
      "step 6516 loss: 2.4692187309265137\n",
      "step 6517 loss: 2.4024744033813477\n",
      "step 6518 loss: 2.6132543087005615\n",
      "step 6519 loss: 2.399949789047241\n",
      "step 6520 loss: 2.540160894393921\n",
      "step 6521 loss: 2.445521831512451\n",
      "step 6522 loss: 2.413849353790283\n",
      "step 6523 loss: 2.5141637325286865\n",
      "step 6524 loss: 2.484407663345337\n",
      "step 6525 loss: 2.574934720993042\n",
      "step 6526 loss: 2.5317885875701904\n",
      "step 6527 loss: 2.5525858402252197\n",
      "step 6528 loss: 2.438453435897827\n",
      "step 6529 loss: 2.460451602935791\n",
      "step 6530 loss: 2.576720714569092\n",
      "step 6531 loss: 2.4332501888275146\n",
      "step 6532 loss: 2.424875497817993\n",
      "step 6533 loss: 2.654287338256836\n",
      "step 6534 loss: 2.40238881111145\n",
      "step 6535 loss: 2.4030025005340576\n",
      "step 6536 loss: 2.4789981842041016\n",
      "step 6537 loss: 2.378789186477661\n",
      "step 6538 loss: 2.444204092025757\n",
      "step 6539 loss: 2.4774580001831055\n",
      "step 6540 loss: 2.4230782985687256\n",
      "step 6541 loss: 2.466614007949829\n",
      "step 6542 loss: 2.5611085891723633\n",
      "step 6543 loss: 2.495338201522827\n",
      "step 6544 loss: 2.494199514389038\n",
      "step 6545 loss: 2.5266451835632324\n",
      "step 6546 loss: 2.3620805740356445\n",
      "step 6547 loss: 2.5169734954833984\n",
      "step 6548 loss: 2.7269270420074463\n",
      "step 6549 loss: 2.459625005722046\n",
      "step 6550 loss: 2.5094804763793945\n",
      "step 6551 loss: 2.547577142715454\n",
      "step 6552 loss: 2.3926825523376465\n",
      "step 6553 loss: 2.57444167137146\n",
      "step 6554 loss: 2.4489777088165283\n",
      "step 6555 loss: 2.581596851348877\n",
      "step 6556 loss: 2.5603432655334473\n",
      "step 6557 loss: 2.5123939514160156\n",
      "step 6558 loss: 2.480151414871216\n",
      "step 6559 loss: 2.476130247116089\n",
      "step 6560 loss: 2.504368543624878\n",
      "step 6561 loss: 2.5113303661346436\n",
      "step 6562 loss: 2.543470621109009\n",
      "step 6563 loss: 2.4295310974121094\n",
      "step 6564 loss: 2.4843852519989014\n",
      "step 6565 loss: 2.401956081390381\n",
      "step 6566 loss: 2.4674041271209717\n",
      "step 6567 loss: 2.4863038063049316\n",
      "step 6568 loss: 2.485595464706421\n",
      "step 6569 loss: 2.472809314727783\n",
      "step 6570 loss: 2.402552366256714\n",
      "step 6571 loss: 2.51358699798584\n",
      "step 6572 loss: 2.5900557041168213\n",
      "step 6573 loss: 2.3507699966430664\n",
      "step 6574 loss: 2.4824283123016357\n",
      "step 6575 loss: 2.3497331142425537\n",
      "step 6576 loss: 2.5003130435943604\n",
      "step 6577 loss: 2.4700162410736084\n",
      "step 6578 loss: 2.517247200012207\n",
      "step 6579 loss: 2.453158140182495\n",
      "step 6580 loss: 2.323709726333618\n",
      "step 6581 loss: 2.652491331100464\n",
      "step 6582 loss: 2.38405442237854\n",
      "step 6583 loss: 2.5174872875213623\n",
      "step 6584 loss: 2.409759521484375\n",
      "step 6585 loss: 2.483290672302246\n",
      "step 6586 loss: 2.5074539184570312\n",
      "step 6587 loss: 2.4514048099517822\n",
      "step 6588 loss: 2.622117519378662\n",
      "step 6589 loss: 2.4342894554138184\n",
      "step 6590 loss: 2.502317190170288\n",
      "step 6591 loss: 2.579210042953491\n",
      "step 6592 loss: 2.445081949234009\n",
      "step 6593 loss: 2.464099645614624\n",
      "step 6594 loss: 2.5965545177459717\n",
      "step 6595 loss: 2.3907713890075684\n",
      "step 6596 loss: 2.3892433643341064\n",
      "step 6597 loss: 2.4405462741851807\n",
      "step 6598 loss: 2.4726932048797607\n",
      "step 6599 loss: 2.4884274005889893\n",
      "step 6600 loss: 2.5558059215545654\n",
      "step 6601 loss: 2.425776481628418\n",
      "step 6602 loss: 2.5785369873046875\n",
      "step 6603 loss: 2.499190092086792\n",
      "step 6604 loss: 2.532409191131592\n",
      "step 6605 loss: 2.554532289505005\n",
      "step 6606 loss: 2.5437426567077637\n",
      "step 6607 loss: 2.4883346557617188\n",
      "step 6608 loss: 2.4189224243164062\n",
      "step 6609 loss: 2.5738637447357178\n",
      "step 6610 loss: 2.536850690841675\n",
      "step 6611 loss: 2.4904139041900635\n",
      "step 6612 loss: 2.58520770072937\n",
      "step 6613 loss: 2.435006856918335\n",
      "step 6614 loss: 2.5238099098205566\n",
      "step 6615 loss: 2.5222220420837402\n",
      "step 6616 loss: 2.6776342391967773\n",
      "step 6617 loss: 2.421539068222046\n",
      "step 6618 loss: 2.513803243637085\n",
      "step 6619 loss: 2.5416433811187744\n",
      "step 6620 loss: 2.43693470954895\n",
      "step 6621 loss: 2.4448609352111816\n",
      "step 6622 loss: 2.6140599250793457\n",
      "step 6623 loss: 2.4164443016052246\n",
      "step 6624 loss: 2.2902495861053467\n",
      "step 6625 loss: 2.521667003631592\n",
      "step 6626 loss: 2.4410042762756348\n",
      "step 6627 loss: 2.449371099472046\n",
      "step 6628 loss: 2.5051281452178955\n",
      "step 6629 loss: 2.4514050483703613\n",
      "step 6630 loss: 2.5237090587615967\n",
      "step 6631 loss: 2.6454851627349854\n",
      "step 6632 loss: 2.5403332710266113\n",
      "step 6633 loss: 2.5346286296844482\n",
      "step 6634 loss: 2.555722236633301\n",
      "step 6635 loss: 2.457447052001953\n",
      "step 6636 loss: 2.4572701454162598\n",
      "step 6637 loss: 2.5072195529937744\n",
      "step 6638 loss: 2.4324471950531006\n",
      "step 6639 loss: 2.386021137237549\n",
      "step 6640 loss: 2.437650203704834\n",
      "step 6641 loss: 2.383967876434326\n",
      "step 6642 loss: 2.43976092338562\n",
      "step 6643 loss: 2.496593713760376\n",
      "step 6644 loss: 2.5786960124969482\n",
      "step 6645 loss: 2.4971461296081543\n",
      "step 6646 loss: 2.584155797958374\n",
      "step 6647 loss: 2.626405954360962\n",
      "step 6648 loss: 2.550626277923584\n",
      "step 6649 loss: 2.476165533065796\n",
      "step 6650 loss: 2.439581871032715\n",
      "step 6651 loss: 2.5374512672424316\n",
      "step 6652 loss: 2.439922571182251\n",
      "step 6653 loss: 2.385735511779785\n",
      "step 6654 loss: 2.375349998474121\n",
      "step 6655 loss: 2.555112361907959\n",
      "step 6656 loss: 2.5195350646972656\n",
      "step 6657 loss: 2.4303534030914307\n",
      "step 6658 loss: 2.5413315296173096\n",
      "step 6659 loss: 2.5356836318969727\n",
      "step 6660 loss: 2.477614402770996\n",
      "step 6661 loss: 2.5993154048919678\n",
      "step 6662 loss: 2.5770256519317627\n",
      "step 6663 loss: 2.5359365940093994\n",
      "step 6664 loss: 2.3175485134124756\n",
      "step 6665 loss: 2.514737129211426\n",
      "step 6666 loss: 2.513949394226074\n",
      "step 6667 loss: 2.48152756690979\n",
      "step 6668 loss: 2.496631622314453\n",
      "step 6669 loss: 2.580108642578125\n",
      "step 6670 loss: 2.3801498413085938\n",
      "step 6671 loss: 2.5136284828186035\n",
      "step 6672 loss: 2.465632438659668\n",
      "step 6673 loss: 2.406238317489624\n",
      "step 6674 loss: 2.476087808609009\n",
      "step 6675 loss: 2.613990545272827\n",
      "step 6676 loss: 2.6377623081207275\n",
      "step 6677 loss: 2.4802775382995605\n",
      "step 6678 loss: 2.5697643756866455\n",
      "step 6679 loss: 2.606550693511963\n",
      "step 6680 loss: 2.4820687770843506\n",
      "step 6681 loss: 2.443863868713379\n",
      "step 6682 loss: 2.4686973094940186\n",
      "step 6683 loss: 2.6202480792999268\n",
      "step 6684 loss: 2.4581942558288574\n",
      "step 6685 loss: 2.5082404613494873\n",
      "step 6686 loss: 2.602919101715088\n",
      "step 6687 loss: 2.515310525894165\n",
      "step 6688 loss: 2.5623905658721924\n",
      "step 6689 loss: 2.4904556274414062\n",
      "step 6690 loss: 2.521423101425171\n",
      "step 6691 loss: 2.4124441146850586\n",
      "step 6692 loss: 2.531432628631592\n",
      "step 6693 loss: 2.5289814472198486\n",
      "step 6694 loss: 2.2744243144989014\n",
      "step 6695 loss: 2.46968150138855\n",
      "step 6696 loss: 2.5733883380889893\n",
      "step 6697 loss: 2.6095125675201416\n",
      "step 6698 loss: 2.3845291137695312\n",
      "step 6699 loss: 2.5225906372070312\n",
      "step 6700 loss: 2.469895839691162\n",
      "step 6701 loss: 2.5595295429229736\n",
      "step 6702 loss: 2.4478890895843506\n",
      "step 6703 loss: 2.5151093006134033\n",
      "step 6704 loss: 2.462336778640747\n",
      "step 6705 loss: 2.440298080444336\n",
      "step 6706 loss: 2.442877769470215\n",
      "step 6707 loss: 2.4875500202178955\n",
      "step 6708 loss: 2.5480315685272217\n",
      "step 6709 loss: 2.4358112812042236\n",
      "step 6710 loss: 2.589860677719116\n",
      "step 6711 loss: 2.4506611824035645\n",
      "step 6712 loss: 2.5463345050811768\n",
      "step 6713 loss: 2.4417996406555176\n",
      "step 6714 loss: 2.5222692489624023\n",
      "step 6715 loss: 2.508655548095703\n",
      "step 6716 loss: 2.393648624420166\n",
      "step 6717 loss: 2.617504835128784\n",
      "step 6718 loss: 2.6738414764404297\n",
      "step 6719 loss: 2.5155491828918457\n",
      "step 6720 loss: 2.47455096244812\n",
      "step 6721 loss: 2.4793083667755127\n",
      "step 6722 loss: 2.441506862640381\n",
      "step 6723 loss: 2.484572410583496\n",
      "step 6724 loss: 2.490492820739746\n",
      "step 6725 loss: 2.615325927734375\n",
      "step 6726 loss: 2.464583158493042\n",
      "step 6727 loss: 2.4505224227905273\n",
      "step 6728 loss: 2.5768983364105225\n",
      "step 6729 loss: 2.475351333618164\n",
      "step 6730 loss: 2.506075859069824\n",
      "step 6731 loss: 2.6605441570281982\n",
      "step 6732 loss: 2.487880229949951\n",
      "step 6733 loss: 2.4499008655548096\n",
      "step 6734 loss: 2.526522159576416\n",
      "step 6735 loss: 2.398954391479492\n",
      "step 6736 loss: 2.522794008255005\n",
      "step 6737 loss: 2.3301968574523926\n",
      "step 6738 loss: 2.5842840671539307\n",
      "step 6739 loss: 2.5219945907592773\n",
      "step 6740 loss: 2.535303831100464\n",
      "step 6741 loss: 2.457465887069702\n",
      "step 6742 loss: 2.549982786178589\n",
      "step 6743 loss: 2.438138723373413\n",
      "step 6744 loss: 2.65853214263916\n",
      "step 6745 loss: 2.6881661415100098\n",
      "step 6746 loss: 2.685192584991455\n",
      "step 6747 loss: 2.5872738361358643\n",
      "step 6748 loss: 2.5088396072387695\n",
      "step 6749 loss: 2.5499167442321777\n",
      "step 6750 loss: 2.4975836277008057\n",
      "step 6751 loss: 2.505570888519287\n",
      "step 6752 loss: 2.400822877883911\n",
      "step 6753 loss: 2.5223002433776855\n",
      "step 6754 loss: 2.4934325218200684\n",
      "step 6755 loss: 2.406741142272949\n",
      "step 6756 loss: 2.5270166397094727\n",
      "step 6757 loss: 2.560872793197632\n",
      "step 6758 loss: 2.406825065612793\n",
      "step 6759 loss: 2.537858724594116\n",
      "step 6760 loss: 2.4901387691497803\n",
      "step 6761 loss: 2.4627997875213623\n",
      "step 6762 loss: 2.4074368476867676\n",
      "step 6763 loss: 2.5245137214660645\n",
      "step 6764 loss: 2.479100465774536\n",
      "step 6765 loss: 2.5644919872283936\n",
      "step 6766 loss: 2.53398060798645\n",
      "step 6767 loss: 2.524385452270508\n",
      "step 6768 loss: 2.464801788330078\n",
      "step 6769 loss: 2.4517135620117188\n",
      "step 6770 loss: 2.4732298851013184\n",
      "step 6771 loss: 2.396595001220703\n",
      "step 6772 loss: 2.4416794776916504\n",
      "step 6773 loss: 2.4510209560394287\n",
      "step 6774 loss: 2.460415840148926\n",
      "step 6775 loss: 2.3484389781951904\n",
      "step 6776 loss: 2.680241107940674\n",
      "step 6777 loss: 2.4856228828430176\n",
      "step 6778 loss: 2.5469911098480225\n",
      "step 6779 loss: 2.5040860176086426\n",
      "step 6780 loss: 2.6961498260498047\n",
      "step 6781 loss: 2.444157361984253\n",
      "step 6782 loss: 2.3937954902648926\n",
      "step 6783 loss: 2.5316929817199707\n",
      "step 6784 loss: 2.588310718536377\n",
      "step 6785 loss: 2.5331294536590576\n",
      "step 6786 loss: 2.5138750076293945\n",
      "step 6787 loss: 2.522336483001709\n",
      "step 6788 loss: 2.534048318862915\n",
      "step 6789 loss: 2.449936628341675\n",
      "step 6790 loss: 2.372354030609131\n",
      "step 6791 loss: 2.41748046875\n",
      "step 6792 loss: 2.5404043197631836\n",
      "step 6793 loss: 2.3911352157592773\n",
      "step 6794 loss: 2.555988073348999\n",
      "step 6795 loss: 2.5631039142608643\n",
      "step 6796 loss: 2.5899317264556885\n",
      "step 6797 loss: 2.4417693614959717\n",
      "step 6798 loss: 2.4763312339782715\n",
      "step 6799 loss: 2.531423568725586\n",
      "step 6800 loss: 2.605414867401123\n",
      "step 6801 loss: 2.424996852874756\n",
      "step 6802 loss: 2.5772078037261963\n",
      "step 6803 loss: 2.5023727416992188\n",
      "step 6804 loss: 2.4948432445526123\n",
      "step 6805 loss: 2.4240968227386475\n",
      "step 6806 loss: 2.4865572452545166\n",
      "step 6807 loss: 2.5120368003845215\n",
      "step 6808 loss: 2.468418836593628\n",
      "step 6809 loss: 2.4698009490966797\n",
      "step 6810 loss: 2.475809097290039\n",
      "step 6811 loss: 2.4178872108459473\n",
      "step 6812 loss: 2.3882434368133545\n",
      "step 6813 loss: 2.4028046131134033\n",
      "step 6814 loss: 2.5202314853668213\n",
      "step 6815 loss: 2.5220091342926025\n",
      "step 6816 loss: 2.466514825820923\n",
      "step 6817 loss: 2.5565319061279297\n",
      "step 6818 loss: 2.5679891109466553\n",
      "step 6819 loss: 2.4411492347717285\n",
      "step 6820 loss: 2.461688280105591\n",
      "step 6821 loss: 2.544565439224243\n",
      "step 6822 loss: 2.443538188934326\n",
      "step 6823 loss: 2.4910359382629395\n",
      "step 6824 loss: 2.5154895782470703\n",
      "step 6825 loss: 2.5721967220306396\n",
      "step 6826 loss: 2.386932134628296\n",
      "step 6827 loss: 2.519369125366211\n",
      "step 6828 loss: 2.4757423400878906\n",
      "step 6829 loss: 2.588153839111328\n",
      "step 6830 loss: 2.4459803104400635\n",
      "step 6831 loss: 2.5213840007781982\n",
      "step 6832 loss: 2.3652496337890625\n",
      "step 6833 loss: 2.5350165367126465\n",
      "step 6834 loss: 2.4211511611938477\n",
      "step 6835 loss: 2.533867835998535\n",
      "step 6836 loss: 2.4744749069213867\n",
      "step 6837 loss: 2.5769307613372803\n",
      "step 6838 loss: 2.4611358642578125\n",
      "step 6839 loss: 2.5453903675079346\n",
      "step 6840 loss: 2.517441749572754\n",
      "step 6841 loss: 2.452258825302124\n",
      "step 6842 loss: 2.5745301246643066\n",
      "step 6843 loss: 2.5533392429351807\n",
      "step 6844 loss: 2.5860698223114014\n",
      "step 6845 loss: 2.4155561923980713\n",
      "step 6846 loss: 2.6049797534942627\n",
      "step 6847 loss: 2.5376110076904297\n",
      "step 6848 loss: 2.505394697189331\n",
      "step 6849 loss: 2.4896137714385986\n",
      "step 6850 loss: 2.587484836578369\n",
      "step 6851 loss: 2.612311840057373\n",
      "step 6852 loss: 2.501769542694092\n",
      "step 6853 loss: 2.548273801803589\n",
      "step 6854 loss: 2.45459246635437\n",
      "step 6855 loss: 2.407407760620117\n",
      "step 6856 loss: 2.400369167327881\n",
      "step 6857 loss: 2.578028440475464\n",
      "step 6858 loss: 2.348703384399414\n",
      "step 6859 loss: 2.5385749340057373\n",
      "step 6860 loss: 2.469733238220215\n",
      "step 6861 loss: 2.40077805519104\n",
      "step 6862 loss: 2.6349124908447266\n",
      "step 6863 loss: 2.459263801574707\n",
      "step 6864 loss: 2.444462537765503\n",
      "step 6865 loss: 2.5668230056762695\n",
      "step 6866 loss: 2.5138001441955566\n",
      "step 6867 loss: 2.4434709548950195\n",
      "step 6868 loss: 2.5389890670776367\n",
      "step 6869 loss: 2.4374730587005615\n",
      "step 6870 loss: 2.4781148433685303\n",
      "step 6871 loss: 2.4662277698516846\n",
      "step 6872 loss: 2.7227468490600586\n",
      "step 6873 loss: 2.4954962730407715\n",
      "step 6874 loss: 2.544323682785034\n",
      "step 6875 loss: 2.5015923976898193\n",
      "step 6876 loss: 2.5768086910247803\n",
      "step 6877 loss: 2.634688138961792\n",
      "step 6878 loss: 2.5127456188201904\n",
      "step 6879 loss: 2.580326557159424\n",
      "step 6880 loss: 2.3355600833892822\n",
      "step 6881 loss: 2.5287227630615234\n",
      "step 6882 loss: 2.525385618209839\n",
      "step 6883 loss: 2.54957914352417\n",
      "step 6884 loss: 2.5504751205444336\n",
      "step 6885 loss: 2.479816436767578\n",
      "step 6886 loss: 2.441077709197998\n",
      "step 6887 loss: 2.515023708343506\n",
      "step 6888 loss: 2.5986549854278564\n",
      "step 6889 loss: 2.519350051879883\n",
      "step 6890 loss: 2.543111562728882\n",
      "step 6891 loss: 2.5332772731781006\n",
      "step 6892 loss: 2.534708023071289\n",
      "step 6893 loss: 2.431140184402466\n",
      "step 6894 loss: 2.4252898693084717\n",
      "step 6895 loss: 2.385809898376465\n",
      "step 6896 loss: 2.4109272956848145\n",
      "step 6897 loss: 2.577157735824585\n",
      "step 6898 loss: 2.467451810836792\n",
      "step 6899 loss: 2.484015464782715\n",
      "step 6900 loss: 2.570693254470825\n",
      "step 6901 loss: 2.524343967437744\n",
      "step 6902 loss: 2.463649034500122\n",
      "step 6903 loss: 2.343674659729004\n",
      "step 6904 loss: 2.6309149265289307\n",
      "step 6905 loss: 2.488471746444702\n",
      "step 6906 loss: 2.5923964977264404\n",
      "step 6907 loss: 2.3420681953430176\n",
      "step 6908 loss: 2.5073158740997314\n",
      "step 6909 loss: 2.5094454288482666\n",
      "step 6910 loss: 2.4630374908447266\n",
      "step 6911 loss: 2.5407397747039795\n",
      "step 6912 loss: 2.4994559288024902\n",
      "step 6913 loss: 2.4686803817749023\n",
      "step 6914 loss: 2.431842803955078\n",
      "step 6915 loss: 2.5084218978881836\n",
      "step 6916 loss: 2.4211087226867676\n",
      "step 6917 loss: 2.4805123805999756\n",
      "step 6918 loss: 2.386638641357422\n",
      "step 6919 loss: 2.5003952980041504\n",
      "step 6920 loss: 2.448723316192627\n",
      "step 6921 loss: 2.516376495361328\n",
      "step 6922 loss: 2.4804818630218506\n",
      "step 6923 loss: 2.5299837589263916\n",
      "step 6924 loss: 2.4952776432037354\n",
      "step 6925 loss: 2.5686423778533936\n",
      "step 6926 loss: 2.5297751426696777\n",
      "step 6927 loss: 2.4847476482391357\n",
      "step 6928 loss: 2.457780122756958\n",
      "step 6929 loss: 2.4451005458831787\n",
      "step 6930 loss: 2.4137516021728516\n",
      "step 6931 loss: 2.4464850425720215\n",
      "step 6932 loss: 2.439370632171631\n",
      "step 6933 loss: 2.5014944076538086\n",
      "step 6934 loss: 2.4743194580078125\n",
      "step 6935 loss: 2.404076337814331\n",
      "step 6936 loss: 2.435739040374756\n",
      "step 6937 loss: 2.3773891925811768\n",
      "step 6938 loss: 2.569329261779785\n",
      "step 6939 loss: 2.559593439102173\n",
      "step 6940 loss: 2.545107126235962\n",
      "step 6941 loss: 2.6656548976898193\n",
      "step 6942 loss: 2.5518460273742676\n",
      "step 6943 loss: 2.538935899734497\n",
      "step 6944 loss: 2.3662853240966797\n",
      "step 6945 loss: 2.5861027240753174\n",
      "step 6946 loss: 2.4566211700439453\n",
      "step 6947 loss: 2.4524478912353516\n",
      "step 6948 loss: 2.4899587631225586\n",
      "step 6949 loss: 2.518099069595337\n",
      "step 6950 loss: 2.5365798473358154\n",
      "step 6951 loss: 2.4444832801818848\n",
      "step 6952 loss: 2.5155069828033447\n",
      "step 6953 loss: 2.5707154273986816\n",
      "step 6954 loss: 2.607012987136841\n",
      "step 6955 loss: 2.5505924224853516\n",
      "step 6956 loss: 2.5978801250457764\n",
      "step 6957 loss: 2.572782039642334\n",
      "step 6958 loss: 2.4675276279449463\n",
      "step 6959 loss: 2.451451063156128\n",
      "step 6960 loss: 2.4689881801605225\n",
      "step 6961 loss: 2.517803430557251\n",
      "step 6962 loss: 2.501099109649658\n",
      "step 6963 loss: 2.5108871459960938\n",
      "step 6964 loss: 2.5964748859405518\n",
      "step 6965 loss: 2.466801643371582\n",
      "step 6966 loss: 2.6325769424438477\n",
      "step 6967 loss: 2.480379581451416\n",
      "step 6968 loss: 2.301682949066162\n",
      "step 6969 loss: 2.5355618000030518\n",
      "step 6970 loss: 2.471905469894409\n",
      "step 6971 loss: 2.377540111541748\n",
      "step 6972 loss: 2.509676933288574\n",
      "step 6973 loss: 2.475414276123047\n",
      "step 6974 loss: 2.604670763015747\n",
      "step 6975 loss: 2.5342485904693604\n",
      "step 6976 loss: 2.5107789039611816\n",
      "step 6977 loss: 2.534655809402466\n",
      "step 6978 loss: 2.5077764987945557\n",
      "step 6979 loss: 2.588287353515625\n",
      "step 6980 loss: 2.4187517166137695\n",
      "step 6981 loss: 2.557060480117798\n",
      "step 6982 loss: 2.4925119876861572\n",
      "step 6983 loss: 2.486093759536743\n",
      "step 6984 loss: 2.522494077682495\n",
      "step 6985 loss: 2.5878746509552\n",
      "step 6986 loss: 2.5043177604675293\n",
      "step 6987 loss: 2.5256335735321045\n",
      "step 6988 loss: 2.472862720489502\n",
      "step 6989 loss: 2.4851255416870117\n",
      "step 6990 loss: 2.5614688396453857\n",
      "step 6991 loss: 2.479053497314453\n",
      "step 6992 loss: 2.530973196029663\n",
      "step 6993 loss: 2.507194757461548\n",
      "step 6994 loss: 2.5468595027923584\n",
      "step 6995 loss: 2.5904877185821533\n",
      "step 6996 loss: 2.5476911067962646\n",
      "step 6997 loss: 2.43389630317688\n",
      "step 6998 loss: 2.499394655227661\n",
      "step 6999 loss: 2.517559051513672\n",
      "step 7000 loss: 2.504757881164551\n",
      "step 7001 loss: 2.4312140941619873\n",
      "step 7002 loss: 2.4917232990264893\n",
      "step 7003 loss: 2.493942975997925\n",
      "step 7004 loss: 2.6464040279388428\n",
      "step 7005 loss: 2.4273295402526855\n",
      "step 7006 loss: 2.396103620529175\n",
      "step 7007 loss: 2.5201401710510254\n",
      "step 7008 loss: 2.564143657684326\n",
      "step 7009 loss: 2.449270009994507\n",
      "step 7010 loss: 2.459980010986328\n",
      "step 7011 loss: 2.5120232105255127\n",
      "step 7012 loss: 2.446530818939209\n",
      "step 7013 loss: 2.48508358001709\n",
      "step 7014 loss: 2.4987003803253174\n",
      "step 7015 loss: 2.420501232147217\n",
      "step 7016 loss: 2.494936943054199\n",
      "step 7017 loss: 2.493778944015503\n",
      "step 7018 loss: 2.4213473796844482\n",
      "step 7019 loss: 2.6276166439056396\n",
      "step 7020 loss: 2.344341516494751\n",
      "step 7021 loss: 2.5178916454315186\n",
      "step 7022 loss: 2.554939031600952\n",
      "step 7023 loss: 2.3889153003692627\n",
      "step 7024 loss: 2.558351993560791\n",
      "step 7025 loss: 2.5615298748016357\n",
      "step 7026 loss: 2.4417786598205566\n",
      "step 7027 loss: 2.5090513229370117\n",
      "step 7028 loss: 2.5827536582946777\n",
      "step 7029 loss: 2.549887180328369\n",
      "step 7030 loss: 2.505934238433838\n",
      "step 7031 loss: 2.514432668685913\n",
      "step 7032 loss: 2.441650390625\n",
      "step 7033 loss: 2.4678051471710205\n",
      "step 7034 loss: 2.572606325149536\n",
      "step 7035 loss: 2.370337724685669\n",
      "step 7036 loss: 2.63696551322937\n",
      "step 7037 loss: 2.519526243209839\n",
      "step 7038 loss: 2.4584109783172607\n",
      "step 7039 loss: 2.551126480102539\n",
      "step 7040 loss: 2.433948278427124\n",
      "step 7041 loss: 2.609389543533325\n",
      "step 7042 loss: 2.302676200866699\n",
      "step 7043 loss: 2.448615550994873\n",
      "step 7044 loss: 2.5022928714752197\n",
      "step 7045 loss: 2.629936456680298\n",
      "step 7046 loss: 2.6217007637023926\n",
      "step 7047 loss: 2.547805070877075\n",
      "step 7048 loss: 2.4779880046844482\n",
      "step 7049 loss: 2.482851028442383\n",
      "step 7050 loss: 2.418731689453125\n",
      "step 7051 loss: 2.56552791595459\n",
      "step 7052 loss: 2.4267497062683105\n",
      "step 7053 loss: 2.533416748046875\n",
      "step 7054 loss: 2.5252137184143066\n",
      "step 7055 loss: 2.5482826232910156\n",
      "step 7056 loss: 2.4496066570281982\n",
      "step 7057 loss: 2.4660215377807617\n",
      "step 7058 loss: 2.4360814094543457\n",
      "step 7059 loss: 2.5128109455108643\n",
      "step 7060 loss: 2.665731191635132\n",
      "step 7061 loss: 2.429279088973999\n",
      "step 7062 loss: 2.46474027633667\n",
      "step 7063 loss: 2.5419297218322754\n",
      "step 7064 loss: 2.5131776332855225\n",
      "step 7065 loss: 2.537966251373291\n",
      "step 7066 loss: 2.5181736946105957\n",
      "step 7067 loss: 2.504798173904419\n",
      "step 7068 loss: 2.4442660808563232\n",
      "step 7069 loss: 2.4340877532958984\n",
      "step 7070 loss: 2.5982606410980225\n",
      "step 7071 loss: 2.469937324523926\n",
      "step 7072 loss: 2.514915943145752\n",
      "step 7073 loss: 2.4527740478515625\n",
      "step 7074 loss: 2.393874168395996\n",
      "step 7075 loss: 2.527083158493042\n",
      "step 7076 loss: 2.627660036087036\n",
      "step 7077 loss: 2.39792799949646\n",
      "step 7078 loss: 2.4600489139556885\n",
      "step 7079 loss: 2.4445595741271973\n",
      "step 7080 loss: 2.4563372135162354\n",
      "step 7081 loss: 2.429936647415161\n",
      "step 7082 loss: 2.4711430072784424\n",
      "step 7083 loss: 2.4909024238586426\n",
      "step 7084 loss: 2.5808520317077637\n",
      "step 7085 loss: 2.4813544750213623\n",
      "step 7086 loss: 2.4587514400482178\n",
      "step 7087 loss: 2.5152299404144287\n",
      "step 7088 loss: 2.5776984691619873\n",
      "step 7089 loss: 2.4195470809936523\n",
      "step 7090 loss: 2.453223466873169\n",
      "step 7091 loss: 2.5257530212402344\n",
      "step 7092 loss: 2.441740036010742\n",
      "step 7093 loss: 2.4658210277557373\n",
      "step 7094 loss: 2.5108864307403564\n",
      "step 7095 loss: 2.6275763511657715\n",
      "step 7096 loss: 2.534929037094116\n",
      "step 7097 loss: 2.5371429920196533\n",
      "step 7098 loss: 2.4910736083984375\n",
      "step 7099 loss: 2.4341986179351807\n",
      "step 7100 loss: 2.444789171218872\n",
      "step 7101 loss: 2.455197811126709\n",
      "step 7102 loss: 2.450674533843994\n",
      "step 7103 loss: 2.4245362281799316\n",
      "step 7104 loss: 2.5828700065612793\n",
      "step 7105 loss: 2.5193002223968506\n",
      "step 7106 loss: 2.486428737640381\n",
      "step 7107 loss: 2.523775100708008\n",
      "step 7108 loss: 2.390972852706909\n",
      "step 7109 loss: 2.6554746627807617\n",
      "step 7110 loss: 2.556262493133545\n",
      "step 7111 loss: 2.4248907566070557\n",
      "step 7112 loss: 2.475607395172119\n",
      "step 7113 loss: 2.6038401126861572\n",
      "step 7114 loss: 2.534221649169922\n",
      "step 7115 loss: 2.4623825550079346\n",
      "step 7116 loss: 2.529766082763672\n",
      "step 7117 loss: 2.5019006729125977\n",
      "step 7118 loss: 2.5418899059295654\n",
      "step 7119 loss: 2.4586615562438965\n",
      "step 7120 loss: 2.4233250617980957\n",
      "step 7121 loss: 2.5499916076660156\n",
      "step 7122 loss: 2.560964584350586\n",
      "step 7123 loss: 2.4813754558563232\n",
      "step 7124 loss: 2.484792470932007\n",
      "step 7125 loss: 2.6287271976470947\n",
      "step 7126 loss: 2.465372085571289\n",
      "step 7127 loss: 2.4437713623046875\n",
      "step 7128 loss: 2.431469678878784\n",
      "step 7129 loss: 2.4119632244110107\n",
      "step 7130 loss: 2.5700290203094482\n",
      "step 7131 loss: 2.454331874847412\n",
      "step 7132 loss: 2.4398584365844727\n",
      "step 7133 loss: 2.4177072048187256\n",
      "step 7134 loss: 2.509141683578491\n",
      "step 7135 loss: 2.4550509452819824\n",
      "step 7136 loss: 2.5866096019744873\n",
      "step 7137 loss: 2.4401910305023193\n",
      "step 7138 loss: 2.381201982498169\n",
      "step 7139 loss: 2.435957193374634\n",
      "step 7140 loss: 2.5447497367858887\n",
      "step 7141 loss: 2.5411624908447266\n",
      "step 7142 loss: 2.4791078567504883\n",
      "step 7143 loss: 2.4929299354553223\n",
      "step 7144 loss: 2.416667938232422\n",
      "step 7145 loss: 2.4796454906463623\n",
      "step 7146 loss: 2.48350191116333\n",
      "step 7147 loss: 2.428739309310913\n",
      "step 7148 loss: 2.4917514324188232\n",
      "step 7149 loss: 2.502107858657837\n",
      "step 7150 loss: 2.454188108444214\n",
      "step 7151 loss: 2.4264540672302246\n",
      "step 7152 loss: 2.57456111907959\n",
      "step 7153 loss: 2.615309476852417\n",
      "step 7154 loss: 2.454253911972046\n",
      "step 7155 loss: 2.561098575592041\n",
      "step 7156 loss: 2.43316912651062\n",
      "step 7157 loss: 2.4233365058898926\n",
      "step 7158 loss: 2.4235317707061768\n",
      "step 7159 loss: 2.484408140182495\n",
      "step 7160 loss: 2.477626323699951\n",
      "step 7161 loss: 2.5376691818237305\n",
      "step 7162 loss: 2.459172248840332\n",
      "step 7163 loss: 2.440607786178589\n",
      "step 7164 loss: 2.5182316303253174\n",
      "step 7165 loss: 2.446584939956665\n",
      "step 7166 loss: 2.4607880115509033\n",
      "step 7167 loss: 2.4519541263580322\n",
      "step 7168 loss: 2.5384140014648438\n",
      "step 7169 loss: 2.581480026245117\n",
      "step 7170 loss: 2.4037747383117676\n",
      "step 7171 loss: 2.54417085647583\n",
      "step 7172 loss: 2.47080397605896\n",
      "step 7173 loss: 2.5048539638519287\n",
      "step 7174 loss: 2.5546000003814697\n",
      "step 7175 loss: 2.2742226123809814\n",
      "step 7176 loss: 2.5739951133728027\n",
      "step 7177 loss: 2.4328887462615967\n",
      "step 7178 loss: 2.5608277320861816\n",
      "step 7179 loss: 2.5162434577941895\n",
      "step 7180 loss: 2.6921114921569824\n",
      "step 7181 loss: 2.473492383956909\n",
      "step 7182 loss: 2.547407627105713\n",
      "step 7183 loss: 2.5355100631713867\n",
      "step 7184 loss: 2.432279109954834\n",
      "step 7185 loss: 2.4452016353607178\n",
      "step 7186 loss: 2.4574451446533203\n",
      "step 7187 loss: 2.513011932373047\n",
      "step 7188 loss: 2.455928087234497\n",
      "step 7189 loss: 2.4163830280303955\n",
      "step 7190 loss: 2.4458518028259277\n",
      "step 7191 loss: 2.587125778198242\n",
      "step 7192 loss: 2.549565315246582\n",
      "step 7193 loss: 2.4541828632354736\n",
      "step 7194 loss: 2.515643835067749\n",
      "step 7195 loss: 2.4892070293426514\n",
      "step 7196 loss: 2.570420742034912\n",
      "step 7197 loss: 2.521372079849243\n",
      "step 7198 loss: 2.4817042350769043\n",
      "step 7199 loss: 2.5682320594787598\n",
      "step 7200 loss: 2.5223405361175537\n",
      "step 7201 loss: 2.42319655418396\n",
      "step 7202 loss: 2.4893863201141357\n",
      "step 7203 loss: 2.670083999633789\n",
      "step 7204 loss: 2.4317708015441895\n",
      "step 7205 loss: 2.5078773498535156\n",
      "step 7206 loss: 2.536931276321411\n",
      "step 7207 loss: 2.524660587310791\n",
      "step 7208 loss: 2.5441222190856934\n",
      "step 7209 loss: 2.4752869606018066\n",
      "step 7210 loss: 2.525803565979004\n",
      "step 7211 loss: 2.578275442123413\n",
      "step 7212 loss: 2.416306734085083\n",
      "step 7213 loss: 2.5578536987304688\n",
      "step 7214 loss: 2.4679887294769287\n",
      "step 7215 loss: 2.598578929901123\n",
      "step 7216 loss: 2.3405723571777344\n",
      "step 7217 loss: 2.369950294494629\n",
      "step 7218 loss: 2.4817795753479004\n",
      "step 7219 loss: 2.5001049041748047\n",
      "step 7220 loss: 2.567922830581665\n",
      "step 7221 loss: 2.500537633895874\n",
      "step 7222 loss: 2.488630771636963\n",
      "step 7223 loss: 2.487640380859375\n",
      "step 7224 loss: 2.4697518348693848\n",
      "step 7225 loss: 2.5095229148864746\n",
      "step 7226 loss: 2.5459518432617188\n",
      "step 7227 loss: 2.437455415725708\n",
      "step 7228 loss: 2.5055336952209473\n",
      "step 7229 loss: 2.5676074028015137\n",
      "step 7230 loss: 2.4471523761749268\n",
      "step 7231 loss: 2.5011658668518066\n",
      "step 7232 loss: 2.492266893386841\n",
      "step 7233 loss: 2.5329606533050537\n",
      "step 7234 loss: 2.666586399078369\n",
      "step 7235 loss: 2.6447415351867676\n",
      "step 7236 loss: 2.504427909851074\n",
      "step 7237 loss: 2.4511094093322754\n",
      "step 7238 loss: 2.517025947570801\n",
      "step 7239 loss: 2.5869908332824707\n",
      "step 7240 loss: 2.536139726638794\n",
      "step 7241 loss: 2.4959475994110107\n",
      "step 7242 loss: 2.4947290420532227\n",
      "step 7243 loss: 2.4449462890625\n",
      "step 7244 loss: 2.496250629425049\n",
      "step 7245 loss: 2.4637136459350586\n",
      "step 7246 loss: 2.547999143600464\n",
      "step 7247 loss: 2.526228666305542\n",
      "step 7248 loss: 2.5150699615478516\n",
      "step 7249 loss: 2.564154863357544\n",
      "step 7250 loss: 2.6909632682800293\n",
      "step 7251 loss: 2.5665159225463867\n",
      "step 7252 loss: 2.515794277191162\n",
      "step 7253 loss: 2.429344892501831\n",
      "step 7254 loss: 2.5156850814819336\n",
      "step 7255 loss: 2.524132251739502\n",
      "step 7256 loss: 2.442208766937256\n",
      "step 7257 loss: 2.4185428619384766\n",
      "step 7258 loss: 2.575894832611084\n",
      "step 7259 loss: 2.5406343936920166\n",
      "step 7260 loss: 2.435565948486328\n",
      "step 7261 loss: 2.4515438079833984\n",
      "step 7262 loss: 2.4383018016815186\n",
      "step 7263 loss: 2.4468610286712646\n",
      "step 7264 loss: 2.4389684200286865\n",
      "step 7265 loss: 2.586024045944214\n",
      "step 7266 loss: 2.531651496887207\n",
      "step 7267 loss: 2.531611442565918\n",
      "step 7268 loss: 2.5231196880340576\n",
      "step 7269 loss: 2.5355570316314697\n",
      "step 7270 loss: 2.5949718952178955\n",
      "step 7271 loss: 2.4071743488311768\n",
      "step 7272 loss: 2.5308995246887207\n",
      "step 7273 loss: 2.467928409576416\n",
      "step 7274 loss: 2.5231752395629883\n",
      "step 7275 loss: 2.464191436767578\n",
      "step 7276 loss: 2.4213902950286865\n",
      "step 7277 loss: 2.4025821685791016\n",
      "step 7278 loss: 2.4205684661865234\n",
      "step 7279 loss: 2.48927903175354\n",
      "step 7280 loss: 2.4733810424804688\n",
      "step 7281 loss: 2.5776021480560303\n",
      "step 7282 loss: 2.3862361907958984\n",
      "step 7283 loss: 2.4826910495758057\n",
      "step 7284 loss: 2.5583343505859375\n",
      "step 7285 loss: 2.475139617919922\n",
      "step 7286 loss: 2.6503281593322754\n",
      "step 7287 loss: 2.5916900634765625\n",
      "step 7288 loss: 2.536780595779419\n",
      "step 7289 loss: 2.5213780403137207\n",
      "step 7290 loss: 2.521597146987915\n",
      "step 7291 loss: 2.46828556060791\n",
      "step 7292 loss: 2.5031163692474365\n",
      "step 7293 loss: 2.5116488933563232\n",
      "step 7294 loss: 2.4373912811279297\n",
      "step 7295 loss: 2.519611358642578\n",
      "step 7296 loss: 2.4980053901672363\n",
      "step 7297 loss: 2.512432336807251\n",
      "step 7298 loss: 2.449643850326538\n",
      "step 7299 loss: 2.4427897930145264\n",
      "step 7300 loss: 2.4506444931030273\n",
      "step 7301 loss: 2.4243576526641846\n",
      "step 7302 loss: 2.364696502685547\n",
      "step 7303 loss: 2.3971681594848633\n",
      "step 7304 loss: 2.51182222366333\n",
      "step 7305 loss: 2.4347305297851562\n",
      "step 7306 loss: 2.485410451889038\n",
      "step 7307 loss: 2.457202672958374\n",
      "step 7308 loss: 2.5134427547454834\n",
      "step 7309 loss: 2.3866732120513916\n",
      "step 7310 loss: 2.4963653087615967\n",
      "step 7311 loss: 2.6485705375671387\n",
      "step 7312 loss: 2.46978759765625\n",
      "step 7313 loss: 2.4921352863311768\n",
      "step 7314 loss: 2.5573418140411377\n",
      "step 7315 loss: 2.4567391872406006\n",
      "step 7316 loss: 2.4596967697143555\n",
      "step 7317 loss: 2.6658878326416016\n",
      "step 7318 loss: 2.641005516052246\n",
      "step 7319 loss: 2.5244736671447754\n",
      "step 7320 loss: 2.3823189735412598\n",
      "step 7321 loss: 2.5171144008636475\n",
      "step 7322 loss: 2.445887804031372\n",
      "step 7323 loss: 2.553727388381958\n",
      "step 7324 loss: 2.506632089614868\n",
      "step 7325 loss: 2.4007439613342285\n",
      "step 7326 loss: 2.5094199180603027\n",
      "step 7327 loss: 2.479106903076172\n",
      "step 7328 loss: 2.5227513313293457\n",
      "step 7329 loss: 2.4949257373809814\n",
      "step 7330 loss: 2.4775383472442627\n",
      "step 7331 loss: 2.407358407974243\n",
      "step 7332 loss: 2.452601909637451\n",
      "step 7333 loss: 2.5924277305603027\n",
      "step 7334 loss: 2.5128908157348633\n",
      "step 7335 loss: 2.4644172191619873\n",
      "step 7336 loss: 2.506283760070801\n",
      "step 7337 loss: 2.4312195777893066\n",
      "step 7338 loss: 2.572619676589966\n",
      "step 7339 loss: 2.5341193675994873\n",
      "step 7340 loss: 2.6335222721099854\n",
      "step 7341 loss: 2.3397085666656494\n",
      "step 7342 loss: 2.3698198795318604\n",
      "step 7343 loss: 2.3782663345336914\n",
      "step 7344 loss: 2.5242135524749756\n",
      "step 7345 loss: 2.5613789558410645\n",
      "step 7346 loss: 2.4973983764648438\n",
      "step 7347 loss: 2.5662307739257812\n",
      "step 7348 loss: 2.541987657546997\n",
      "step 7349 loss: 2.4733726978302\n",
      "step 7350 loss: 2.430629014968872\n",
      "step 7351 loss: 2.6068222522735596\n",
      "step 7352 loss: 2.495572566986084\n",
      "step 7353 loss: 2.577535629272461\n",
      "step 7354 loss: 2.4516661167144775\n",
      "step 7355 loss: 2.3827857971191406\n",
      "step 7356 loss: 2.5076165199279785\n",
      "step 7357 loss: 2.6021785736083984\n",
      "step 7358 loss: 2.5701828002929688\n",
      "step 7359 loss: 2.5117361545562744\n",
      "step 7360 loss: 2.5106046199798584\n",
      "step 7361 loss: 2.579103469848633\n",
      "step 7362 loss: 2.5349807739257812\n",
      "step 7363 loss: 2.614250421524048\n",
      "step 7364 loss: 2.4613475799560547\n",
      "step 7365 loss: 2.38718581199646\n",
      "step 7366 loss: 2.5222878456115723\n",
      "step 7367 loss: 2.4413671493530273\n",
      "step 7368 loss: 2.517427921295166\n",
      "step 7369 loss: 2.536634922027588\n",
      "step 7370 loss: 2.6230969429016113\n",
      "step 7371 loss: 2.5345070362091064\n",
      "step 7372 loss: 2.5729122161865234\n",
      "step 7373 loss: 2.624708414077759\n",
      "step 7374 loss: 2.434018850326538\n",
      "step 7375 loss: 2.5391457080841064\n",
      "step 7376 loss: 2.5100362300872803\n",
      "step 7377 loss: 2.4755806922912598\n",
      "step 7378 loss: 2.3670833110809326\n",
      "step 7379 loss: 2.405975818634033\n",
      "step 7380 loss: 2.5334150791168213\n",
      "step 7381 loss: 2.428415060043335\n",
      "step 7382 loss: 2.5411083698272705\n",
      "step 7383 loss: 2.4249496459960938\n",
      "step 7384 loss: 2.4837839603424072\n",
      "step 7385 loss: 2.5240161418914795\n",
      "step 7386 loss: 2.516993284225464\n",
      "step 7387 loss: 2.5216941833496094\n",
      "step 7388 loss: 2.466249942779541\n",
      "step 7389 loss: 2.4803595542907715\n",
      "step 7390 loss: 2.5100250244140625\n",
      "step 7391 loss: 2.6131844520568848\n",
      "step 7392 loss: 2.445913076400757\n",
      "step 7393 loss: 2.4167282581329346\n",
      "step 7394 loss: 2.5123937129974365\n",
      "step 7395 loss: 2.553118944168091\n",
      "step 7396 loss: 2.45137095451355\n",
      "step 7397 loss: 2.5152482986450195\n",
      "step 7398 loss: 2.3818106651306152\n",
      "step 7399 loss: 2.4874775409698486\n",
      "step 7400 loss: 2.410496234893799\n",
      "step 7401 loss: 2.5501315593719482\n",
      "step 7402 loss: 2.463282346725464\n",
      "step 7403 loss: 2.478539228439331\n",
      "step 7404 loss: 2.54347562789917\n",
      "step 7405 loss: 2.5030248165130615\n",
      "step 7406 loss: 2.4950337409973145\n",
      "step 7407 loss: 2.4869706630706787\n",
      "step 7408 loss: 2.4412407875061035\n",
      "step 7409 loss: 2.5760281085968018\n",
      "step 7410 loss: 2.476757049560547\n",
      "step 7411 loss: 2.505899429321289\n",
      "step 7412 loss: 2.5140931606292725\n",
      "step 7413 loss: 2.4691293239593506\n",
      "step 7414 loss: 2.4502980709075928\n",
      "step 7415 loss: 2.413094997406006\n",
      "step 7416 loss: 2.45617413520813\n",
      "step 7417 loss: 2.481227397918701\n",
      "step 7418 loss: 2.5687777996063232\n",
      "step 7419 loss: 2.3036534786224365\n",
      "step 7420 loss: 2.46974515914917\n",
      "step 7421 loss: 2.367872476577759\n",
      "step 7422 loss: 2.357774257659912\n",
      "step 7423 loss: 2.4547739028930664\n",
      "step 7424 loss: 2.473050832748413\n",
      "step 7425 loss: 2.422912836074829\n",
      "step 7426 loss: 2.4805850982666016\n",
      "step 7427 loss: 2.5338268280029297\n",
      "step 7428 loss: 2.493648052215576\n",
      "step 7429 loss: 2.4381556510925293\n",
      "step 7430 loss: 2.4440741539001465\n",
      "step 7431 loss: 2.511780023574829\n",
      "step 7432 loss: 2.4661521911621094\n",
      "step 7433 loss: 2.519355535507202\n",
      "step 7434 loss: 2.4724438190460205\n",
      "step 7435 loss: 2.4862911701202393\n",
      "step 7436 loss: 2.547809362411499\n",
      "step 7437 loss: 2.4774017333984375\n",
      "step 7438 loss: 2.4245383739471436\n",
      "step 7439 loss: 2.573373317718506\n",
      "step 7440 loss: 2.5524344444274902\n",
      "step 7441 loss: 2.3214852809906006\n",
      "step 7442 loss: 2.553746461868286\n",
      "step 7443 loss: 2.555720806121826\n",
      "step 7444 loss: 2.510978937149048\n",
      "step 7445 loss: 2.523519515991211\n",
      "step 7446 loss: 2.5081820487976074\n",
      "step 7447 loss: 2.437993049621582\n",
      "step 7448 loss: 2.458488941192627\n",
      "step 7449 loss: 2.457772731781006\n",
      "step 7450 loss: 2.395981788635254\n",
      "step 7451 loss: 2.4752066135406494\n",
      "step 7452 loss: 2.602294445037842\n",
      "step 7453 loss: 2.380314588546753\n",
      "step 7454 loss: 2.547557830810547\n",
      "step 7455 loss: 2.4361109733581543\n",
      "step 7456 loss: 2.5115134716033936\n",
      "step 7457 loss: 2.5785224437713623\n",
      "step 7458 loss: 2.564509630203247\n",
      "step 7459 loss: 2.3646867275238037\n",
      "step 7460 loss: 2.5259852409362793\n",
      "step 7461 loss: 2.553854465484619\n",
      "step 7462 loss: 2.408562660217285\n",
      "step 7463 loss: 2.513681411743164\n",
      "step 7464 loss: 2.3384146690368652\n",
      "step 7465 loss: 2.5842082500457764\n",
      "step 7466 loss: 2.590423107147217\n",
      "step 7467 loss: 2.502026319503784\n",
      "step 7468 loss: 2.463216543197632\n",
      "step 7469 loss: 2.5072782039642334\n",
      "step 7470 loss: 2.4688143730163574\n",
      "step 7471 loss: 2.3436334133148193\n",
      "step 7472 loss: 2.5966477394104004\n",
      "step 7473 loss: 2.5207395553588867\n",
      "step 7474 loss: 2.414647340774536\n",
      "step 7475 loss: 2.5085246562957764\n",
      "step 7476 loss: 2.4391634464263916\n",
      "step 7477 loss: 2.4718210697174072\n",
      "step 7478 loss: 2.568685293197632\n",
      "step 7479 loss: 2.3578999042510986\n",
      "step 7480 loss: 2.471184253692627\n",
      "step 7481 loss: 2.6445465087890625\n",
      "step 7482 loss: 2.6530239582061768\n",
      "step 7483 loss: 2.4856815338134766\n",
      "step 7484 loss: 2.4811244010925293\n",
      "step 7485 loss: 2.5597100257873535\n",
      "step 7486 loss: 2.574223279953003\n",
      "step 7487 loss: 2.5220227241516113\n",
      "step 7488 loss: 2.4291555881500244\n",
      "step 7489 loss: 2.514453172683716\n",
      "step 7490 loss: 2.397291898727417\n",
      "step 7491 loss: 2.462291955947876\n",
      "step 7492 loss: 2.461606502532959\n",
      "step 7493 loss: 2.5488533973693848\n",
      "step 7494 loss: 2.4684154987335205\n",
      "step 7495 loss: 2.468240737915039\n",
      "step 7496 loss: 2.5489277839660645\n",
      "step 7497 loss: 2.4976344108581543\n",
      "step 7498 loss: 2.5290396213531494\n",
      "step 7499 loss: 2.5140116214752197\n",
      "step 7500 loss: 2.400172472000122\n",
      "step 7501 loss: 2.4549272060394287\n",
      "step 7502 loss: 2.5122060775756836\n",
      "step 7503 loss: 2.4541609287261963\n",
      "step 7504 loss: 2.438734531402588\n",
      "step 7505 loss: 2.571592330932617\n",
      "step 7506 loss: 2.3231959342956543\n",
      "step 7507 loss: 2.491929054260254\n",
      "step 7508 loss: 2.395781993865967\n",
      "step 7509 loss: 2.408615827560425\n",
      "step 7510 loss: 2.5362019538879395\n",
      "step 7511 loss: 2.6124751567840576\n",
      "step 7512 loss: 2.4898064136505127\n",
      "step 7513 loss: 2.3768365383148193\n",
      "step 7514 loss: 2.5130715370178223\n",
      "step 7515 loss: 2.4782841205596924\n",
      "step 7516 loss: 2.462188959121704\n",
      "step 7517 loss: 2.4540188312530518\n",
      "step 7518 loss: 2.5545287132263184\n",
      "step 7519 loss: 2.477522611618042\n",
      "step 7520 loss: 2.4035427570343018\n",
      "step 7521 loss: 2.4259307384490967\n",
      "step 7522 loss: 2.481865406036377\n",
      "step 7523 loss: 2.6512393951416016\n",
      "step 7524 loss: 2.536957025527954\n",
      "step 7525 loss: 2.497460126876831\n",
      "step 7526 loss: 2.4346466064453125\n",
      "step 7527 loss: 2.4471354484558105\n",
      "step 7528 loss: 2.4891676902770996\n",
      "step 7529 loss: 2.4406752586364746\n",
      "step 7530 loss: 2.4714741706848145\n",
      "step 7531 loss: 2.4798905849456787\n",
      "step 7532 loss: 2.4948203563690186\n",
      "step 7533 loss: 2.5024895668029785\n",
      "step 7534 loss: 2.2602131366729736\n",
      "step 7535 loss: 2.50748348236084\n",
      "step 7536 loss: 2.4313735961914062\n",
      "step 7537 loss: 2.4578707218170166\n",
      "step 7538 loss: 2.5016839504241943\n",
      "step 7539 loss: 2.547767162322998\n",
      "step 7540 loss: 2.3812341690063477\n",
      "step 7541 loss: 2.4000062942504883\n",
      "step 7542 loss: 2.5134682655334473\n",
      "step 7543 loss: 2.598621368408203\n",
      "step 7544 loss: 2.4759953022003174\n",
      "step 7545 loss: 2.555079460144043\n",
      "step 7546 loss: 2.5269775390625\n",
      "step 7547 loss: 2.442204475402832\n",
      "step 7548 loss: 2.6193110942840576\n",
      "step 7549 loss: 2.4730517864227295\n",
      "step 7550 loss: 2.3125107288360596\n",
      "step 7551 loss: 2.5357282161712646\n",
      "step 7552 loss: 2.5031347274780273\n",
      "step 7553 loss: 2.4315242767333984\n",
      "step 7554 loss: 2.4613306522369385\n",
      "step 7555 loss: 2.4886834621429443\n",
      "step 7556 loss: 2.367191791534424\n",
      "step 7557 loss: 2.560765027999878\n",
      "step 7558 loss: 2.629221200942993\n",
      "step 7559 loss: 2.454766273498535\n",
      "step 7560 loss: 2.4976508617401123\n",
      "step 7561 loss: 2.5148346424102783\n",
      "step 7562 loss: 2.367872953414917\n",
      "step 7563 loss: 2.525507926940918\n",
      "step 7564 loss: 2.551239252090454\n",
      "step 7565 loss: 2.4781336784362793\n",
      "step 7566 loss: 2.485377550125122\n",
      "step 7567 loss: 2.503924608230591\n",
      "step 7568 loss: 2.5544536113739014\n",
      "step 7569 loss: 2.5839762687683105\n",
      "step 7570 loss: 2.5161306858062744\n",
      "step 7571 loss: 2.4290170669555664\n",
      "step 7572 loss: 2.4494829177856445\n",
      "step 7573 loss: 2.5719597339630127\n",
      "step 7574 loss: 2.5139050483703613\n",
      "step 7575 loss: 2.4621379375457764\n",
      "step 7576 loss: 2.485478401184082\n",
      "step 7577 loss: 2.3939995765686035\n",
      "step 7578 loss: 2.488572835922241\n",
      "step 7579 loss: 2.365335464477539\n",
      "step 7580 loss: 2.436396837234497\n",
      "step 7581 loss: 2.353059768676758\n",
      "step 7582 loss: 2.6312851905822754\n",
      "step 7583 loss: 2.539804458618164\n",
      "step 7584 loss: 2.490969181060791\n",
      "step 7585 loss: 2.604637384414673\n",
      "step 7586 loss: 2.470883369445801\n",
      "step 7587 loss: 2.347321033477783\n",
      "step 7588 loss: 2.450742244720459\n",
      "step 7589 loss: 2.487433671951294\n",
      "step 7590 loss: 2.5003905296325684\n",
      "step 7591 loss: 2.509761333465576\n",
      "step 7592 loss: 2.4511332511901855\n",
      "step 7593 loss: 2.5375993251800537\n",
      "step 7594 loss: 2.4732789993286133\n",
      "step 7595 loss: 2.4740469455718994\n",
      "step 7596 loss: 2.5154340267181396\n",
      "step 7597 loss: 2.4856579303741455\n",
      "step 7598 loss: 2.4962477684020996\n",
      "step 7599 loss: 2.501052141189575\n",
      "step 7600 loss: 2.4295127391815186\n",
      "step 7601 loss: 2.5341200828552246\n",
      "step 7602 loss: 2.461989164352417\n",
      "step 7603 loss: 2.49096941947937\n",
      "step 7604 loss: 2.470635414123535\n",
      "step 7605 loss: 2.569244623184204\n",
      "step 7606 loss: 2.4763710498809814\n",
      "step 7607 loss: 2.538111686706543\n",
      "step 7608 loss: 2.610515832901001\n",
      "step 7609 loss: 2.457803249359131\n",
      "step 7610 loss: 2.457303762435913\n",
      "step 7611 loss: 2.4619712829589844\n",
      "step 7612 loss: 2.5765230655670166\n",
      "step 7613 loss: 2.526337146759033\n",
      "step 7614 loss: 2.491941213607788\n",
      "step 7615 loss: 2.4394922256469727\n",
      "step 7616 loss: 2.481311321258545\n",
      "step 7617 loss: 2.6501505374908447\n",
      "step 7618 loss: 2.5794928073883057\n",
      "step 7619 loss: 2.4251084327697754\n",
      "step 7620 loss: 2.4533305168151855\n",
      "step 7621 loss: 2.629652261734009\n",
      "step 7622 loss: 2.3684728145599365\n",
      "step 7623 loss: 2.5490224361419678\n",
      "step 7624 loss: 2.4646854400634766\n",
      "step 7625 loss: 2.4979963302612305\n",
      "step 7626 loss: 2.412592649459839\n",
      "step 7627 loss: 2.4907517433166504\n",
      "step 7628 loss: 2.4666271209716797\n",
      "step 7629 loss: 2.38663649559021\n",
      "step 7630 loss: 2.4600636959075928\n",
      "step 7631 loss: 2.4566152095794678\n",
      "step 7632 loss: 2.5726187229156494\n",
      "step 7633 loss: 2.7218780517578125\n",
      "step 7634 loss: 2.5199155807495117\n",
      "step 7635 loss: 2.46708083152771\n",
      "step 7636 loss: 2.3744266033172607\n",
      "step 7637 loss: 2.43379545211792\n",
      "step 7638 loss: 2.5296599864959717\n",
      "step 7639 loss: 2.380851984024048\n",
      "step 7640 loss: 2.5737245082855225\n",
      "step 7641 loss: 2.5159833431243896\n",
      "step 7642 loss: 2.555737257003784\n",
      "step 7643 loss: 2.4553606510162354\n",
      "step 7644 loss: 2.597893238067627\n",
      "step 7645 loss: 2.5197083950042725\n",
      "step 7646 loss: 2.46122145652771\n",
      "step 7647 loss: 2.473144769668579\n",
      "step 7648 loss: 2.3919970989227295\n",
      "step 7649 loss: 2.517822742462158\n",
      "step 7650 loss: 2.447765350341797\n",
      "step 7651 loss: 2.420713186264038\n",
      "step 7652 loss: 2.5117058753967285\n",
      "step 7653 loss: 2.591830253601074\n",
      "step 7654 loss: 2.456798553466797\n",
      "step 7655 loss: 2.5022759437561035\n",
      "step 7656 loss: 2.5232527256011963\n",
      "step 7657 loss: 2.4567205905914307\n",
      "step 7658 loss: 2.4495348930358887\n",
      "step 7659 loss: 2.5468900203704834\n",
      "step 7660 loss: 2.4565463066101074\n",
      "step 7661 loss: 2.390885353088379\n",
      "step 7662 loss: 2.576807975769043\n",
      "step 7663 loss: 2.4706902503967285\n",
      "step 7664 loss: 2.5402071475982666\n",
      "step 7665 loss: 2.594241142272949\n",
      "step 7666 loss: 2.431889057159424\n",
      "step 7667 loss: 2.4656693935394287\n",
      "step 7668 loss: 2.546441078186035\n",
      "step 7669 loss: 2.4522206783294678\n",
      "step 7670 loss: 2.5562798976898193\n",
      "step 7671 loss: 2.5041491985321045\n",
      "step 7672 loss: 2.490183115005493\n",
      "step 7673 loss: 2.41642427444458\n",
      "step 7674 loss: 2.4903862476348877\n",
      "step 7675 loss: 2.582991600036621\n",
      "step 7676 loss: 2.515852689743042\n",
      "step 7677 loss: 2.5139458179473877\n",
      "step 7678 loss: 2.550363779067993\n",
      "step 7679 loss: 2.4141387939453125\n",
      "step 7680 loss: 2.5384795665740967\n",
      "step 7681 loss: 2.5695645809173584\n",
      "step 7682 loss: 2.4595601558685303\n",
      "step 7683 loss: 2.484254837036133\n",
      "step 7684 loss: 2.4145431518554688\n",
      "step 7685 loss: 2.5234339237213135\n",
      "step 7686 loss: 2.485670804977417\n",
      "step 7687 loss: 2.5151634216308594\n",
      "step 7688 loss: 2.636781930923462\n",
      "step 7689 loss: 2.4376368522644043\n",
      "step 7690 loss: 2.483954906463623\n",
      "step 7691 loss: 2.4107728004455566\n",
      "step 7692 loss: 2.4957873821258545\n",
      "step 7693 loss: 2.5187947750091553\n",
      "step 7694 loss: 2.5933897495269775\n",
      "step 7695 loss: 2.383589506149292\n",
      "step 7696 loss: 2.4279119968414307\n",
      "step 7697 loss: 2.5350842475891113\n",
      "step 7698 loss: 2.563336133956909\n",
      "step 7699 loss: 2.510124683380127\n",
      "step 7700 loss: 2.424211025238037\n",
      "step 7701 loss: 2.575577735900879\n",
      "step 7702 loss: 2.5442054271698\n",
      "step 7703 loss: 2.498520612716675\n",
      "step 7704 loss: 2.5188753604888916\n",
      "step 7705 loss: 2.5336477756500244\n",
      "step 7706 loss: 2.5014359951019287\n",
      "step 7707 loss: 2.43965482711792\n",
      "step 7708 loss: 2.5206363201141357\n",
      "step 7709 loss: 2.613985061645508\n",
      "step 7710 loss: 2.52490234375\n",
      "step 7711 loss: 2.4828083515167236\n",
      "step 7712 loss: 2.5649383068084717\n",
      "step 7713 loss: 2.477268934249878\n",
      "step 7714 loss: 2.4833123683929443\n",
      "step 7715 loss: 2.430142402648926\n",
      "step 7716 loss: 2.4940943717956543\n",
      "step 7717 loss: 2.564709424972534\n",
      "step 7718 loss: 2.535391092300415\n",
      "step 7719 loss: 2.5086987018585205\n",
      "step 7720 loss: 2.424849033355713\n",
      "step 7721 loss: 2.4768195152282715\n",
      "step 7722 loss: 2.553309202194214\n",
      "step 7723 loss: 2.4641432762145996\n",
      "step 7724 loss: 2.370054006576538\n",
      "step 7725 loss: 2.4923064708709717\n",
      "step 7726 loss: 2.4269356727600098\n",
      "step 7727 loss: 2.5615994930267334\n",
      "step 7728 loss: 2.658945083618164\n",
      "step 7729 loss: 2.4425559043884277\n",
      "step 7730 loss: 2.5940096378326416\n",
      "step 7731 loss: 2.5679068565368652\n",
      "step 7732 loss: 2.5305373668670654\n",
      "step 7733 loss: 2.5967278480529785\n",
      "step 7734 loss: 2.518784999847412\n",
      "step 7735 loss: 2.4534878730773926\n",
      "step 7736 loss: 2.429454803466797\n",
      "step 7737 loss: 2.574522018432617\n",
      "step 7738 loss: 2.510145425796509\n",
      "step 7739 loss: 2.5390825271606445\n",
      "step 7740 loss: 2.420001745223999\n",
      "step 7741 loss: 2.4585771560668945\n",
      "step 7742 loss: 2.446990728378296\n",
      "step 7743 loss: 2.453852653503418\n",
      "step 7744 loss: 2.344884157180786\n",
      "step 7745 loss: 2.627267599105835\n",
      "step 7746 loss: 2.435340404510498\n",
      "step 7747 loss: 2.380321979522705\n",
      "step 7748 loss: 2.552950143814087\n",
      "step 7749 loss: 2.4756176471710205\n",
      "step 7750 loss: 2.4818248748779297\n",
      "step 7751 loss: 2.4020376205444336\n",
      "step 7752 loss: 2.4429538249969482\n",
      "step 7753 loss: 2.4020605087280273\n",
      "step 7754 loss: 2.4116153717041016\n",
      "step 7755 loss: 2.426177978515625\n",
      "step 7756 loss: 2.5827183723449707\n",
      "step 7757 loss: 2.406947135925293\n",
      "step 7758 loss: 2.498694658279419\n",
      "step 7759 loss: 2.560286521911621\n",
      "step 7760 loss: 2.5482442378997803\n",
      "step 7761 loss: 2.600939989089966\n",
      "step 7762 loss: 2.5650155544281006\n",
      "step 7763 loss: 2.508354663848877\n",
      "step 7764 loss: 2.5341715812683105\n",
      "step 7765 loss: 2.4576570987701416\n",
      "step 7766 loss: 2.5082366466522217\n",
      "step 7767 loss: 2.5161149501800537\n",
      "step 7768 loss: 2.5091729164123535\n",
      "step 7769 loss: 2.4660263061523438\n",
      "step 7770 loss: 2.5469589233398438\n",
      "step 7771 loss: 2.539766311645508\n",
      "step 7772 loss: 2.417649030685425\n",
      "step 7773 loss: 2.4966399669647217\n",
      "step 7774 loss: 2.498181104660034\n",
      "step 7775 loss: 2.401656150817871\n",
      "step 7776 loss: 2.4781110286712646\n",
      "step 7777 loss: 2.524012327194214\n",
      "step 7778 loss: 2.4776461124420166\n",
      "step 7779 loss: 2.473170280456543\n",
      "step 7780 loss: 2.503511667251587\n",
      "step 7781 loss: 2.4216504096984863\n",
      "step 7782 loss: 2.4789724349975586\n",
      "step 7783 loss: 2.5426266193389893\n",
      "step 7784 loss: 2.507629632949829\n",
      "step 7785 loss: 2.444479465484619\n",
      "step 7786 loss: 2.4797189235687256\n",
      "step 7787 loss: 2.6023359298706055\n",
      "step 7788 loss: 2.468126058578491\n",
      "step 7789 loss: 2.5399351119995117\n",
      "step 7790 loss: 2.5412070751190186\n",
      "step 7791 loss: 2.5138542652130127\n",
      "step 7792 loss: 2.4884872436523438\n",
      "step 7793 loss: 2.5487093925476074\n",
      "step 7794 loss: 2.396538496017456\n",
      "step 7795 loss: 2.442012071609497\n",
      "step 7796 loss: 2.521481990814209\n",
      "step 7797 loss: 2.427842617034912\n",
      "step 7798 loss: 2.4898264408111572\n",
      "step 7799 loss: 2.499981164932251\n",
      "step 7800 loss: 2.494037389755249\n",
      "step 7801 loss: 2.463472604751587\n",
      "step 7802 loss: 2.355027198791504\n",
      "step 7803 loss: 2.4371042251586914\n",
      "step 7804 loss: 2.4610068798065186\n",
      "step 7805 loss: 2.501330614089966\n",
      "step 7806 loss: 2.4727048873901367\n",
      "step 7807 loss: 2.587099552154541\n",
      "step 7808 loss: 2.5703155994415283\n",
      "step 7809 loss: 2.455939531326294\n",
      "step 7810 loss: 2.6266965866088867\n",
      "step 7811 loss: 2.3840603828430176\n",
      "step 7812 loss: 2.463688373565674\n",
      "step 7813 loss: 2.51588773727417\n",
      "step 7814 loss: 2.5477912425994873\n",
      "step 7815 loss: 2.5436348915100098\n",
      "step 7816 loss: 2.4545071125030518\n",
      "step 7817 loss: 2.4784414768218994\n",
      "step 7818 loss: 2.531336545944214\n",
      "step 7819 loss: 2.406484842300415\n",
      "step 7820 loss: 2.5869827270507812\n",
      "step 7821 loss: 2.4845683574676514\n",
      "step 7822 loss: 2.506627321243286\n",
      "step 7823 loss: 2.5896379947662354\n",
      "step 7824 loss: 2.4164226055145264\n",
      "step 7825 loss: 2.474515914916992\n",
      "step 7826 loss: 2.5572900772094727\n",
      "step 7827 loss: 2.4466216564178467\n",
      "step 7828 loss: 2.451090097427368\n",
      "step 7829 loss: 2.442394256591797\n",
      "step 7830 loss: 2.510702133178711\n",
      "step 7831 loss: 2.6017987728118896\n",
      "step 7832 loss: 2.4397220611572266\n",
      "step 7833 loss: 2.48795223236084\n",
      "step 7834 loss: 2.5523624420166016\n",
      "step 7835 loss: 2.565398931503296\n",
      "step 7836 loss: 2.5280895233154297\n",
      "step 7837 loss: 2.3760149478912354\n",
      "step 7838 loss: 2.477980613708496\n",
      "step 7839 loss: 2.5129776000976562\n",
      "step 7840 loss: 2.5660600662231445\n",
      "step 7841 loss: 2.4510021209716797\n",
      "step 7842 loss: 2.4186818599700928\n",
      "step 7843 loss: 2.376657724380493\n",
      "step 7844 loss: 2.366325855255127\n",
      "step 7845 loss: 2.4306561946868896\n",
      "step 7846 loss: 2.4485883712768555\n",
      "step 7847 loss: 2.4015822410583496\n",
      "step 7848 loss: 2.5207390785217285\n",
      "step 7849 loss: 2.497929096221924\n",
      "step 7850 loss: 2.4884192943573\n",
      "step 7851 loss: 2.512990713119507\n",
      "step 7852 loss: 2.4820632934570312\n",
      "step 7853 loss: 2.5248312950134277\n",
      "step 7854 loss: 2.6041817665100098\n",
      "step 7855 loss: 2.4759418964385986\n",
      "step 7856 loss: 2.558145761489868\n",
      "step 7857 loss: 2.5425662994384766\n",
      "step 7858 loss: 2.4401426315307617\n",
      "step 7859 loss: 2.492192506790161\n",
      "step 7860 loss: 2.462104082107544\n",
      "step 7861 loss: 2.535895586013794\n",
      "step 7862 loss: 2.5785958766937256\n",
      "step 7863 loss: 2.5133633613586426\n",
      "step 7864 loss: 2.5441033840179443\n",
      "step 7865 loss: 2.3965744972229004\n",
      "step 7866 loss: 2.448721170425415\n",
      "step 7867 loss: 2.5284714698791504\n",
      "step 7868 loss: 2.4431369304656982\n",
      "step 7869 loss: 2.5064799785614014\n",
      "step 7870 loss: 2.4622538089752197\n",
      "step 7871 loss: 2.490999937057495\n",
      "step 7872 loss: 2.486659526824951\n",
      "step 7873 loss: 2.4663212299346924\n",
      "step 7874 loss: 2.5737035274505615\n",
      "step 7875 loss: 2.4354074001312256\n",
      "step 7876 loss: 2.4761531352996826\n",
      "step 7877 loss: 2.4875950813293457\n",
      "step 7878 loss: 2.4907898902893066\n",
      "step 7879 loss: 2.623701333999634\n",
      "step 7880 loss: 2.4419281482696533\n",
      "step 7881 loss: 2.5056705474853516\n",
      "step 7882 loss: 2.514317512512207\n",
      "step 7883 loss: 2.505156993865967\n",
      "step 7884 loss: 2.452791452407837\n",
      "step 7885 loss: 2.3808696269989014\n",
      "step 7886 loss: 2.454867124557495\n",
      "step 7887 loss: 2.415865421295166\n",
      "step 7888 loss: 2.5398528575897217\n",
      "step 7889 loss: 2.569288969039917\n",
      "step 7890 loss: 2.4734160900115967\n",
      "step 7891 loss: 2.4943161010742188\n",
      "step 7892 loss: 2.396104097366333\n",
      "step 7893 loss: 2.4877889156341553\n",
      "step 7894 loss: 2.4499783515930176\n",
      "step 7895 loss: 2.4817442893981934\n",
      "step 7896 loss: 2.4824492931365967\n",
      "step 7897 loss: 2.530076742172241\n",
      "step 7898 loss: 2.3450260162353516\n",
      "step 7899 loss: 2.4937801361083984\n",
      "step 7900 loss: 2.438490390777588\n",
      "step 7901 loss: 2.5826308727264404\n",
      "step 7902 loss: 2.3746607303619385\n",
      "step 7903 loss: 2.55739426612854\n",
      "step 7904 loss: 2.517333984375\n",
      "step 7905 loss: 2.543715715408325\n",
      "step 7906 loss: 2.549830436706543\n",
      "step 7907 loss: 2.542973279953003\n",
      "step 7908 loss: 2.452451467514038\n",
      "step 7909 loss: 2.59340500831604\n",
      "step 7910 loss: 2.679779291152954\n",
      "step 7911 loss: 2.495912551879883\n",
      "step 7912 loss: 2.5155577659606934\n",
      "step 7913 loss: 2.516979455947876\n",
      "step 7914 loss: 2.6328623294830322\n",
      "step 7915 loss: 2.430962562561035\n",
      "step 7916 loss: 2.548178195953369\n",
      "step 7917 loss: 2.5358426570892334\n",
      "step 7918 loss: 2.463351249694824\n",
      "step 7919 loss: 2.5148584842681885\n",
      "step 7920 loss: 2.5308406352996826\n",
      "step 7921 loss: 2.495662212371826\n",
      "step 7922 loss: 2.5618743896484375\n",
      "step 7923 loss: 2.641535520553589\n",
      "step 7924 loss: 2.4890878200531006\n",
      "step 7925 loss: 2.389126777648926\n",
      "step 7926 loss: 2.549654722213745\n",
      "step 7927 loss: 2.473658323287964\n",
      "step 7928 loss: 2.5601394176483154\n",
      "step 7929 loss: 2.3379111289978027\n",
      "step 7930 loss: 2.4188809394836426\n",
      "step 7931 loss: 2.4372997283935547\n",
      "step 7932 loss: 2.543846368789673\n",
      "step 7933 loss: 2.4769489765167236\n",
      "step 7934 loss: 2.3756814002990723\n",
      "step 7935 loss: 2.533006191253662\n",
      "step 7936 loss: 2.566202163696289\n",
      "step 7937 loss: 2.4930953979492188\n",
      "step 7938 loss: 2.624377489089966\n",
      "step 7939 loss: 2.4639475345611572\n",
      "step 7940 loss: 2.3770411014556885\n",
      "step 7941 loss: 2.469329595565796\n",
      "step 7942 loss: 2.512845993041992\n",
      "step 7943 loss: 2.5132458209991455\n",
      "step 7944 loss: 2.5419960021972656\n",
      "step 7945 loss: 2.4239096641540527\n",
      "step 7946 loss: 2.5866901874542236\n",
      "step 7947 loss: 2.4193296432495117\n",
      "step 7948 loss: 2.4036056995391846\n",
      "step 7949 loss: 2.387582302093506\n",
      "step 7950 loss: 2.4387433528900146\n",
      "step 7951 loss: 2.460371494293213\n",
      "step 7952 loss: 2.474283218383789\n",
      "step 7953 loss: 2.532139301300049\n",
      "step 7954 loss: 2.5070700645446777\n",
      "step 7955 loss: 2.5634074211120605\n",
      "step 7956 loss: 2.4953341484069824\n",
      "step 7957 loss: 2.4568707942962646\n",
      "step 7958 loss: 2.477733850479126\n",
      "step 7959 loss: 2.454299211502075\n",
      "step 7960 loss: 2.538508176803589\n",
      "step 7961 loss: 2.5606110095977783\n",
      "step 7962 loss: 2.436208724975586\n",
      "step 7963 loss: 2.4872117042541504\n",
      "step 7964 loss: 2.5232887268066406\n",
      "step 7965 loss: 2.4783787727355957\n",
      "step 7966 loss: 2.5515034198760986\n",
      "step 7967 loss: 2.3473784923553467\n",
      "step 7968 loss: 2.546351194381714\n",
      "step 7969 loss: 2.372882127761841\n",
      "step 7970 loss: 2.4362990856170654\n",
      "step 7971 loss: 2.526078224182129\n",
      "step 7972 loss: 2.3443222045898438\n",
      "step 7973 loss: 2.367856979370117\n",
      "step 7974 loss: 2.4824678897857666\n",
      "step 7975 loss: 2.4506163597106934\n",
      "step 7976 loss: 2.4489004611968994\n",
      "step 7977 loss: 2.396210193634033\n",
      "step 7978 loss: 2.543907880783081\n",
      "step 7979 loss: 2.4723594188690186\n",
      "step 7980 loss: 2.4830617904663086\n",
      "step 7981 loss: 2.4422731399536133\n",
      "step 7982 loss: 2.538259267807007\n",
      "step 7983 loss: 2.518472671508789\n",
      "step 7984 loss: 2.4241080284118652\n",
      "step 7985 loss: 2.602621555328369\n",
      "step 7986 loss: 2.4049742221832275\n",
      "step 7987 loss: 2.4626576900482178\n",
      "step 7988 loss: 2.4915170669555664\n",
      "step 7989 loss: 2.6001088619232178\n",
      "step 7990 loss: 2.58617901802063\n",
      "step 7991 loss: 2.4658241271972656\n",
      "step 7992 loss: 2.4508538246154785\n",
      "step 7993 loss: 2.5873897075653076\n",
      "step 7994 loss: 2.5345518589019775\n",
      "step 7995 loss: 2.447753429412842\n",
      "step 7996 loss: 2.3747503757476807\n",
      "step 7997 loss: 2.4771833419799805\n",
      "step 7998 loss: 2.5175156593322754\n",
      "step 7999 loss: 2.469970941543579\n",
      "step 8000 loss: 2.4696712493896484\n",
      "step 8001 loss: 2.5575015544891357\n",
      "step 8002 loss: 2.544621706008911\n",
      "step 8003 loss: 2.4444122314453125\n",
      "step 8004 loss: 2.4189515113830566\n",
      "step 8005 loss: 2.7092599868774414\n",
      "step 8006 loss: 2.6432576179504395\n",
      "step 8007 loss: 2.549365520477295\n",
      "step 8008 loss: 2.3687686920166016\n",
      "step 8009 loss: 2.49302339553833\n",
      "step 8010 loss: 2.582176685333252\n",
      "step 8011 loss: 2.435250997543335\n",
      "step 8012 loss: 2.4593520164489746\n",
      "step 8013 loss: 2.5368728637695312\n",
      "step 8014 loss: 2.5756404399871826\n",
      "step 8015 loss: 2.4461402893066406\n",
      "step 8016 loss: 2.49191951751709\n",
      "step 8017 loss: 2.461608648300171\n",
      "step 8018 loss: 2.5069730281829834\n",
      "step 8019 loss: 2.488271474838257\n",
      "step 8020 loss: 2.5120325088500977\n",
      "step 8021 loss: 2.4961562156677246\n",
      "step 8022 loss: 2.4621331691741943\n",
      "step 8023 loss: 2.4664361476898193\n",
      "step 8024 loss: 2.4256646633148193\n",
      "step 8025 loss: 2.4834675788879395\n",
      "step 8026 loss: 2.519824981689453\n",
      "step 8027 loss: 2.336674451828003\n",
      "step 8028 loss: 2.4231393337249756\n",
      "step 8029 loss: 2.5428929328918457\n",
      "step 8030 loss: 2.5285422801971436\n",
      "step 8031 loss: 2.4822492599487305\n",
      "step 8032 loss: 2.4788973331451416\n",
      "step 8033 loss: 2.530951738357544\n",
      "step 8034 loss: 2.511566162109375\n",
      "step 8035 loss: 2.6941797733306885\n",
      "step 8036 loss: 2.5298948287963867\n",
      "step 8037 loss: 2.478590488433838\n",
      "step 8038 loss: 2.4147002696990967\n",
      "step 8039 loss: 2.6001007556915283\n",
      "step 8040 loss: 2.5788350105285645\n",
      "step 8041 loss: 2.3936009407043457\n",
      "step 8042 loss: 2.509737253189087\n",
      "step 8043 loss: 2.559800624847412\n",
      "step 8044 loss: 2.5180106163024902\n",
      "step 8045 loss: 2.5201833248138428\n",
      "step 8046 loss: 2.593909978866577\n",
      "step 8047 loss: 2.5777206420898438\n",
      "step 8048 loss: 2.5749056339263916\n",
      "step 8049 loss: 2.5615267753601074\n",
      "step 8050 loss: 2.5813913345336914\n",
      "step 8051 loss: 2.4576640129089355\n",
      "step 8052 loss: 2.506037712097168\n",
      "step 8053 loss: 2.4113340377807617\n",
      "step 8054 loss: 2.4037086963653564\n",
      "step 8055 loss: 2.504467487335205\n",
      "step 8056 loss: 2.4774928092956543\n",
      "step 8057 loss: 2.5123584270477295\n",
      "step 8058 loss: 2.471209764480591\n",
      "step 8059 loss: 2.510146141052246\n",
      "step 8060 loss: 2.545161247253418\n",
      "step 8061 loss: 2.5257601737976074\n",
      "step 8062 loss: 2.610173225402832\n",
      "step 8063 loss: 2.406031847000122\n",
      "step 8064 loss: 2.5215959548950195\n",
      "step 8065 loss: 2.4630913734436035\n",
      "step 8066 loss: 2.649181604385376\n",
      "step 8067 loss: 2.5284788608551025\n",
      "step 8068 loss: 2.450916051864624\n",
      "step 8069 loss: 2.5005080699920654\n",
      "step 8070 loss: 2.5262420177459717\n",
      "step 8071 loss: 2.3519880771636963\n",
      "step 8072 loss: 2.5100655555725098\n",
      "step 8073 loss: 2.4387879371643066\n",
      "step 8074 loss: 2.42975115776062\n",
      "step 8075 loss: 2.5123512744903564\n",
      "step 8076 loss: 2.4694457054138184\n",
      "step 8077 loss: 2.5350866317749023\n",
      "step 8078 loss: 2.4305009841918945\n",
      "step 8079 loss: 2.45204758644104\n",
      "step 8080 loss: 2.557786226272583\n",
      "step 8081 loss: 2.5562539100646973\n",
      "step 8082 loss: 2.4027347564697266\n",
      "step 8083 loss: 2.45857834815979\n",
      "step 8084 loss: 2.4896790981292725\n",
      "step 8085 loss: 2.4118711948394775\n",
      "step 8086 loss: 2.5347824096679688\n",
      "step 8087 loss: 2.577348470687866\n",
      "step 8088 loss: 2.498265027999878\n",
      "step 8089 loss: 2.471827745437622\n",
      "step 8090 loss: 2.4187498092651367\n",
      "step 8091 loss: 2.4528627395629883\n",
      "step 8092 loss: 2.341853141784668\n",
      "step 8093 loss: 2.4945383071899414\n",
      "step 8094 loss: 2.401078939437866\n",
      "step 8095 loss: 2.540367603302002\n",
      "step 8096 loss: 2.4596219062805176\n",
      "step 8097 loss: 2.442484140396118\n",
      "step 8098 loss: 2.4492416381835938\n",
      "step 8099 loss: 2.3517558574676514\n",
      "step 8100 loss: 2.4945294857025146\n",
      "step 8101 loss: 2.3881850242614746\n",
      "step 8102 loss: 2.49143385887146\n",
      "step 8103 loss: 2.5397655963897705\n",
      "step 8104 loss: 2.4374382495880127\n",
      "step 8105 loss: 2.65301513671875\n",
      "step 8106 loss: 2.4314258098602295\n",
      "step 8107 loss: 2.526200771331787\n",
      "step 8108 loss: 2.564441204071045\n",
      "step 8109 loss: 2.5630595684051514\n",
      "step 8110 loss: 2.4111785888671875\n",
      "step 8111 loss: 2.5203604698181152\n",
      "step 8112 loss: 2.411905527114868\n",
      "step 8113 loss: 2.561999797821045\n",
      "step 8114 loss: 2.5048251152038574\n",
      "step 8115 loss: 2.427518606185913\n",
      "step 8116 loss: 2.5463874340057373\n",
      "step 8117 loss: 2.542257308959961\n",
      "step 8118 loss: 2.519308090209961\n",
      "step 8119 loss: 2.4083237648010254\n",
      "step 8120 loss: 2.4325613975524902\n",
      "step 8121 loss: 2.4912493228912354\n",
      "step 8122 loss: 2.3216512203216553\n",
      "step 8123 loss: 2.6282548904418945\n",
      "step 8124 loss: 2.4984145164489746\n",
      "step 8125 loss: 2.4757981300354004\n",
      "step 8126 loss: 2.5332651138305664\n",
      "step 8127 loss: 2.6717913150787354\n",
      "step 8128 loss: 2.5156519412994385\n",
      "step 8129 loss: 2.5085487365722656\n",
      "step 8130 loss: 2.5453295707702637\n",
      "step 8131 loss: 2.451935291290283\n",
      "step 8132 loss: 2.511629581451416\n",
      "step 8133 loss: 2.4265220165252686\n",
      "step 8134 loss: 2.472852945327759\n",
      "step 8135 loss: 2.5586178302764893\n",
      "step 8136 loss: 2.5059945583343506\n",
      "step 8137 loss: 2.4419777393341064\n",
      "step 8138 loss: 2.6264405250549316\n",
      "step 8139 loss: 2.532770872116089\n",
      "step 8140 loss: 2.5393266677856445\n",
      "step 8141 loss: 2.437150478363037\n",
      "step 8142 loss: 2.459726333618164\n",
      "step 8143 loss: 2.374373435974121\n",
      "step 8144 loss: 2.5065252780914307\n",
      "step 8145 loss: 2.673496723175049\n",
      "step 8146 loss: 2.496169090270996\n",
      "step 8147 loss: 2.357387065887451\n",
      "step 8148 loss: 2.5392262935638428\n",
      "step 8149 loss: 2.5537703037261963\n",
      "step 8150 loss: 2.436763048171997\n",
      "step 8151 loss: 2.599531412124634\n",
      "step 8152 loss: 2.505115032196045\n",
      "step 8153 loss: 2.4593138694763184\n",
      "step 8154 loss: 2.438755750656128\n",
      "step 8155 loss: 2.512343406677246\n",
      "step 8156 loss: 2.5567126274108887\n",
      "step 8157 loss: 2.462707281112671\n",
      "step 8158 loss: 2.4108924865722656\n",
      "step 8159 loss: 2.4584102630615234\n",
      "step 8160 loss: 2.496274471282959\n",
      "step 8161 loss: 2.5343358516693115\n",
      "step 8162 loss: 2.427746534347534\n",
      "step 8163 loss: 2.5166125297546387\n",
      "step 8164 loss: 2.4064626693725586\n",
      "step 8165 loss: 2.50299072265625\n",
      "step 8166 loss: 2.398761749267578\n",
      "step 8167 loss: 2.5354769229888916\n",
      "step 8168 loss: 2.342378616333008\n",
      "step 8169 loss: 2.452211618423462\n",
      "step 8170 loss: 2.4584970474243164\n",
      "step 8171 loss: 2.523648500442505\n",
      "step 8172 loss: 2.465283155441284\n",
      "step 8173 loss: 2.4676287174224854\n",
      "step 8174 loss: 2.482442855834961\n",
      "step 8175 loss: 2.5006980895996094\n",
      "step 8176 loss: 2.496124744415283\n",
      "step 8177 loss: 2.5314958095550537\n",
      "step 8178 loss: 2.5429720878601074\n",
      "step 8179 loss: 2.540236473083496\n",
      "step 8180 loss: 2.56367826461792\n",
      "step 8181 loss: 2.4891843795776367\n",
      "step 8182 loss: 2.498082160949707\n",
      "step 8183 loss: 2.4031267166137695\n",
      "step 8184 loss: 2.5643961429595947\n",
      "step 8185 loss: 2.4498984813690186\n",
      "step 8186 loss: 2.4883055686950684\n",
      "step 8187 loss: 2.4992611408233643\n",
      "step 8188 loss: 2.6457924842834473\n",
      "step 8189 loss: 2.394975185394287\n",
      "step 8190 loss: 2.4975810050964355\n",
      "step 8191 loss: 2.4735231399536133\n",
      "step 8192 loss: 2.5285112857818604\n",
      "step 8193 loss: 2.4590904712677\n",
      "step 8194 loss: 2.455612897872925\n",
      "step 8195 loss: 2.4614458084106445\n",
      "step 8196 loss: 2.399806261062622\n",
      "step 8197 loss: 2.481297254562378\n",
      "step 8198 loss: 2.4180703163146973\n",
      "step 8199 loss: 2.5038516521453857\n",
      "step 8200 loss: 2.4096617698669434\n",
      "step 8201 loss: 2.377664089202881\n",
      "step 8202 loss: 2.537358045578003\n",
      "step 8203 loss: 2.4422924518585205\n",
      "step 8204 loss: 2.4756643772125244\n",
      "step 8205 loss: 2.538529396057129\n",
      "step 8206 loss: 2.5176405906677246\n",
      "step 8207 loss: 2.5299105644226074\n",
      "step 8208 loss: 2.559540033340454\n",
      "step 8209 loss: 2.487093448638916\n",
      "step 8210 loss: 2.4483091831207275\n",
      "step 8211 loss: 2.514788866043091\n",
      "step 8212 loss: 2.4946935176849365\n",
      "step 8213 loss: 2.5656332969665527\n",
      "step 8214 loss: 2.384760856628418\n",
      "step 8215 loss: 2.5561447143554688\n",
      "step 8216 loss: 2.4594507217407227\n",
      "step 8217 loss: 2.433647394180298\n",
      "step 8218 loss: 2.4197349548339844\n",
      "step 8219 loss: 2.5866904258728027\n",
      "step 8220 loss: 2.643857002258301\n",
      "step 8221 loss: 2.4430480003356934\n",
      "step 8222 loss: 2.4060115814208984\n",
      "step 8223 loss: 2.4479076862335205\n",
      "step 8224 loss: 2.4837796688079834\n",
      "step 8225 loss: 2.594271183013916\n",
      "step 8226 loss: 2.6574604511260986\n",
      "step 8227 loss: 2.482384204864502\n",
      "step 8228 loss: 2.3661067485809326\n",
      "step 8229 loss: 2.542912244796753\n",
      "step 8230 loss: 2.5299127101898193\n",
      "step 8231 loss: 2.418333053588867\n",
      "step 8232 loss: 2.5112791061401367\n",
      "step 8233 loss: 2.4063453674316406\n",
      "step 8234 loss: 2.45866322517395\n",
      "step 8235 loss: 2.498060703277588\n",
      "step 8236 loss: 2.507197856903076\n",
      "step 8237 loss: 2.4826712608337402\n",
      "step 8238 loss: 2.5748133659362793\n",
      "step 8239 loss: 2.513366460800171\n",
      "step 8240 loss: 2.485538959503174\n",
      "step 8241 loss: 2.619295597076416\n",
      "step 8242 loss: 2.5227341651916504\n",
      "step 8243 loss: 2.4424338340759277\n",
      "step 8244 loss: 2.3382489681243896\n",
      "step 8245 loss: 2.2809488773345947\n",
      "step 8246 loss: 2.478280544281006\n",
      "step 8247 loss: 2.526113271713257\n",
      "step 8248 loss: 2.515622854232788\n",
      "step 8249 loss: 2.533353805541992\n",
      "step 8250 loss: 2.4087328910827637\n",
      "step 8251 loss: 2.436171054840088\n",
      "step 8252 loss: 2.3329458236694336\n",
      "step 8253 loss: 2.524348020553589\n",
      "step 8254 loss: 2.473569393157959\n",
      "step 8255 loss: 2.425717353820801\n",
      "step 8256 loss: 2.4490904808044434\n",
      "step 8257 loss: 2.3998122215270996\n",
      "step 8258 loss: 2.4525363445281982\n",
      "step 8259 loss: 2.448436975479126\n",
      "step 8260 loss: 2.4740216732025146\n",
      "step 8261 loss: 2.5159926414489746\n",
      "step 8262 loss: 2.5557446479797363\n",
      "step 8263 loss: 2.5474164485931396\n",
      "step 8264 loss: 2.634159564971924\n",
      "step 8265 loss: 2.404123306274414\n",
      "step 8266 loss: 2.584404468536377\n",
      "step 8267 loss: 2.4559719562530518\n",
      "step 8268 loss: 2.409334182739258\n",
      "step 8269 loss: 2.621556282043457\n",
      "step 8270 loss: 2.395458936691284\n",
      "step 8271 loss: 2.398474931716919\n",
      "step 8272 loss: 2.528055429458618\n",
      "step 8273 loss: 2.3378591537475586\n",
      "step 8274 loss: 2.5880560874938965\n",
      "step 8275 loss: 2.4607906341552734\n",
      "step 8276 loss: 2.48502516746521\n",
      "step 8277 loss: 2.3900370597839355\n",
      "step 8278 loss: 2.4360432624816895\n",
      "step 8279 loss: 2.429429531097412\n",
      "step 8280 loss: 2.66829776763916\n",
      "step 8281 loss: 2.5189971923828125\n",
      "step 8282 loss: 2.5522096157073975\n",
      "step 8283 loss: 2.5428550243377686\n",
      "step 8284 loss: 2.3941612243652344\n",
      "step 8285 loss: 2.5748090744018555\n",
      "step 8286 loss: 2.45656418800354\n",
      "step 8287 loss: 2.4679949283599854\n",
      "step 8288 loss: 2.473626136779785\n",
      "step 8289 loss: 2.40978741645813\n",
      "step 8290 loss: 2.512902021408081\n",
      "step 8291 loss: 2.441311836242676\n",
      "step 8292 loss: 2.3282713890075684\n",
      "step 8293 loss: 2.5491714477539062\n",
      "step 8294 loss: 2.4887006282806396\n",
      "step 8295 loss: 2.5239367485046387\n",
      "step 8296 loss: 2.571605920791626\n",
      "step 8297 loss: 2.2778208255767822\n",
      "step 8298 loss: 2.5353219509124756\n",
      "step 8299 loss: 2.4630017280578613\n",
      "step 8300 loss: 2.6117146015167236\n",
      "step 8301 loss: 2.4327409267425537\n",
      "step 8302 loss: 2.466344118118286\n",
      "step 8303 loss: 2.526174783706665\n",
      "step 8304 loss: 2.5333077907562256\n",
      "step 8305 loss: 2.5483901500701904\n",
      "step 8306 loss: 2.493711471557617\n",
      "step 8307 loss: 2.636474609375\n",
      "step 8308 loss: 2.503080368041992\n",
      "step 8309 loss: 2.4845640659332275\n",
      "step 8310 loss: 2.3843393325805664\n",
      "step 8311 loss: 2.3670074939727783\n",
      "step 8312 loss: 2.5137779712677\n",
      "step 8313 loss: 2.525977373123169\n",
      "step 8314 loss: 2.5084080696105957\n",
      "step 8315 loss: 2.4330899715423584\n",
      "step 8316 loss: 2.5662412643432617\n",
      "step 8317 loss: 2.3040082454681396\n",
      "step 8318 loss: 2.55495548248291\n",
      "step 8319 loss: 2.4600062370300293\n",
      "step 8320 loss: 2.4862148761749268\n",
      "step 8321 loss: 2.49383282661438\n",
      "step 8322 loss: 2.360513687133789\n",
      "step 8323 loss: 2.5362024307250977\n",
      "step 8324 loss: 2.329399824142456\n",
      "step 8325 loss: 2.5155673027038574\n",
      "step 8326 loss: 2.5186662673950195\n",
      "step 8327 loss: 2.5014779567718506\n",
      "step 8328 loss: 2.457580804824829\n",
      "step 8329 loss: 2.532543659210205\n",
      "step 8330 loss: 2.4755167961120605\n",
      "step 8331 loss: 2.3479013442993164\n",
      "step 8332 loss: 2.4626216888427734\n",
      "step 8333 loss: 2.5593740940093994\n",
      "step 8334 loss: 2.3869402408599854\n",
      "step 8335 loss: 2.4448673725128174\n",
      "step 8336 loss: 2.480229616165161\n",
      "step 8337 loss: 2.487565517425537\n",
      "step 8338 loss: 2.510075807571411\n",
      "step 8339 loss: 2.411344051361084\n",
      "step 8340 loss: 2.501554489135742\n",
      "step 8341 loss: 2.492647886276245\n",
      "step 8342 loss: 2.3555474281311035\n",
      "step 8343 loss: 2.502711772918701\n",
      "step 8344 loss: 2.4642984867095947\n",
      "step 8345 loss: 2.4653267860412598\n",
      "step 8346 loss: 2.445725917816162\n",
      "step 8347 loss: 2.4458751678466797\n",
      "step 8348 loss: 2.559894323348999\n",
      "step 8349 loss: 2.511260747909546\n",
      "step 8350 loss: 2.545457363128662\n",
      "step 8351 loss: 2.442319393157959\n",
      "step 8352 loss: 2.6043267250061035\n",
      "step 8353 loss: 2.4877004623413086\n",
      "step 8354 loss: 2.426880359649658\n",
      "step 8355 loss: 2.4432272911071777\n",
      "step 8356 loss: 2.4918148517608643\n",
      "step 8357 loss: 2.511809825897217\n",
      "step 8358 loss: 2.6444013118743896\n",
      "step 8359 loss: 2.417548179626465\n",
      "step 8360 loss: 2.538079023361206\n",
      "step 8361 loss: 2.465615749359131\n",
      "step 8362 loss: 2.495466947555542\n",
      "step 8363 loss: 2.5265700817108154\n",
      "step 8364 loss: 2.4447920322418213\n",
      "step 8365 loss: 2.462592124938965\n",
      "step 8366 loss: 2.4806289672851562\n",
      "step 8367 loss: 2.433711290359497\n",
      "step 8368 loss: 2.533172369003296\n",
      "step 8369 loss: 2.5398688316345215\n",
      "step 8370 loss: 2.4324514865875244\n",
      "step 8371 loss: 2.491118907928467\n",
      "step 8372 loss: 2.5066885948181152\n",
      "step 8373 loss: 2.506077527999878\n",
      "step 8374 loss: 2.4905970096588135\n",
      "step 8375 loss: 2.5382962226867676\n",
      "step 8376 loss: 2.5058982372283936\n",
      "step 8377 loss: 2.5151329040527344\n",
      "step 8378 loss: 2.500065326690674\n",
      "step 8379 loss: 2.5749082565307617\n",
      "step 8380 loss: 2.6101324558258057\n",
      "step 8381 loss: 2.4748315811157227\n",
      "step 8382 loss: 2.4477813243865967\n",
      "step 8383 loss: 2.3820695877075195\n",
      "step 8384 loss: 2.5348522663116455\n",
      "step 8385 loss: 2.5263423919677734\n",
      "step 8386 loss: 2.3752946853637695\n",
      "step 8387 loss: 2.472564220428467\n",
      "step 8388 loss: 2.590008020401001\n",
      "step 8389 loss: 2.53855299949646\n",
      "step 8390 loss: 2.465008497238159\n",
      "step 8391 loss: 2.3754310607910156\n",
      "step 8392 loss: 2.4716105461120605\n",
      "step 8393 loss: 2.4710233211517334\n",
      "step 8394 loss: 2.573814868927002\n",
      "step 8395 loss: 2.594200849533081\n",
      "step 8396 loss: 2.4127769470214844\n",
      "step 8397 loss: 2.5342025756835938\n",
      "step 8398 loss: 2.5063223838806152\n",
      "step 8399 loss: 2.559051513671875\n",
      "step 8400 loss: 2.4675140380859375\n",
      "step 8401 loss: 2.5415520668029785\n",
      "step 8402 loss: 2.4144768714904785\n",
      "step 8403 loss: 2.4561078548431396\n",
      "step 8404 loss: 2.4518837928771973\n",
      "step 8405 loss: 2.4844765663146973\n",
      "step 8406 loss: 2.3815133571624756\n",
      "step 8407 loss: 2.4048426151275635\n",
      "step 8408 loss: 2.4975342750549316\n",
      "step 8409 loss: 2.5287928581237793\n",
      "step 8410 loss: 2.390947103500366\n",
      "step 8411 loss: 2.598578453063965\n",
      "step 8412 loss: 2.4998037815093994\n",
      "step 8413 loss: 2.6133430004119873\n",
      "step 8414 loss: 2.4820215702056885\n",
      "step 8415 loss: 2.5540008544921875\n",
      "step 8416 loss: 2.433742046356201\n",
      "step 8417 loss: 2.457559108734131\n",
      "step 8418 loss: 2.4958226680755615\n",
      "step 8419 loss: 2.435178756713867\n",
      "step 8420 loss: 2.443014621734619\n",
      "step 8421 loss: 2.4147486686706543\n",
      "step 8422 loss: 2.4613394737243652\n",
      "step 8423 loss: 2.476301670074463\n",
      "step 8424 loss: 2.4898886680603027\n",
      "step 8425 loss: 2.5051939487457275\n",
      "step 8426 loss: 2.439399480819702\n",
      "step 8427 loss: 2.4871926307678223\n",
      "step 8428 loss: 2.3502705097198486\n",
      "step 8429 loss: 2.517974853515625\n",
      "step 8430 loss: 2.427347183227539\n",
      "step 8431 loss: 2.4076015949249268\n",
      "step 8432 loss: 2.5608174800872803\n",
      "step 8433 loss: 2.3776164054870605\n",
      "step 8434 loss: 2.423335552215576\n",
      "step 8435 loss: 2.306844711303711\n",
      "step 8436 loss: 2.4450182914733887\n",
      "step 8437 loss: 2.3748815059661865\n",
      "step 8438 loss: 2.4785430431365967\n",
      "step 8439 loss: 2.4054982662200928\n",
      "step 8440 loss: 2.525479793548584\n",
      "step 8441 loss: 2.5684897899627686\n",
      "step 8442 loss: 2.5006637573242188\n",
      "step 8443 loss: 2.434828519821167\n",
      "step 8444 loss: 2.438469648361206\n",
      "step 8445 loss: 2.4572839736938477\n",
      "step 8446 loss: 2.5407910346984863\n",
      "step 8447 loss: 2.370577812194824\n",
      "step 8448 loss: 2.4651453495025635\n",
      "step 8449 loss: 2.4704904556274414\n",
      "step 8450 loss: 2.432389259338379\n",
      "step 8451 loss: 2.379699468612671\n",
      "step 8452 loss: 2.458219051361084\n",
      "step 8453 loss: 2.492401599884033\n",
      "step 8454 loss: 2.4402782917022705\n",
      "step 8455 loss: 2.4901723861694336\n",
      "step 8456 loss: 2.4757466316223145\n",
      "step 8457 loss: 2.4416985511779785\n",
      "step 8458 loss: 2.5034070014953613\n",
      "step 8459 loss: 2.4489972591400146\n",
      "step 8460 loss: 2.561154365539551\n",
      "step 8461 loss: 2.398473024368286\n",
      "step 8462 loss: 2.52165150642395\n",
      "step 8463 loss: 2.403008222579956\n",
      "step 8464 loss: 2.4976022243499756\n",
      "step 8465 loss: 2.270615816116333\n",
      "step 8466 loss: 2.3707802295684814\n",
      "step 8467 loss: 2.4841301441192627\n",
      "step 8468 loss: 2.5234971046447754\n",
      "step 8469 loss: 2.476341485977173\n",
      "step 8470 loss: 2.5799343585968018\n",
      "step 8471 loss: 2.3424675464630127\n",
      "step 8472 loss: 2.371648073196411\n",
      "step 8473 loss: 2.490316152572632\n",
      "step 8474 loss: 2.4565632343292236\n",
      "step 8475 loss: 2.4828813076019287\n",
      "step 8476 loss: 2.5439276695251465\n",
      "step 8477 loss: 2.398756742477417\n",
      "step 8478 loss: 2.3383500576019287\n",
      "step 8479 loss: 2.566885232925415\n",
      "step 8480 loss: 2.465639114379883\n",
      "step 8481 loss: 2.490964651107788\n",
      "step 8482 loss: 2.4274580478668213\n",
      "step 8483 loss: 2.5257620811462402\n",
      "step 8484 loss: 2.6136300563812256\n",
      "step 8485 loss: 2.4653258323669434\n",
      "step 8486 loss: 2.5013651847839355\n",
      "step 8487 loss: 2.5657341480255127\n",
      "step 8488 loss: 2.5228092670440674\n",
      "step 8489 loss: 2.586409091949463\n",
      "step 8490 loss: 2.335799217224121\n",
      "step 8491 loss: 2.3889896869659424\n",
      "step 8492 loss: 2.509812831878662\n",
      "step 8493 loss: 2.4619970321655273\n",
      "step 8494 loss: 2.4580860137939453\n",
      "step 8495 loss: 2.520153522491455\n",
      "step 8496 loss: 2.475571632385254\n",
      "step 8497 loss: 2.4969875812530518\n",
      "step 8498 loss: 2.479580879211426\n",
      "step 8499 loss: 2.5017764568328857\n",
      "step 8500 loss: 2.4209394454956055\n",
      "step 8501 loss: 2.4795494079589844\n",
      "step 8502 loss: 2.4652345180511475\n",
      "step 8503 loss: 2.525773763656616\n",
      "step 8504 loss: 2.5341434478759766\n",
      "step 8505 loss: 2.608968496322632\n",
      "step 8506 loss: 2.518768548965454\n",
      "step 8507 loss: 2.4583003520965576\n",
      "step 8508 loss: 2.363469123840332\n",
      "step 8509 loss: 2.571873664855957\n",
      "step 8510 loss: 2.5458221435546875\n",
      "step 8511 loss: 2.42069673538208\n",
      "step 8512 loss: 2.4433326721191406\n",
      "step 8513 loss: 2.490858554840088\n",
      "step 8514 loss: 2.4743573665618896\n",
      "step 8515 loss: 2.5820868015289307\n",
      "step 8516 loss: 2.6147055625915527\n",
      "step 8517 loss: 2.541025400161743\n",
      "step 8518 loss: 2.457260847091675\n",
      "step 8519 loss: 2.409167528152466\n",
      "step 8520 loss: 2.5186386108398438\n",
      "step 8521 loss: 2.377488613128662\n",
      "step 8522 loss: 2.502892017364502\n",
      "step 8523 loss: 2.4858992099761963\n",
      "step 8524 loss: 2.4112443923950195\n",
      "step 8525 loss: 2.4448788166046143\n",
      "step 8526 loss: 2.5093271732330322\n",
      "step 8527 loss: 2.3046865463256836\n",
      "step 8528 loss: 2.522547483444214\n",
      "step 8529 loss: 2.4937381744384766\n",
      "step 8530 loss: 2.441364288330078\n",
      "step 8531 loss: 2.559053897857666\n",
      "step 8532 loss: 2.5816285610198975\n",
      "step 8533 loss: 2.558366537094116\n",
      "step 8534 loss: 2.5660102367401123\n",
      "step 8535 loss: 2.4106879234313965\n",
      "step 8536 loss: 2.401968479156494\n",
      "step 8537 loss: 2.4566681385040283\n",
      "step 8538 loss: 2.533299207687378\n",
      "step 8539 loss: 2.6049845218658447\n",
      "step 8540 loss: 2.4874937534332275\n",
      "step 8541 loss: 2.4756929874420166\n",
      "step 8542 loss: 2.572171211242676\n",
      "step 8543 loss: 2.3783044815063477\n",
      "step 8544 loss: 2.4867067337036133\n",
      "step 8545 loss: 2.484403133392334\n",
      "step 8546 loss: 2.420689105987549\n",
      "step 8547 loss: 2.3774540424346924\n",
      "step 8548 loss: 2.532808542251587\n",
      "step 8549 loss: 2.5008397102355957\n",
      "step 8550 loss: 2.3937559127807617\n",
      "step 8551 loss: 2.5017037391662598\n",
      "step 8552 loss: 2.360278367996216\n",
      "step 8553 loss: 2.474954843521118\n",
      "step 8554 loss: 2.4926321506500244\n",
      "step 8555 loss: 2.503207206726074\n",
      "step 8556 loss: 2.566026210784912\n",
      "step 8557 loss: 2.514298439025879\n",
      "step 8558 loss: 2.4608609676361084\n",
      "step 8559 loss: 2.4949324131011963\n",
      "step 8560 loss: 2.4048922061920166\n",
      "step 8561 loss: 2.529118776321411\n",
      "step 8562 loss: 2.4312384128570557\n",
      "step 8563 loss: 2.3632140159606934\n",
      "step 8564 loss: 2.524994134902954\n",
      "step 8565 loss: 2.543569326400757\n",
      "step 8566 loss: 2.458627223968506\n",
      "step 8567 loss: 2.387272596359253\n",
      "step 8568 loss: 2.4221901893615723\n",
      "step 8569 loss: 2.3697543144226074\n",
      "step 8570 loss: 2.466961622238159\n",
      "step 8571 loss: 2.5058348178863525\n",
      "step 8572 loss: 2.4123647212982178\n",
      "step 8573 loss: 2.605665445327759\n",
      "step 8574 loss: 2.3682479858398438\n",
      "step 8575 loss: 2.5425782203674316\n",
      "step 8576 loss: 2.4597158432006836\n",
      "step 8577 loss: 2.3855655193328857\n",
      "step 8578 loss: 2.580899953842163\n",
      "step 8579 loss: 2.519524097442627\n",
      "step 8580 loss: 2.422571897506714\n",
      "step 8581 loss: 2.4767842292785645\n",
      "step 8582 loss: 2.510841131210327\n",
      "step 8583 loss: 2.414752960205078\n",
      "step 8584 loss: 2.5530292987823486\n",
      "step 8585 loss: 2.418506622314453\n",
      "step 8586 loss: 2.5057380199432373\n",
      "step 8587 loss: 2.478921890258789\n",
      "step 8588 loss: 2.3877618312835693\n",
      "step 8589 loss: 2.4319708347320557\n",
      "step 8590 loss: 2.4653871059417725\n",
      "step 8591 loss: 2.407130718231201\n",
      "step 8592 loss: 2.5705296993255615\n",
      "step 8593 loss: 2.426675319671631\n",
      "step 8594 loss: 2.449856758117676\n",
      "step 8595 loss: 2.5027334690093994\n",
      "step 8596 loss: 2.4363253116607666\n",
      "step 8597 loss: 2.5460610389709473\n",
      "step 8598 loss: 2.4583804607391357\n",
      "step 8599 loss: 2.4604029655456543\n",
      "step 8600 loss: 2.4633748531341553\n",
      "step 8601 loss: 2.452500343322754\n",
      "step 8602 loss: 2.5186381340026855\n",
      "step 8603 loss: 2.4445266723632812\n",
      "step 8604 loss: 2.480708122253418\n",
      "step 8605 loss: 2.6339826583862305\n",
      "step 8606 loss: 2.454622507095337\n",
      "step 8607 loss: 2.496401786804199\n",
      "step 8608 loss: 2.5122785568237305\n",
      "step 8609 loss: 2.4168860912323\n",
      "step 8610 loss: 2.4454848766326904\n",
      "step 8611 loss: 2.4261107444763184\n",
      "step 8612 loss: 2.5183000564575195\n",
      "step 8613 loss: 2.549511671066284\n",
      "step 8614 loss: 2.3425843715667725\n",
      "step 8615 loss: 2.4632766246795654\n",
      "step 8616 loss: 2.456742763519287\n",
      "step 8617 loss: 2.4118502140045166\n",
      "step 8618 loss: 2.6428520679473877\n",
      "step 8619 loss: 2.5274178981781006\n",
      "step 8620 loss: 2.513632297515869\n",
      "step 8621 loss: 2.494091272354126\n",
      "step 8622 loss: 2.5998668670654297\n",
      "step 8623 loss: 2.6218912601470947\n",
      "step 8624 loss: 2.386780023574829\n",
      "step 8625 loss: 2.5420427322387695\n",
      "step 8626 loss: 2.497004985809326\n",
      "step 8627 loss: 2.4917242527008057\n",
      "step 8628 loss: 2.4429097175598145\n",
      "step 8629 loss: 2.5367472171783447\n",
      "step 8630 loss: 2.4308815002441406\n",
      "step 8631 loss: 2.4485433101654053\n",
      "step 8632 loss: 2.4622631072998047\n",
      "step 8633 loss: 2.4262847900390625\n",
      "step 8634 loss: 2.3782572746276855\n",
      "step 8635 loss: 2.545421838760376\n",
      "step 8636 loss: 2.50803804397583\n",
      "step 8637 loss: 2.4456474781036377\n",
      "step 8638 loss: 2.430462598800659\n",
      "step 8639 loss: 2.435598611831665\n",
      "step 8640 loss: 2.3162882328033447\n",
      "step 8641 loss: 2.3652045726776123\n",
      "step 8642 loss: 2.4208643436431885\n",
      "step 8643 loss: 2.4731149673461914\n",
      "step 8644 loss: 2.6301469802856445\n",
      "step 8645 loss: 2.413231134414673\n",
      "step 8646 loss: 2.4858531951904297\n",
      "step 8647 loss: 2.5377256870269775\n",
      "step 8648 loss: 2.3902225494384766\n",
      "step 8649 loss: 2.5323617458343506\n",
      "step 8650 loss: 2.5006103515625\n",
      "step 8651 loss: 2.6284661293029785\n",
      "step 8652 loss: 2.369356870651245\n",
      "step 8653 loss: 2.3485562801361084\n",
      "step 8654 loss: 2.5131189823150635\n",
      "step 8655 loss: 2.60854434967041\n",
      "step 8656 loss: 2.4973771572113037\n",
      "step 8657 loss: 2.513357162475586\n",
      "step 8658 loss: 2.5217785835266113\n",
      "step 8659 loss: 2.5060200691223145\n",
      "step 8660 loss: 2.6573550701141357\n",
      "step 8661 loss: 2.3549747467041016\n",
      "step 8662 loss: 2.447638988494873\n",
      "step 8663 loss: 2.5133252143859863\n",
      "step 8664 loss: 2.5509889125823975\n",
      "step 8665 loss: 2.4458703994750977\n",
      "step 8666 loss: 2.3498692512512207\n",
      "step 8667 loss: 2.491748809814453\n",
      "step 8668 loss: 2.4923129081726074\n",
      "step 8669 loss: 2.3730201721191406\n",
      "step 8670 loss: 2.4721431732177734\n",
      "step 8671 loss: 2.3908233642578125\n",
      "step 8672 loss: 2.4853498935699463\n",
      "step 8673 loss: 2.530945301055908\n",
      "step 8674 loss: 2.327845335006714\n",
      "step 8675 loss: 2.5227813720703125\n",
      "step 8676 loss: 2.4066596031188965\n",
      "step 8677 loss: 2.479858160018921\n",
      "step 8678 loss: 2.714595317840576\n",
      "step 8679 loss: 2.4550514221191406\n",
      "step 8680 loss: 2.498840093612671\n",
      "step 8681 loss: 2.497809648513794\n",
      "step 8682 loss: 2.464411735534668\n",
      "step 8683 loss: 2.4277122020721436\n",
      "step 8684 loss: 2.5906543731689453\n",
      "step 8685 loss: 2.5189192295074463\n",
      "step 8686 loss: 2.388791799545288\n",
      "step 8687 loss: 2.4911653995513916\n",
      "step 8688 loss: 2.437561273574829\n",
      "step 8689 loss: 2.4057304859161377\n",
      "step 8690 loss: 2.438042163848877\n",
      "step 8691 loss: 2.60142183303833\n",
      "step 8692 loss: 2.3414463996887207\n",
      "step 8693 loss: 2.4831459522247314\n",
      "step 8694 loss: 2.4237053394317627\n",
      "step 8695 loss: 2.4872512817382812\n",
      "step 8696 loss: 2.4908394813537598\n",
      "step 8697 loss: 2.5108137130737305\n",
      "step 8698 loss: 2.4478414058685303\n",
      "step 8699 loss: 2.4853744506835938\n",
      "step 8700 loss: 2.4416561126708984\n",
      "step 8701 loss: 2.4715147018432617\n",
      "step 8702 loss: 2.475755214691162\n",
      "step 8703 loss: 2.493480920791626\n",
      "step 8704 loss: 2.462780237197876\n",
      "step 8705 loss: 2.495253801345825\n",
      "step 8706 loss: 2.4051663875579834\n",
      "step 8707 loss: 2.4086389541625977\n",
      "step 8708 loss: 2.46728515625\n",
      "step 8709 loss: 2.4502038955688477\n",
      "step 8710 loss: 2.436310052871704\n",
      "step 8711 loss: 2.567765712738037\n",
      "step 8712 loss: 2.524531602859497\n",
      "step 8713 loss: 2.428907871246338\n",
      "step 8714 loss: 2.465463161468506\n",
      "step 8715 loss: 2.4693145751953125\n",
      "step 8716 loss: 2.4974524974823\n",
      "step 8717 loss: 2.4445323944091797\n",
      "step 8718 loss: 2.4854838848114014\n",
      "step 8719 loss: 2.3613076210021973\n",
      "step 8720 loss: 2.37039852142334\n",
      "step 8721 loss: 2.584425210952759\n",
      "step 8722 loss: 2.600615978240967\n",
      "step 8723 loss: 2.382713556289673\n",
      "step 8724 loss: 2.588461399078369\n",
      "step 8725 loss: 2.393238067626953\n",
      "step 8726 loss: 2.4774742126464844\n",
      "step 8727 loss: 2.517110586166382\n",
      "step 8728 loss: 2.400900363922119\n",
      "step 8729 loss: 2.454976797103882\n",
      "step 8730 loss: 2.474820375442505\n",
      "step 8731 loss: 2.4189627170562744\n",
      "step 8732 loss: 2.4894750118255615\n",
      "step 8733 loss: 2.600537061691284\n",
      "step 8734 loss: 2.452979803085327\n",
      "step 8735 loss: 2.597485065460205\n",
      "step 8736 loss: 2.4741809368133545\n",
      "step 8737 loss: 2.4508652687072754\n",
      "step 8738 loss: 2.440194606781006\n",
      "step 8739 loss: 2.5233514308929443\n",
      "step 8740 loss: 2.3809762001037598\n",
      "step 8741 loss: 2.472090005874634\n",
      "step 8742 loss: 2.5017342567443848\n",
      "step 8743 loss: 2.5152504444122314\n",
      "step 8744 loss: 2.391153573989868\n",
      "step 8745 loss: 2.6057889461517334\n",
      "step 8746 loss: 2.4422707557678223\n",
      "step 8747 loss: 2.4072999954223633\n",
      "step 8748 loss: 2.487321138381958\n",
      "step 8749 loss: 2.5287108421325684\n",
      "step 8750 loss: 2.5030364990234375\n",
      "step 8751 loss: 2.393795967102051\n",
      "step 8752 loss: 2.5400185585021973\n",
      "step 8753 loss: 2.519244909286499\n",
      "step 8754 loss: 2.484530448913574\n",
      "step 8755 loss: 2.441809892654419\n",
      "step 8756 loss: 2.417062282562256\n",
      "step 8757 loss: 2.5175957679748535\n",
      "step 8758 loss: 2.5102241039276123\n",
      "step 8759 loss: 2.4084105491638184\n",
      "step 8760 loss: 2.4692556858062744\n",
      "step 8761 loss: 2.564242124557495\n",
      "step 8762 loss: 2.4342479705810547\n",
      "step 8763 loss: 2.389885425567627\n",
      "step 8764 loss: 2.556333303451538\n",
      "step 8765 loss: 2.467961311340332\n",
      "step 8766 loss: 2.473334550857544\n",
      "step 8767 loss: 2.5536746978759766\n",
      "step 8768 loss: 2.3914952278137207\n",
      "step 8769 loss: 2.5929579734802246\n",
      "step 8770 loss: 2.523292064666748\n",
      "step 8771 loss: 2.5378737449645996\n",
      "step 8772 loss: 2.5525827407836914\n",
      "step 8773 loss: 2.408226251602173\n",
      "step 8774 loss: 2.465094566345215\n",
      "step 8775 loss: 2.414475440979004\n",
      "step 8776 loss: 2.502817153930664\n",
      "step 8777 loss: 2.3719406127929688\n",
      "step 8778 loss: 2.456239938735962\n",
      "step 8779 loss: 2.4752094745635986\n",
      "step 8780 loss: 2.4095728397369385\n",
      "step 8781 loss: 2.5342774391174316\n",
      "step 8782 loss: 2.695605516433716\n",
      "step 8783 loss: 2.435899019241333\n",
      "step 8784 loss: 2.646519184112549\n",
      "step 8785 loss: 2.4349794387817383\n",
      "step 8786 loss: 2.4268150329589844\n",
      "step 8787 loss: 2.60638427734375\n",
      "step 8788 loss: 2.599882125854492\n",
      "step 8789 loss: 2.5370678901672363\n",
      "step 8790 loss: 2.47367000579834\n",
      "step 8791 loss: 2.464656352996826\n",
      "step 8792 loss: 2.480816125869751\n",
      "step 8793 loss: 2.460334300994873\n",
      "step 8794 loss: 2.4168593883514404\n",
      "step 8795 loss: 2.5005061626434326\n",
      "step 8796 loss: 2.516002655029297\n",
      "step 8797 loss: 2.5095770359039307\n",
      "step 8798 loss: 2.530223846435547\n",
      "step 8799 loss: 2.4617302417755127\n",
      "step 8800 loss: 2.634512186050415\n",
      "step 8801 loss: 2.5607717037200928\n",
      "step 8802 loss: 2.5269131660461426\n",
      "step 8803 loss: 2.3816702365875244\n",
      "step 8804 loss: 2.5149359703063965\n",
      "step 8805 loss: 2.4967470169067383\n",
      "step 8806 loss: 2.4585041999816895\n",
      "step 8807 loss: 2.441589117050171\n",
      "step 8808 loss: 2.499743938446045\n",
      "step 8809 loss: 2.4909093379974365\n",
      "step 8810 loss: 2.4962158203125\n",
      "step 8811 loss: 2.479536294937134\n",
      "step 8812 loss: 2.448547840118408\n",
      "step 8813 loss: 2.36102032661438\n",
      "step 8814 loss: 2.435814142227173\n",
      "step 8815 loss: 2.5967276096343994\n",
      "step 8816 loss: 2.44734525680542\n",
      "step 8817 loss: 2.3710520267486572\n",
      "step 8818 loss: 2.472830057144165\n",
      "step 8819 loss: 2.5572359561920166\n",
      "step 8820 loss: 2.4398131370544434\n",
      "step 8821 loss: 2.4076945781707764\n",
      "step 8822 loss: 2.595353126525879\n",
      "step 8823 loss: 2.505510091781616\n",
      "step 8824 loss: 2.551130533218384\n",
      "step 8825 loss: 2.4251327514648438\n",
      "step 8826 loss: 2.4271883964538574\n",
      "step 8827 loss: 2.6052777767181396\n",
      "step 8828 loss: 2.490783214569092\n",
      "step 8829 loss: 2.446528196334839\n",
      "step 8830 loss: 2.430159568786621\n",
      "step 8831 loss: 2.494907855987549\n",
      "step 8832 loss: 2.6443839073181152\n",
      "step 8833 loss: 2.40926456451416\n",
      "step 8834 loss: 2.5232930183410645\n",
      "step 8835 loss: 2.4954466819763184\n",
      "step 8836 loss: 2.377157211303711\n",
      "step 8837 loss: 2.426772117614746\n",
      "step 8838 loss: 2.494290828704834\n",
      "step 8839 loss: 2.4297332763671875\n",
      "step 8840 loss: 2.5411195755004883\n",
      "step 8841 loss: 2.474196672439575\n",
      "step 8842 loss: 2.600905656814575\n",
      "step 8843 loss: 2.5040524005889893\n",
      "step 8844 loss: 2.46025013923645\n",
      "step 8845 loss: 2.465669870376587\n",
      "step 8846 loss: 2.431800603866577\n",
      "step 8847 loss: 2.5200095176696777\n",
      "step 8848 loss: 2.3662960529327393\n",
      "step 8849 loss: 2.505300760269165\n",
      "step 8850 loss: 2.4463484287261963\n",
      "step 8851 loss: 2.5444374084472656\n",
      "step 8852 loss: 2.4543168544769287\n",
      "step 8853 loss: 2.4034929275512695\n",
      "step 8854 loss: 2.4496753215789795\n",
      "step 8855 loss: 2.3711211681365967\n",
      "step 8856 loss: 2.4828574657440186\n",
      "step 8857 loss: 2.3472325801849365\n",
      "step 8858 loss: 2.372549533843994\n",
      "step 8859 loss: 2.3944365978240967\n",
      "step 8860 loss: 2.5164947509765625\n",
      "step 8861 loss: 2.404503583908081\n",
      "step 8862 loss: 2.6396398544311523\n",
      "step 8863 loss: 2.39011549949646\n",
      "step 8864 loss: 2.4728152751922607\n",
      "step 8865 loss: 2.4060287475585938\n",
      "step 8866 loss: 2.332693576812744\n",
      "step 8867 loss: 2.4647388458251953\n",
      "step 8868 loss: 2.528716564178467\n",
      "step 8869 loss: 2.474081039428711\n",
      "step 8870 loss: 2.5186245441436768\n",
      "step 8871 loss: 2.4460628032684326\n",
      "step 8872 loss: 2.497877836227417\n",
      "step 8873 loss: 2.4325954914093018\n",
      "step 8874 loss: 2.4678237438201904\n",
      "step 8875 loss: 2.5793793201446533\n",
      "step 8876 loss: 2.413081169128418\n",
      "step 8877 loss: 2.376577138900757\n",
      "step 8878 loss: 2.4470205307006836\n",
      "step 8879 loss: 2.5362982749938965\n",
      "step 8880 loss: 2.6145944595336914\n",
      "step 8881 loss: 2.447284698486328\n",
      "step 8882 loss: 2.514810085296631\n",
      "step 8883 loss: 2.4322896003723145\n",
      "step 8884 loss: 2.501782178878784\n",
      "step 8885 loss: 2.4392619132995605\n",
      "step 8886 loss: 2.5439441204071045\n",
      "step 8887 loss: 2.4623947143554688\n",
      "step 8888 loss: 2.5670785903930664\n",
      "step 8889 loss: 2.4483747482299805\n",
      "step 8890 loss: 2.5327980518341064\n",
      "step 8891 loss: 2.4294867515563965\n",
      "step 8892 loss: 2.441539764404297\n",
      "step 8893 loss: 2.6218149662017822\n",
      "step 8894 loss: 2.604620933532715\n",
      "step 8895 loss: 2.4207205772399902\n",
      "step 8896 loss: 2.5533623695373535\n",
      "step 8897 loss: 2.4409573078155518\n",
      "step 8898 loss: 2.550792694091797\n",
      "step 8899 loss: 2.482347011566162\n",
      "step 8900 loss: 2.3378663063049316\n",
      "step 8901 loss: 2.414628267288208\n",
      "step 8902 loss: 2.4667155742645264\n",
      "step 8903 loss: 2.369201421737671\n",
      "step 8904 loss: 2.434083938598633\n",
      "step 8905 loss: 2.5918753147125244\n",
      "step 8906 loss: 2.5500965118408203\n",
      "step 8907 loss: 2.5186831951141357\n",
      "step 8908 loss: 2.5392184257507324\n",
      "step 8909 loss: 2.515291929244995\n",
      "step 8910 loss: 2.3367230892181396\n",
      "step 8911 loss: 2.4765541553497314\n",
      "step 8912 loss: 2.4014151096343994\n",
      "step 8913 loss: 2.3826262950897217\n",
      "step 8914 loss: 2.45896315574646\n",
      "step 8915 loss: 2.4969193935394287\n",
      "step 8916 loss: 2.5249271392822266\n",
      "step 8917 loss: 2.5374228954315186\n",
      "step 8918 loss: 2.5328667163848877\n",
      "step 8919 loss: 2.5471994876861572\n",
      "step 8920 loss: 2.5236880779266357\n",
      "step 8921 loss: 2.3902924060821533\n",
      "step 8922 loss: 2.484844923019409\n",
      "step 8923 loss: 2.4324162006378174\n",
      "step 8924 loss: 2.491786241531372\n",
      "step 8925 loss: 2.4745216369628906\n",
      "step 8926 loss: 2.4724512100219727\n",
      "step 8927 loss: 2.490607738494873\n",
      "step 8928 loss: 2.4333417415618896\n",
      "step 8929 loss: 2.459895372390747\n",
      "step 8930 loss: 2.5772974491119385\n",
      "step 8931 loss: 2.4948902130126953\n",
      "step 8932 loss: 2.487626791000366\n",
      "step 8933 loss: 2.4634532928466797\n",
      "step 8934 loss: 2.4625613689422607\n",
      "step 8935 loss: 2.4613852500915527\n",
      "step 8936 loss: 2.3899543285369873\n",
      "step 8937 loss: 2.4520957469940186\n",
      "step 8938 loss: 2.515183210372925\n",
      "step 8939 loss: 2.3439106941223145\n",
      "step 8940 loss: 2.486185312271118\n",
      "step 8941 loss: 2.568810224533081\n",
      "step 8942 loss: 2.444444417953491\n",
      "step 8943 loss: 2.4661028385162354\n",
      "step 8944 loss: 2.38454270362854\n",
      "step 8945 loss: 2.5336742401123047\n",
      "step 8946 loss: 2.3723154067993164\n",
      "step 8947 loss: 2.5199532508850098\n",
      "step 8948 loss: 2.5485498905181885\n",
      "step 8949 loss: 2.6362338066101074\n",
      "step 8950 loss: 2.6087357997894287\n",
      "step 8951 loss: 2.4689743518829346\n",
      "step 8952 loss: 2.476672410964966\n",
      "step 8953 loss: 2.3645315170288086\n",
      "step 8954 loss: 2.528308391571045\n",
      "step 8955 loss: 2.501929521560669\n",
      "step 8956 loss: 2.4667510986328125\n",
      "step 8957 loss: 2.405507802963257\n",
      "step 8958 loss: 2.513643980026245\n",
      "step 8959 loss: 2.5083329677581787\n",
      "step 8960 loss: 2.427536964416504\n",
      "step 8961 loss: 2.4591832160949707\n",
      "step 8962 loss: 2.5204381942749023\n",
      "step 8963 loss: 2.556706428527832\n",
      "step 8964 loss: 2.4633021354675293\n",
      "step 8965 loss: 2.3608222007751465\n",
      "step 8966 loss: 2.491410255432129\n",
      "step 8967 loss: 2.4717206954956055\n",
      "step 8968 loss: 2.5004591941833496\n",
      "step 8969 loss: 2.331329107284546\n",
      "step 8970 loss: 2.4902827739715576\n",
      "step 8971 loss: 2.4058825969696045\n",
      "step 8972 loss: 2.4604697227478027\n",
      "step 8973 loss: 2.3644111156463623\n",
      "step 8974 loss: 2.4968578815460205\n",
      "step 8975 loss: 2.361395835876465\n",
      "step 8976 loss: 2.504016160964966\n",
      "step 8977 loss: 2.5636165142059326\n",
      "step 8978 loss: 2.5081217288970947\n",
      "step 8979 loss: 2.381593942642212\n",
      "step 8980 loss: 2.4083445072174072\n",
      "step 8981 loss: 2.386953592300415\n",
      "step 8982 loss: 2.3382036685943604\n",
      "step 8983 loss: 2.460124969482422\n",
      "step 8984 loss: 2.5755913257598877\n",
      "step 8985 loss: 2.487790107727051\n",
      "step 8986 loss: 2.409228801727295\n",
      "step 8987 loss: 2.472982883453369\n",
      "step 8988 loss: 2.6218903064727783\n",
      "step 8989 loss: 2.522773027420044\n",
      "step 8990 loss: 2.6636152267456055\n",
      "step 8991 loss: 2.486158609390259\n",
      "step 8992 loss: 2.4516096115112305\n",
      "step 8993 loss: 2.4518842697143555\n",
      "step 8994 loss: 2.510240316390991\n",
      "step 8995 loss: 2.5016348361968994\n",
      "step 8996 loss: 2.463620901107788\n",
      "step 8997 loss: 2.401662588119507\n",
      "step 8998 loss: 2.3344151973724365\n",
      "step 8999 loss: 2.40480637550354\n",
      "step 9000 loss: 2.4838879108428955\n",
      "step 9001 loss: 2.384921073913574\n",
      "step 9002 loss: 2.5104174613952637\n",
      "step 9003 loss: 2.6078999042510986\n",
      "step 9004 loss: 2.3859636783599854\n",
      "step 9005 loss: 2.461421251296997\n",
      "step 9006 loss: 2.512284755706787\n",
      "step 9007 loss: 2.498438835144043\n",
      "step 9008 loss: 2.560758113861084\n",
      "step 9009 loss: 2.3805153369903564\n",
      "step 9010 loss: 2.4449989795684814\n",
      "step 9011 loss: 2.400688886642456\n",
      "step 9012 loss: 2.3931522369384766\n",
      "step 9013 loss: 2.5436601638793945\n",
      "step 9014 loss: 2.4647998809814453\n",
      "step 9015 loss: 2.370436191558838\n",
      "step 9016 loss: 2.495975971221924\n",
      "step 9017 loss: 2.343329906463623\n",
      "step 9018 loss: 2.3503692150115967\n",
      "step 9019 loss: 2.370062828063965\n",
      "step 9020 loss: 2.508294105529785\n",
      "step 9021 loss: 2.4579644203186035\n",
      "step 9022 loss: 2.4820477962493896\n",
      "step 9023 loss: 2.5083959102630615\n",
      "step 9024 loss: 2.479285717010498\n",
      "step 9025 loss: 2.424631118774414\n",
      "step 9026 loss: 2.468188762664795\n",
      "step 9027 loss: 2.3814985752105713\n",
      "step 9028 loss: 2.5116000175476074\n",
      "step 9029 loss: 2.502943992614746\n",
      "step 9030 loss: 2.3888349533081055\n",
      "step 9031 loss: 2.4274771213531494\n",
      "step 9032 loss: 2.363450527191162\n",
      "step 9033 loss: 2.5057919025421143\n",
      "step 9034 loss: 2.421351671218872\n",
      "step 9035 loss: 2.486788511276245\n",
      "step 9036 loss: 2.5156240463256836\n",
      "step 9037 loss: 2.5480477809906006\n",
      "step 9038 loss: 2.388753652572632\n",
      "step 9039 loss: 2.565694808959961\n",
      "step 9040 loss: 2.4111881256103516\n",
      "step 9041 loss: 2.518497943878174\n",
      "step 9042 loss: 2.432143449783325\n",
      "step 9043 loss: 2.6062896251678467\n",
      "step 9044 loss: 2.468569040298462\n",
      "step 9045 loss: 2.5346298217773438\n",
      "step 9046 loss: 2.3954851627349854\n",
      "step 9047 loss: 2.506505012512207\n",
      "step 9048 loss: 2.4577674865722656\n",
      "step 9049 loss: 2.3491902351379395\n",
      "step 9050 loss: 2.4265947341918945\n",
      "step 9051 loss: 2.4807381629943848\n",
      "step 9052 loss: 2.4906399250030518\n",
      "step 9053 loss: 2.5242717266082764\n",
      "step 9054 loss: 2.4774348735809326\n",
      "step 9055 loss: 2.4589314460754395\n",
      "step 9056 loss: 2.4531991481781006\n",
      "step 9057 loss: 2.4938950538635254\n",
      "step 9058 loss: 2.5096988677978516\n",
      "step 9059 loss: 2.517878293991089\n",
      "step 9060 loss: 2.4176599979400635\n",
      "step 9061 loss: 2.428602457046509\n",
      "step 9062 loss: 2.4598729610443115\n",
      "step 9063 loss: 2.4006683826446533\n",
      "step 9064 loss: 2.4805850982666016\n",
      "step 9065 loss: 2.674377918243408\n",
      "step 9066 loss: 2.542360544204712\n",
      "step 9067 loss: 2.473665475845337\n",
      "step 9068 loss: 2.5697665214538574\n",
      "step 9069 loss: 2.435915470123291\n",
      "step 9070 loss: 2.3253159523010254\n",
      "step 9071 loss: 2.4581947326660156\n",
      "step 9072 loss: 2.439695358276367\n",
      "step 9073 loss: 2.4550492763519287\n",
      "step 9074 loss: 2.473325490951538\n",
      "step 9075 loss: 2.4382705688476562\n",
      "step 9076 loss: 2.496441125869751\n",
      "step 9077 loss: 2.4825572967529297\n",
      "step 9078 loss: 2.5288848876953125\n",
      "step 9079 loss: 2.4555373191833496\n",
      "step 9080 loss: 2.542213201522827\n",
      "step 9081 loss: 2.477841377258301\n",
      "step 9082 loss: 2.4810965061187744\n",
      "step 9083 loss: 2.443070650100708\n",
      "step 9084 loss: 2.455024242401123\n",
      "step 9085 loss: 2.494980812072754\n",
      "step 9086 loss: 2.3682026863098145\n",
      "step 9087 loss: 2.4259109497070312\n",
      "step 9088 loss: 2.5066967010498047\n",
      "step 9089 loss: 2.3973984718322754\n",
      "step 9090 loss: 2.5501315593719482\n",
      "step 9091 loss: 2.410823106765747\n",
      "step 9092 loss: 2.4009172916412354\n",
      "step 9093 loss: 2.5274417400360107\n",
      "step 9094 loss: 2.492522716522217\n",
      "step 9095 loss: 2.4593656063079834\n",
      "step 9096 loss: 2.564072847366333\n",
      "step 9097 loss: 2.56400728225708\n",
      "step 9098 loss: 2.3816280364990234\n",
      "step 9099 loss: 2.594815731048584\n",
      "step 9100 loss: 2.534738779067993\n",
      "step 9101 loss: 2.459930658340454\n",
      "step 9102 loss: 2.4888548851013184\n",
      "step 9103 loss: 2.4302306175231934\n",
      "step 9104 loss: 2.4893009662628174\n",
      "step 9105 loss: 2.4074337482452393\n",
      "step 9106 loss: 2.6394245624542236\n",
      "step 9107 loss: 2.460275173187256\n",
      "step 9108 loss: 2.5441644191741943\n",
      "step 9109 loss: 2.5196444988250732\n",
      "step 9110 loss: 2.4934756755828857\n",
      "step 9111 loss: 2.4426913261413574\n",
      "step 9112 loss: 2.50409197807312\n",
      "step 9113 loss: 2.5922482013702393\n",
      "step 9114 loss: 2.4845054149627686\n",
      "step 9115 loss: 2.428149938583374\n",
      "step 9116 loss: 2.5691986083984375\n",
      "step 9117 loss: 2.4431300163269043\n",
      "step 9118 loss: 2.546138048171997\n",
      "step 9119 loss: 2.484652519226074\n",
      "step 9120 loss: 2.4728190898895264\n",
      "step 9121 loss: 2.4974987506866455\n",
      "step 9122 loss: 2.5239014625549316\n",
      "step 9123 loss: 2.5454392433166504\n",
      "step 9124 loss: 2.504587411880493\n",
      "step 9125 loss: 2.5183920860290527\n",
      "step 9126 loss: 2.4315357208251953\n",
      "step 9127 loss: 2.4607691764831543\n",
      "step 9128 loss: 2.453139543533325\n",
      "step 9129 loss: 2.5945279598236084\n",
      "step 9130 loss: 2.5111589431762695\n",
      "step 9131 loss: 2.509476661682129\n",
      "step 9132 loss: 2.328064203262329\n",
      "step 9133 loss: 2.3884615898132324\n",
      "step 9134 loss: 2.39682674407959\n",
      "step 9135 loss: 2.3860156536102295\n",
      "step 9136 loss: 2.453763484954834\n",
      "step 9137 loss: 2.5527231693267822\n",
      "step 9138 loss: 2.497319221496582\n",
      "step 9139 loss: 2.5139288902282715\n",
      "step 9140 loss: 2.429898977279663\n",
      "step 9141 loss: 2.5124056339263916\n",
      "step 9142 loss: 2.450321912765503\n",
      "step 9143 loss: 2.6500556468963623\n",
      "step 9144 loss: 2.4293417930603027\n",
      "step 9145 loss: 2.4616734981536865\n",
      "step 9146 loss: 2.470170736312866\n",
      "step 9147 loss: 2.5425398349761963\n",
      "step 9148 loss: 2.4684653282165527\n",
      "step 9149 loss: 2.4427506923675537\n",
      "step 9150 loss: 2.44576096534729\n",
      "step 9151 loss: 2.5586867332458496\n",
      "step 9152 loss: 2.4528493881225586\n",
      "step 9153 loss: 2.4630484580993652\n",
      "step 9154 loss: 2.4996984004974365\n",
      "step 9155 loss: 2.5085904598236084\n",
      "step 9156 loss: 2.396850824356079\n",
      "step 9157 loss: 2.4152190685272217\n",
      "step 9158 loss: 2.452810764312744\n",
      "step 9159 loss: 2.3742141723632812\n",
      "step 9160 loss: 2.5164382457733154\n",
      "step 9161 loss: 2.560955047607422\n",
      "step 9162 loss: 2.3643815517425537\n",
      "step 9163 loss: 2.5276684761047363\n",
      "step 9164 loss: 2.4902210235595703\n",
      "step 9165 loss: 2.3976311683654785\n",
      "step 9166 loss: 2.469667673110962\n",
      "step 9167 loss: 2.5108048915863037\n",
      "step 9168 loss: 2.444565534591675\n",
      "step 9169 loss: 2.4376444816589355\n",
      "step 9170 loss: 2.59147047996521\n",
      "step 9171 loss: 2.4749374389648438\n",
      "step 9172 loss: 2.517287015914917\n",
      "step 9173 loss: 2.388131856918335\n",
      "step 9174 loss: 2.419830322265625\n",
      "step 9175 loss: 2.4246749877929688\n",
      "step 9176 loss: 2.4831011295318604\n",
      "step 9177 loss: 2.4055442810058594\n",
      "step 9178 loss: 2.530449867248535\n",
      "step 9179 loss: 2.44538950920105\n",
      "step 9180 loss: 2.4614717960357666\n",
      "step 9181 loss: 2.3528003692626953\n",
      "step 9182 loss: 2.5811848640441895\n",
      "step 9183 loss: 2.476693630218506\n",
      "step 9184 loss: 2.4883835315704346\n",
      "step 9185 loss: 2.504563331604004\n",
      "step 9186 loss: 2.513927698135376\n",
      "step 9187 loss: 2.429858684539795\n",
      "step 9188 loss: 2.40317702293396\n",
      "step 9189 loss: 2.454841375350952\n",
      "step 9190 loss: 2.4767496585845947\n",
      "step 9191 loss: 2.5929622650146484\n",
      "step 9192 loss: 2.5156586170196533\n",
      "step 9193 loss: 2.505234479904175\n",
      "step 9194 loss: 2.4519693851470947\n",
      "step 9195 loss: 2.5156688690185547\n",
      "step 9196 loss: 2.4394354820251465\n",
      "step 9197 loss: 2.4478931427001953\n",
      "step 9198 loss: 2.413912534713745\n",
      "step 9199 loss: 2.440729856491089\n",
      "step 9200 loss: 2.58534574508667\n",
      "step 9201 loss: 2.41520357131958\n",
      "step 9202 loss: 2.4481561183929443\n",
      "step 9203 loss: 2.3607568740844727\n",
      "step 9204 loss: 2.4947404861450195\n",
      "step 9205 loss: 2.5013372898101807\n",
      "step 9206 loss: 2.464747905731201\n",
      "step 9207 loss: 2.509140968322754\n",
      "step 9208 loss: 2.451047658920288\n",
      "step 9209 loss: 2.5394585132598877\n",
      "step 9210 loss: 2.4111499786376953\n",
      "step 9211 loss: 2.5921685695648193\n",
      "step 9212 loss: 2.4390108585357666\n",
      "step 9213 loss: 2.419369697570801\n",
      "step 9214 loss: 2.5605263710021973\n",
      "step 9215 loss: 2.410367727279663\n",
      "step 9216 loss: 2.486393928527832\n",
      "step 9217 loss: 2.486661911010742\n",
      "step 9218 loss: 2.5140833854675293\n",
      "step 9219 loss: 2.516119956970215\n",
      "step 9220 loss: 2.616591453552246\n",
      "step 9221 loss: 2.472330331802368\n",
      "step 9222 loss: 2.474287509918213\n",
      "step 9223 loss: 2.5275166034698486\n",
      "step 9224 loss: 2.306148052215576\n",
      "step 9225 loss: 2.5178205966949463\n",
      "step 9226 loss: 2.5271098613739014\n",
      "step 9227 loss: 2.526047945022583\n",
      "step 9228 loss: 2.430034637451172\n",
      "step 9229 loss: 2.4723825454711914\n",
      "step 9230 loss: 2.4772300720214844\n",
      "step 9231 loss: 2.5739586353302\n",
      "step 9232 loss: 2.484245538711548\n",
      "step 9233 loss: 2.434065103530884\n",
      "step 9234 loss: 2.4623615741729736\n",
      "step 9235 loss: 2.466799259185791\n",
      "step 9236 loss: 2.4002597332000732\n",
      "step 9237 loss: 2.40859055519104\n",
      "step 9238 loss: 2.4253315925598145\n",
      "step 9239 loss: 2.3604793548583984\n",
      "step 9240 loss: 2.56846022605896\n",
      "step 9241 loss: 2.5244736671447754\n",
      "step 9242 loss: 2.334533452987671\n",
      "step 9243 loss: 2.4813623428344727\n",
      "step 9244 loss: 2.438101053237915\n",
      "step 9245 loss: 2.4787023067474365\n",
      "step 9246 loss: 2.447474718093872\n",
      "step 9247 loss: 2.498908281326294\n",
      "step 9248 loss: 2.5145936012268066\n",
      "step 9249 loss: 2.4745442867279053\n",
      "step 9250 loss: 2.4942257404327393\n",
      "step 9251 loss: 2.4754695892333984\n",
      "step 9252 loss: 2.373929023742676\n",
      "step 9253 loss: 2.514547348022461\n",
      "step 9254 loss: 2.492476224899292\n",
      "step 9255 loss: 2.4883015155792236\n",
      "step 9256 loss: 2.4604334831237793\n",
      "step 9257 loss: 2.4366965293884277\n",
      "step 9258 loss: 2.419053554534912\n",
      "step 9259 loss: 2.4636597633361816\n",
      "step 9260 loss: 2.522426128387451\n",
      "step 9261 loss: 2.516953468322754\n",
      "step 9262 loss: 2.468493700027466\n",
      "step 9263 loss: 2.545546054840088\n",
      "step 9264 loss: 2.5962207317352295\n",
      "step 9265 loss: 2.5884897708892822\n",
      "step 9266 loss: 2.434094190597534\n",
      "step 9267 loss: 2.4162790775299072\n",
      "step 9268 loss: 2.5969398021698\n",
      "step 9269 loss: 2.5308656692504883\n",
      "step 9270 loss: 2.4598045349121094\n",
      "step 9271 loss: 2.45503306388855\n",
      "step 9272 loss: 2.577967882156372\n",
      "step 9273 loss: 2.518789529800415\n",
      "step 9274 loss: 2.4106452465057373\n",
      "step 9275 loss: 2.5819146633148193\n",
      "step 9276 loss: 2.524651527404785\n",
      "step 9277 loss: 2.4706244468688965\n",
      "step 9278 loss: 2.516251564025879\n",
      "step 9279 loss: 2.490372657775879\n",
      "step 9280 loss: 2.4689109325408936\n",
      "step 9281 loss: 2.399523973464966\n",
      "step 9282 loss: 2.5343446731567383\n",
      "step 9283 loss: 2.538031578063965\n",
      "step 9284 loss: 2.4329211711883545\n",
      "step 9285 loss: 2.5074729919433594\n",
      "step 9286 loss: 2.470515251159668\n",
      "step 9287 loss: 2.551508665084839\n",
      "step 9288 loss: 2.4193105697631836\n",
      "step 9289 loss: 2.5339057445526123\n",
      "step 9290 loss: 2.508272886276245\n",
      "step 9291 loss: 2.5570006370544434\n",
      "step 9292 loss: 2.3484292030334473\n",
      "step 9293 loss: 2.4862520694732666\n",
      "step 9294 loss: 2.4384734630584717\n",
      "step 9295 loss: 2.4225010871887207\n",
      "step 9296 loss: 2.3726418018341064\n",
      "step 9297 loss: 2.569793224334717\n",
      "step 9298 loss: 2.457973003387451\n",
      "step 9299 loss: 2.4733006954193115\n",
      "step 9300 loss: 2.3732097148895264\n",
      "step 9301 loss: 2.522707223892212\n",
      "step 9302 loss: 2.410959243774414\n",
      "step 9303 loss: 2.4664177894592285\n",
      "step 9304 loss: 2.483781099319458\n",
      "step 9305 loss: 2.351487874984741\n",
      "step 9306 loss: 2.4666271209716797\n",
      "step 9307 loss: 2.416746139526367\n",
      "step 9308 loss: 2.4009883403778076\n",
      "step 9309 loss: 2.4128289222717285\n",
      "step 9310 loss: 2.5192368030548096\n",
      "step 9311 loss: 2.5622212886810303\n",
      "step 9312 loss: 2.614927291870117\n",
      "step 9313 loss: 2.478961706161499\n",
      "step 9314 loss: 2.429090976715088\n",
      "step 9315 loss: 2.4716877937316895\n",
      "step 9316 loss: 2.419187307357788\n",
      "step 9317 loss: 2.5201878547668457\n",
      "step 9318 loss: 2.540806293487549\n",
      "step 9319 loss: 2.4287612438201904\n",
      "step 9320 loss: 2.431645154953003\n",
      "step 9321 loss: 2.4453225135803223\n",
      "step 9322 loss: 2.2772281169891357\n",
      "step 9323 loss: 2.4390146732330322\n",
      "step 9324 loss: 2.448577880859375\n",
      "step 9325 loss: 2.4923410415649414\n",
      "step 9326 loss: 2.541956901550293\n",
      "step 9327 loss: 2.5754215717315674\n",
      "step 9328 loss: 2.5099446773529053\n",
      "step 9329 loss: 2.3601906299591064\n",
      "step 9330 loss: 2.5257017612457275\n",
      "step 9331 loss: 2.5382440090179443\n",
      "step 9332 loss: 2.512840747833252\n",
      "step 9333 loss: 2.5981380939483643\n",
      "step 9334 loss: 2.5548055171966553\n",
      "step 9335 loss: 2.410548686981201\n",
      "step 9336 loss: 2.5407326221466064\n",
      "step 9337 loss: 2.5367202758789062\n",
      "step 9338 loss: 2.315180540084839\n",
      "step 9339 loss: 2.517909049987793\n",
      "step 9340 loss: 2.3659679889678955\n",
      "step 9341 loss: 2.4719529151916504\n",
      "step 9342 loss: 2.582883834838867\n",
      "step 9343 loss: 2.527599811553955\n",
      "step 9344 loss: 2.5453896522521973\n",
      "step 9345 loss: 2.4699525833129883\n",
      "step 9346 loss: 2.588299512863159\n",
      "step 9347 loss: 2.356013059616089\n",
      "step 9348 loss: 2.4041080474853516\n",
      "step 9349 loss: 2.33206844329834\n",
      "step 9350 loss: 2.462275981903076\n",
      "step 9351 loss: 2.4161007404327393\n",
      "step 9352 loss: 2.4214131832122803\n",
      "step 9353 loss: 2.346994400024414\n",
      "step 9354 loss: 2.4959938526153564\n",
      "step 9355 loss: 2.6362802982330322\n",
      "step 9356 loss: 2.541613817214966\n",
      "step 9357 loss: 2.455427885055542\n",
      "step 9358 loss: 2.489521026611328\n",
      "step 9359 loss: 2.553393602371216\n",
      "step 9360 loss: 2.44651198387146\n",
      "step 9361 loss: 2.499467372894287\n",
      "step 9362 loss: 2.541883707046509\n",
      "step 9363 loss: 2.4119513034820557\n",
      "step 9364 loss: 2.519277572631836\n",
      "step 9365 loss: 2.4647672176361084\n",
      "step 9366 loss: 2.5514142513275146\n",
      "step 9367 loss: 2.4506945610046387\n",
      "step 9368 loss: 2.569314956665039\n",
      "step 9369 loss: 2.435598611831665\n",
      "step 9370 loss: 2.459585189819336\n",
      "step 9371 loss: 2.5867180824279785\n",
      "step 9372 loss: 2.35378098487854\n",
      "step 9373 loss: 2.492643117904663\n",
      "step 9374 loss: 2.4887800216674805\n",
      "step 9375 loss: 2.426992654800415\n",
      "step 9376 loss: 2.465028762817383\n",
      "step 9377 loss: 2.4211251735687256\n",
      "step 9378 loss: 2.4696526527404785\n",
      "step 9379 loss: 2.451997756958008\n",
      "step 9380 loss: 2.46553373336792\n",
      "step 9381 loss: 2.3958191871643066\n",
      "step 9382 loss: 2.4810400009155273\n",
      "step 9383 loss: 2.4437673091888428\n",
      "step 9384 loss: 2.5438907146453857\n",
      "step 9385 loss: 2.485130548477173\n",
      "step 9386 loss: 2.4492244720458984\n",
      "step 9387 loss: 2.5684986114501953\n",
      "step 9388 loss: 2.3490419387817383\n",
      "step 9389 loss: 2.4035580158233643\n",
      "step 9390 loss: 2.405027389526367\n",
      "step 9391 loss: 2.427163600921631\n",
      "step 9392 loss: 2.4561049938201904\n",
      "step 9393 loss: 2.3250954151153564\n",
      "step 9394 loss: 2.4997646808624268\n",
      "step 9395 loss: 2.4032905101776123\n",
      "step 9396 loss: 2.3396706581115723\n",
      "step 9397 loss: 2.3795623779296875\n",
      "step 9398 loss: 2.6008806228637695\n",
      "step 9399 loss: 2.4691579341888428\n",
      "step 9400 loss: 2.4463818073272705\n",
      "step 9401 loss: 2.479790210723877\n",
      "step 9402 loss: 2.3316121101379395\n",
      "step 9403 loss: 2.3663408756256104\n",
      "step 9404 loss: 2.4473092555999756\n",
      "step 9405 loss: 2.510410785675049\n",
      "step 9406 loss: 2.4833998680114746\n",
      "step 9407 loss: 2.5039358139038086\n",
      "step 9408 loss: 2.4670658111572266\n",
      "step 9409 loss: 2.510295867919922\n",
      "step 9410 loss: 2.40755558013916\n",
      "step 9411 loss: 2.5675272941589355\n",
      "step 9412 loss: 2.4537150859832764\n",
      "step 9413 loss: 2.4485223293304443\n",
      "step 9414 loss: 2.4898605346679688\n",
      "step 9415 loss: 2.4537878036499023\n",
      "step 9416 loss: 2.374945878982544\n",
      "step 9417 loss: 2.6323771476745605\n",
      "step 9418 loss: 2.3632397651672363\n",
      "step 9419 loss: 2.4209251403808594\n",
      "step 9420 loss: 2.5468192100524902\n",
      "step 9421 loss: 2.472893238067627\n",
      "step 9422 loss: 2.42925763130188\n",
      "step 9423 loss: 2.456751823425293\n",
      "step 9424 loss: 2.361175775527954\n",
      "step 9425 loss: 2.4550721645355225\n",
      "step 9426 loss: 2.4377238750457764\n",
      "step 9427 loss: 2.5403356552124023\n",
      "step 9428 loss: 2.4324779510498047\n",
      "step 9429 loss: 2.4213898181915283\n",
      "step 9430 loss: 2.526637077331543\n",
      "step 9431 loss: 2.5384883880615234\n",
      "step 9432 loss: 2.5190136432647705\n",
      "step 9433 loss: 2.5029685497283936\n",
      "step 9434 loss: 2.476135015487671\n",
      "step 9435 loss: 2.4057154655456543\n",
      "step 9436 loss: 2.5352797508239746\n",
      "step 9437 loss: 2.5151636600494385\n",
      "step 9438 loss: 2.4699971675872803\n",
      "step 9439 loss: 2.4514880180358887\n",
      "step 9440 loss: 2.499342918395996\n",
      "step 9441 loss: 2.5405936241149902\n",
      "step 9442 loss: 2.5953006744384766\n",
      "step 9443 loss: 2.5067992210388184\n",
      "step 9444 loss: 2.496187686920166\n",
      "step 9445 loss: 2.3535044193267822\n",
      "step 9446 loss: 2.435056209564209\n",
      "step 9447 loss: 2.50479793548584\n",
      "step 9448 loss: 2.444225549697876\n",
      "step 9449 loss: 2.582934856414795\n",
      "step 9450 loss: 2.374795436859131\n",
      "step 9451 loss: 2.484912633895874\n",
      "step 9452 loss: 2.51547908782959\n",
      "step 9453 loss: 2.4808244705200195\n",
      "step 9454 loss: 2.449467897415161\n",
      "step 9455 loss: 2.3545165061950684\n",
      "step 9456 loss: 2.472576141357422\n",
      "step 9457 loss: 2.4862844944000244\n",
      "step 9458 loss: 2.360503673553467\n",
      "step 9459 loss: 2.6158382892608643\n",
      "step 9460 loss: 2.5653951168060303\n",
      "step 9461 loss: 2.4769561290740967\n",
      "step 9462 loss: 2.3826212882995605\n",
      "step 9463 loss: 2.496102809906006\n",
      "step 9464 loss: 2.408154249191284\n",
      "step 9465 loss: 2.5204355716705322\n",
      "step 9466 loss: 2.429929256439209\n",
      "step 9467 loss: 2.4403464794158936\n",
      "step 9468 loss: 2.3673574924468994\n",
      "step 9469 loss: 2.4343812465667725\n",
      "step 9470 loss: 2.477313756942749\n",
      "step 9471 loss: 2.5414669513702393\n",
      "step 9472 loss: 2.429450750350952\n",
      "step 9473 loss: 2.4157257080078125\n",
      "step 9474 loss: 2.4386470317840576\n",
      "step 9475 loss: 2.4510366916656494\n",
      "step 9476 loss: 2.4807145595550537\n",
      "step 9477 loss: 2.4964659214019775\n",
      "step 9478 loss: 2.450629711151123\n",
      "step 9479 loss: 2.4453580379486084\n",
      "step 9480 loss: 2.401190757751465\n",
      "step 9481 loss: 2.478937864303589\n",
      "step 9482 loss: 2.4651172161102295\n",
      "step 9483 loss: 2.6178486347198486\n",
      "step 9484 loss: 2.440826654434204\n",
      "step 9485 loss: 2.647367477416992\n",
      "step 9486 loss: 2.5532827377319336\n",
      "step 9487 loss: 2.501891613006592\n",
      "step 9488 loss: 2.4431285858154297\n",
      "step 9489 loss: 2.4303951263427734\n",
      "step 9490 loss: 2.4290125370025635\n",
      "step 9491 loss: 2.4928665161132812\n",
      "step 9492 loss: 2.5600528717041016\n",
      "step 9493 loss: 2.5973827838897705\n",
      "step 9494 loss: 2.542325258255005\n",
      "step 9495 loss: 2.498629093170166\n",
      "step 9496 loss: 2.4592666625976562\n",
      "step 9497 loss: 2.3884458541870117\n",
      "step 9498 loss: 2.5459794998168945\n",
      "step 9499 loss: 2.442556381225586\n",
      "step 9500 loss: 2.407996892929077\n",
      "step 9501 loss: 2.328618049621582\n",
      "step 9502 loss: 2.569030523300171\n",
      "step 9503 loss: 2.4675137996673584\n",
      "step 9504 loss: 2.4380338191986084\n",
      "step 9505 loss: 2.477060317993164\n",
      "step 9506 loss: 2.573878049850464\n",
      "step 9507 loss: 2.4865167140960693\n",
      "step 9508 loss: 2.3252999782562256\n",
      "step 9509 loss: 2.300806999206543\n",
      "step 9510 loss: 2.386991262435913\n",
      "step 9511 loss: 2.457367420196533\n",
      "step 9512 loss: 2.385455846786499\n",
      "step 9513 loss: 2.4112043380737305\n",
      "step 9514 loss: 2.4561610221862793\n",
      "step 9515 loss: 2.4354100227355957\n",
      "step 9516 loss: 2.451819896697998\n",
      "step 9517 loss: 2.5120015144348145\n",
      "step 9518 loss: 2.570554733276367\n",
      "step 9519 loss: 2.4548659324645996\n",
      "step 9520 loss: 2.5523393154144287\n",
      "step 9521 loss: 2.4329569339752197\n",
      "step 9522 loss: 2.4190361499786377\n",
      "step 9523 loss: 2.5160274505615234\n",
      "step 9524 loss: 2.592578649520874\n",
      "step 9525 loss: 2.2985384464263916\n",
      "step 9526 loss: 2.3947410583496094\n",
      "step 9527 loss: 2.513763666152954\n",
      "step 9528 loss: 2.395087242126465\n",
      "step 9529 loss: 2.506472587585449\n",
      "step 9530 loss: 2.428328275680542\n",
      "step 9531 loss: 2.3916728496551514\n",
      "step 9532 loss: 2.5648531913757324\n",
      "step 9533 loss: 2.493534803390503\n",
      "step 9534 loss: 2.4126741886138916\n",
      "step 9535 loss: 2.5460398197174072\n",
      "step 9536 loss: 2.4638102054595947\n",
      "step 9537 loss: 2.4182021617889404\n",
      "step 9538 loss: 2.506007432937622\n",
      "step 9539 loss: 2.560145139694214\n",
      "step 9540 loss: 2.3484771251678467\n",
      "step 9541 loss: 2.389768362045288\n",
      "step 9542 loss: 2.3802311420440674\n",
      "step 9543 loss: 2.5292866230010986\n",
      "step 9544 loss: 2.5393447875976562\n",
      "step 9545 loss: 2.5242366790771484\n",
      "step 9546 loss: 2.383145332336426\n",
      "step 9547 loss: 2.4465858936309814\n",
      "step 9548 loss: 2.4771180152893066\n",
      "step 9549 loss: 2.4938008785247803\n",
      "step 9550 loss: 2.5690343379974365\n",
      "step 9551 loss: 2.3786027431488037\n",
      "step 9552 loss: 2.5156478881835938\n",
      "step 9553 loss: 2.423471689224243\n",
      "step 9554 loss: 2.461150646209717\n",
      "step 9555 loss: 2.4904584884643555\n",
      "step 9556 loss: 2.4216127395629883\n",
      "step 9557 loss: 2.497920036315918\n",
      "step 9558 loss: 2.3891890048980713\n",
      "step 9559 loss: 2.4727578163146973\n",
      "step 9560 loss: 2.6476430892944336\n",
      "step 9561 loss: 2.4208767414093018\n",
      "step 9562 loss: 2.459852457046509\n",
      "step 9563 loss: 2.4087674617767334\n",
      "step 9564 loss: 2.4378409385681152\n",
      "step 9565 loss: 2.5683233737945557\n",
      "step 9566 loss: 2.5599915981292725\n",
      "step 9567 loss: 2.5248477458953857\n",
      "step 9568 loss: 2.5855371952056885\n",
      "step 9569 loss: 2.4575018882751465\n",
      "step 9570 loss: 2.432234048843384\n",
      "step 9571 loss: 2.6480541229248047\n",
      "step 9572 loss: 2.543337106704712\n",
      "step 9573 loss: 2.5022778511047363\n",
      "step 9574 loss: 2.381927967071533\n",
      "step 9575 loss: 2.471944570541382\n",
      "step 9576 loss: 2.3450701236724854\n",
      "step 9577 loss: 2.5246191024780273\n",
      "step 9578 loss: 2.5591647624969482\n",
      "step 9579 loss: 2.548301935195923\n",
      "step 9580 loss: 2.4481470584869385\n",
      "step 9581 loss: 2.4610965251922607\n",
      "step 9582 loss: 2.4855101108551025\n",
      "step 9583 loss: 2.460632562637329\n",
      "step 9584 loss: 2.398029088973999\n",
      "step 9585 loss: 2.5325472354888916\n",
      "step 9586 loss: 2.5303401947021484\n",
      "step 9587 loss: 2.3556151390075684\n",
      "step 9588 loss: 2.487295627593994\n",
      "step 9589 loss: 2.507600784301758\n",
      "step 9590 loss: 2.3657140731811523\n",
      "step 9591 loss: 2.5858874320983887\n",
      "step 9592 loss: 2.46171236038208\n",
      "step 9593 loss: 2.5340986251831055\n",
      "step 9594 loss: 2.485372304916382\n",
      "step 9595 loss: 2.623368263244629\n",
      "step 9596 loss: 2.4935874938964844\n",
      "step 9597 loss: 2.399214267730713\n",
      "step 9598 loss: 2.503255844116211\n",
      "step 9599 loss: 2.4085745811462402\n",
      "step 9600 loss: 2.525811195373535\n",
      "step 9601 loss: 2.4387454986572266\n",
      "step 9602 loss: 2.3648862838745117\n",
      "step 9603 loss: 2.5034701824188232\n",
      "step 9604 loss: 2.4705772399902344\n",
      "step 9605 loss: 2.5056018829345703\n",
      "step 9606 loss: 2.4498610496520996\n",
      "step 9607 loss: 2.557846784591675\n",
      "step 9608 loss: 2.453850030899048\n",
      "step 9609 loss: 2.4611616134643555\n",
      "step 9610 loss: 2.41428279876709\n",
      "step 9611 loss: 2.491899251937866\n",
      "step 9612 loss: 2.33882212638855\n",
      "step 9613 loss: 2.4513485431671143\n",
      "step 9614 loss: 2.622109889984131\n",
      "step 9615 loss: 2.4249463081359863\n",
      "step 9616 loss: 2.406615972518921\n",
      "step 9617 loss: 2.4903690814971924\n",
      "step 9618 loss: 2.525456190109253\n",
      "step 9619 loss: 2.4483325481414795\n",
      "step 9620 loss: 2.4968507289886475\n",
      "step 9621 loss: 2.4645116329193115\n",
      "step 9622 loss: 2.4421727657318115\n",
      "step 9623 loss: 2.592524290084839\n",
      "step 9624 loss: 2.479635000228882\n",
      "step 9625 loss: 2.4749398231506348\n",
      "step 9626 loss: 2.533276319503784\n",
      "step 9627 loss: 2.5022499561309814\n",
      "step 9628 loss: 2.4228403568267822\n",
      "step 9629 loss: 2.4725182056427\n",
      "step 9630 loss: 2.561279535293579\n",
      "step 9631 loss: 2.3824779987335205\n",
      "step 9632 loss: 2.3132858276367188\n",
      "step 9633 loss: 2.4608654975891113\n",
      "step 9634 loss: 2.3934645652770996\n",
      "step 9635 loss: 2.4176456928253174\n",
      "step 9636 loss: 2.3076300621032715\n",
      "step 9637 loss: 2.392814874649048\n",
      "step 9638 loss: 2.4237849712371826\n",
      "step 9639 loss: 2.6091277599334717\n",
      "step 9640 loss: 2.5520193576812744\n",
      "step 9641 loss: 2.4666459560394287\n",
      "step 9642 loss: 2.3679239749908447\n",
      "step 9643 loss: 2.476828098297119\n",
      "step 9644 loss: 2.412091016769409\n",
      "step 9645 loss: 2.435891628265381\n",
      "step 9646 loss: 2.5253188610076904\n",
      "step 9647 loss: 2.5359904766082764\n",
      "step 9648 loss: 2.4877967834472656\n",
      "step 9649 loss: 2.4670350551605225\n",
      "step 9650 loss: 2.4658873081207275\n",
      "step 9651 loss: 2.4089910984039307\n",
      "step 9652 loss: 2.4885997772216797\n",
      "step 9653 loss: 2.565139055252075\n",
      "step 9654 loss: 2.5029053688049316\n",
      "step 9655 loss: 2.394753932952881\n",
      "step 9656 loss: 2.5167806148529053\n",
      "step 9657 loss: 2.3900225162506104\n",
      "step 9658 loss: 2.5465710163116455\n",
      "step 9659 loss: 2.4508745670318604\n",
      "step 9660 loss: 2.469188690185547\n",
      "step 9661 loss: 2.4056107997894287\n",
      "step 9662 loss: 2.3151731491088867\n",
      "step 9663 loss: 2.433323383331299\n",
      "step 9664 loss: 2.4427928924560547\n",
      "step 9665 loss: 2.558006763458252\n",
      "step 9666 loss: 2.3942716121673584\n",
      "step 9667 loss: 2.5677170753479004\n",
      "step 9668 loss: 2.354917287826538\n",
      "step 9669 loss: 2.513720750808716\n",
      "step 9670 loss: 2.483302593231201\n",
      "step 9671 loss: 2.3713784217834473\n",
      "step 9672 loss: 2.376127004623413\n",
      "step 9673 loss: 2.5066518783569336\n",
      "step 9674 loss: 2.617579221725464\n",
      "step 9675 loss: 2.4616858959198\n",
      "step 9676 loss: 2.430370330810547\n",
      "step 9677 loss: 2.335561513900757\n",
      "step 9678 loss: 2.528196096420288\n",
      "step 9679 loss: 2.5533604621887207\n",
      "step 9680 loss: 2.544796943664551\n",
      "step 9681 loss: 2.5009474754333496\n",
      "step 9682 loss: 2.484382390975952\n",
      "step 9683 loss: 2.670759677886963\n",
      "step 9684 loss: 2.444301128387451\n",
      "step 9685 loss: 2.512568712234497\n",
      "step 9686 loss: 2.4831438064575195\n",
      "step 9687 loss: 2.4399261474609375\n",
      "step 9688 loss: 2.417214870452881\n",
      "step 9689 loss: 2.503462553024292\n",
      "step 9690 loss: 2.4740166664123535\n",
      "step 9691 loss: 2.4180378913879395\n",
      "step 9692 loss: 2.410953998565674\n",
      "step 9693 loss: 2.6299357414245605\n",
      "step 9694 loss: 2.4468986988067627\n",
      "step 9695 loss: 2.405365228652954\n",
      "step 9696 loss: 2.4630792140960693\n",
      "step 9697 loss: 2.536085605621338\n",
      "step 9698 loss: 2.4788753986358643\n",
      "step 9699 loss: 2.5381131172180176\n",
      "step 9700 loss: 2.521479845046997\n",
      "step 9701 loss: 2.5863919258117676\n",
      "step 9702 loss: 2.635824203491211\n",
      "step 9703 loss: 2.565613031387329\n",
      "step 9704 loss: 2.5314629077911377\n",
      "step 9705 loss: 2.5497846603393555\n",
      "step 9706 loss: 2.4129467010498047\n",
      "step 9707 loss: 2.475517511367798\n",
      "step 9708 loss: 2.5323398113250732\n",
      "step 9709 loss: 2.404682159423828\n",
      "step 9710 loss: 2.538663625717163\n",
      "step 9711 loss: 2.5013296604156494\n",
      "step 9712 loss: 2.584646463394165\n",
      "step 9713 loss: 2.4872403144836426\n",
      "step 9714 loss: 2.5007479190826416\n",
      "step 9715 loss: 2.579113721847534\n",
      "step 9716 loss: 2.3868908882141113\n",
      "step 9717 loss: 2.418912649154663\n",
      "step 9718 loss: 2.633664131164551\n",
      "step 9719 loss: 2.4703307151794434\n",
      "step 9720 loss: 2.441650390625\n",
      "step 9721 loss: 2.3461902141571045\n",
      "step 9722 loss: 2.5562567710876465\n",
      "step 9723 loss: 2.4613075256347656\n",
      "step 9724 loss: 2.557098150253296\n",
      "step 9725 loss: 2.4101500511169434\n",
      "step 9726 loss: 2.468492269515991\n",
      "step 9727 loss: 2.4722883701324463\n",
      "step 9728 loss: 2.485870599746704\n",
      "step 9729 loss: 2.653974771499634\n",
      "step 9730 loss: 2.733949661254883\n",
      "step 9731 loss: 2.4438350200653076\n",
      "step 9732 loss: 2.3873941898345947\n",
      "step 9733 loss: 2.461622953414917\n",
      "step 9734 loss: 2.455918073654175\n",
      "step 9735 loss: 2.4652700424194336\n",
      "step 9736 loss: 2.4272944927215576\n",
      "step 9737 loss: 2.440413236618042\n",
      "step 9738 loss: 2.5887255668640137\n",
      "step 9739 loss: 2.326442003250122\n",
      "step 9740 loss: 2.551771879196167\n",
      "step 9741 loss: 2.4771766662597656\n",
      "step 9742 loss: 2.447366952896118\n",
      "step 9743 loss: 2.4588003158569336\n",
      "step 9744 loss: 2.4230687618255615\n",
      "step 9745 loss: 2.456885576248169\n",
      "step 9746 loss: 2.481659173965454\n",
      "step 9747 loss: 2.572669744491577\n",
      "step 9748 loss: 2.5195748805999756\n",
      "step 9749 loss: 2.444758176803589\n",
      "step 9750 loss: 2.5466086864471436\n",
      "step 9751 loss: 2.4997644424438477\n",
      "step 9752 loss: 2.534180164337158\n",
      "step 9753 loss: 2.4086146354675293\n",
      "step 9754 loss: 2.503964900970459\n",
      "step 9755 loss: 2.5424904823303223\n",
      "step 9756 loss: 2.3967442512512207\n",
      "step 9757 loss: 2.334765911102295\n",
      "step 9758 loss: 2.515587329864502\n",
      "step 9759 loss: 2.508713960647583\n",
      "step 9760 loss: 2.4610626697540283\n",
      "step 9761 loss: 2.512279987335205\n",
      "step 9762 loss: 2.4928221702575684\n",
      "step 9763 loss: 2.399850606918335\n",
      "step 9764 loss: 2.468038320541382\n",
      "step 9765 loss: 2.45381760597229\n",
      "step 9766 loss: 2.5418810844421387\n",
      "step 9767 loss: 2.4691901206970215\n",
      "step 9768 loss: 2.5760419368743896\n",
      "step 9769 loss: 2.392888307571411\n",
      "step 9770 loss: 2.3908274173736572\n",
      "step 9771 loss: 2.48995304107666\n",
      "step 9772 loss: 2.4289519786834717\n",
      "step 9773 loss: 2.5461232662200928\n",
      "step 9774 loss: 2.485609531402588\n",
      "step 9775 loss: 2.621359348297119\n",
      "step 9776 loss: 2.4753077030181885\n",
      "step 9777 loss: 2.437185764312744\n",
      "step 9778 loss: 2.3960814476013184\n",
      "step 9779 loss: 2.4904191493988037\n",
      "step 9780 loss: 2.526174783706665\n",
      "step 9781 loss: 2.502267360687256\n",
      "step 9782 loss: 2.3801095485687256\n",
      "step 9783 loss: 2.4962446689605713\n",
      "step 9784 loss: 2.3639914989471436\n",
      "step 9785 loss: 2.4811065196990967\n",
      "step 9786 loss: 2.581564426422119\n",
      "step 9787 loss: 2.5418660640716553\n",
      "step 9788 loss: 2.417886734008789\n",
      "step 9789 loss: 2.5587515830993652\n",
      "step 9790 loss: 2.5039937496185303\n",
      "step 9791 loss: 2.476785182952881\n",
      "step 9792 loss: 2.3148956298828125\n",
      "step 9793 loss: 2.534935712814331\n",
      "step 9794 loss: 2.4129998683929443\n",
      "step 9795 loss: 2.5094306468963623\n",
      "step 9796 loss: 2.480210065841675\n",
      "step 9797 loss: 2.543266773223877\n",
      "step 9798 loss: 2.5081777572631836\n",
      "step 9799 loss: 2.5481674671173096\n",
      "step 9800 loss: 2.4382166862487793\n",
      "step 9801 loss: 2.5187172889709473\n",
      "step 9802 loss: 2.54927659034729\n",
      "step 9803 loss: 2.3576877117156982\n",
      "step 9804 loss: 2.456798553466797\n",
      "step 9805 loss: 2.4128339290618896\n",
      "step 9806 loss: 2.4217147827148438\n",
      "step 9807 loss: 2.5244877338409424\n",
      "step 9808 loss: 2.372558355331421\n",
      "step 9809 loss: 2.4235801696777344\n",
      "step 9810 loss: 2.3508644104003906\n",
      "step 9811 loss: 2.3840548992156982\n",
      "step 9812 loss: 2.4839541912078857\n",
      "step 9813 loss: 2.4781148433685303\n",
      "step 9814 loss: 2.488344192504883\n",
      "step 9815 loss: 2.4857680797576904\n",
      "step 9816 loss: 2.51458740234375\n",
      "step 9817 loss: 2.4625961780548096\n",
      "step 9818 loss: 2.5683629512786865\n",
      "step 9819 loss: 2.4647247791290283\n",
      "step 9820 loss: 2.559053897857666\n",
      "step 9821 loss: 2.405249834060669\n",
      "step 9822 loss: 2.575373411178589\n",
      "step 9823 loss: 2.4798460006713867\n",
      "step 9824 loss: 2.4836931228637695\n",
      "step 9825 loss: 2.457926034927368\n",
      "step 9826 loss: 2.4374942779541016\n",
      "step 9827 loss: 2.497119426727295\n",
      "step 9828 loss: 2.523667812347412\n",
      "step 9829 loss: 2.5274126529693604\n",
      "step 9830 loss: 2.491218328475952\n",
      "step 9831 loss: 2.4225399494171143\n",
      "step 9832 loss: 2.4902029037475586\n",
      "step 9833 loss: 2.399491548538208\n",
      "step 9834 loss: 2.6514663696289062\n",
      "step 9835 loss: 2.4442710876464844\n",
      "step 9836 loss: 2.404580593109131\n",
      "step 9837 loss: 2.429748296737671\n",
      "step 9838 loss: 2.4742820262908936\n",
      "step 9839 loss: 2.46848464012146\n",
      "step 9840 loss: 2.551797866821289\n",
      "step 9841 loss: 2.4004194736480713\n",
      "step 9842 loss: 2.450005531311035\n",
      "step 9843 loss: 2.5035269260406494\n",
      "step 9844 loss: 2.500495433807373\n",
      "step 9845 loss: 2.388488292694092\n",
      "step 9846 loss: 2.5126454830169678\n",
      "step 9847 loss: 2.5206191539764404\n",
      "step 9848 loss: 2.4718017578125\n",
      "step 9849 loss: 2.5046355724334717\n",
      "step 9850 loss: 2.3902626037597656\n",
      "step 9851 loss: 2.527641773223877\n",
      "step 9852 loss: 2.618375062942505\n",
      "step 9853 loss: 2.3521666526794434\n",
      "step 9854 loss: 2.468047857284546\n",
      "step 9855 loss: 2.5445504188537598\n",
      "step 9856 loss: 2.451992988586426\n",
      "step 9857 loss: 2.3809330463409424\n",
      "step 9858 loss: 2.6341230869293213\n",
      "step 9859 loss: 2.5070090293884277\n",
      "step 9860 loss: 2.495589256286621\n",
      "step 9861 loss: 2.362548828125\n",
      "step 9862 loss: 2.3651468753814697\n",
      "step 9863 loss: 2.50917387008667\n",
      "step 9864 loss: 2.5031449794769287\n",
      "step 9865 loss: 2.5293586254119873\n",
      "step 9866 loss: 2.4791758060455322\n",
      "step 9867 loss: 2.469303607940674\n",
      "step 9868 loss: 2.506009101867676\n",
      "step 9869 loss: 2.41023588180542\n",
      "step 9870 loss: 2.4118731021881104\n",
      "step 9871 loss: 2.4227049350738525\n",
      "step 9872 loss: 2.5771825313568115\n",
      "step 9873 loss: 2.430553674697876\n",
      "step 9874 loss: 2.4351258277893066\n",
      "step 9875 loss: 2.6268789768218994\n",
      "step 9876 loss: 2.4789764881134033\n",
      "step 9877 loss: 2.482473611831665\n",
      "step 9878 loss: 2.4443914890289307\n",
      "step 9879 loss: 2.388159990310669\n",
      "step 9880 loss: 2.4084980487823486\n",
      "step 9881 loss: 2.459848642349243\n",
      "step 9882 loss: 2.493474245071411\n",
      "step 9883 loss: 2.3930962085723877\n",
      "step 9884 loss: 2.3428826332092285\n",
      "step 9885 loss: 2.5327398777008057\n",
      "step 9886 loss: 2.4676244258880615\n",
      "step 9887 loss: 2.553666830062866\n",
      "step 9888 loss: 2.559746742248535\n",
      "step 9889 loss: 2.4660120010375977\n",
      "step 9890 loss: 2.507852554321289\n",
      "step 9891 loss: 2.5178892612457275\n",
      "step 9892 loss: 2.3569114208221436\n",
      "step 9893 loss: 2.584756374359131\n",
      "step 9894 loss: 2.515111207962036\n",
      "step 9895 loss: 2.4712047576904297\n",
      "step 9896 loss: 2.439481019973755\n",
      "step 9897 loss: 2.4348506927490234\n",
      "step 9898 loss: 2.4837779998779297\n",
      "step 9899 loss: 2.473640203475952\n",
      "step 9900 loss: 2.4321024417877197\n",
      "step 9901 loss: 2.53495717048645\n",
      "step 9902 loss: 2.4038925170898438\n",
      "step 9903 loss: 2.44303822517395\n",
      "step 9904 loss: 2.4512240886688232\n",
      "step 9905 loss: 2.4944729804992676\n",
      "step 9906 loss: 2.4692697525024414\n",
      "step 9907 loss: 2.5360915660858154\n",
      "step 9908 loss: 2.587094783782959\n",
      "step 9909 loss: 2.3909149169921875\n",
      "step 9910 loss: 2.341973066329956\n",
      "step 9911 loss: 2.4611637592315674\n",
      "step 9912 loss: 2.5829098224639893\n",
      "step 9913 loss: 2.483999013900757\n",
      "step 9914 loss: 2.5142886638641357\n",
      "step 9915 loss: 2.395768165588379\n",
      "step 9916 loss: 2.3954458236694336\n",
      "step 9917 loss: 2.492457866668701\n",
      "step 9918 loss: 2.445018768310547\n",
      "step 9919 loss: 2.49212384223938\n",
      "step 9920 loss: 2.378019094467163\n",
      "step 9921 loss: 2.557898759841919\n",
      "step 9922 loss: 2.543599843978882\n",
      "step 9923 loss: 2.422419309616089\n",
      "step 9924 loss: 2.4494738578796387\n",
      "step 9925 loss: 2.4357004165649414\n",
      "step 9926 loss: 2.33097505569458\n",
      "step 9927 loss: 2.502865791320801\n",
      "step 9928 loss: 2.5844857692718506\n",
      "step 9929 loss: 2.5813488960266113\n",
      "step 9930 loss: 2.5092954635620117\n",
      "step 9931 loss: 2.4603703022003174\n",
      "step 9932 loss: 2.413924217224121\n",
      "step 9933 loss: 2.400207996368408\n",
      "step 9934 loss: 2.6405749320983887\n",
      "step 9935 loss: 2.3808398246765137\n",
      "step 9936 loss: 2.531975746154785\n",
      "step 9937 loss: 2.37250018119812\n",
      "step 9938 loss: 2.457491874694824\n",
      "step 9939 loss: 2.5434045791625977\n",
      "step 9940 loss: 2.423229217529297\n",
      "step 9941 loss: 2.5907504558563232\n",
      "step 9942 loss: 2.4823174476623535\n",
      "step 9943 loss: 2.420645236968994\n",
      "step 9944 loss: 2.443000078201294\n",
      "step 9945 loss: 2.4353952407836914\n",
      "step 9946 loss: 2.482022523880005\n",
      "step 9947 loss: 2.4383208751678467\n",
      "step 9948 loss: 2.5570828914642334\n",
      "step 9949 loss: 2.367823600769043\n",
      "step 9950 loss: 2.4900991916656494\n",
      "step 9951 loss: 2.315355062484741\n",
      "step 9952 loss: 2.392831563949585\n",
      "step 9953 loss: 2.368091583251953\n",
      "step 9954 loss: 2.2705936431884766\n",
      "step 9955 loss: 2.4348857402801514\n",
      "step 9956 loss: 2.555929183959961\n",
      "step 9957 loss: 2.5152900218963623\n",
      "step 9958 loss: 2.526524543762207\n",
      "step 9959 loss: 2.4900453090667725\n",
      "step 9960 loss: 2.4228944778442383\n",
      "step 9961 loss: 2.548809051513672\n",
      "step 9962 loss: 2.466085433959961\n",
      "step 9963 loss: 2.4479355812072754\n",
      "step 9964 loss: 2.473177909851074\n",
      "step 9965 loss: 2.45951771736145\n",
      "step 9966 loss: 2.5660500526428223\n",
      "step 9967 loss: 2.4484453201293945\n",
      "step 9968 loss: 2.522836685180664\n",
      "step 9969 loss: 2.422328472137451\n",
      "step 9970 loss: 2.3908982276916504\n",
      "step 9971 loss: 2.420789957046509\n",
      "step 9972 loss: 2.335397958755493\n",
      "step 9973 loss: 2.5702807903289795\n",
      "step 9974 loss: 2.4982945919036865\n",
      "step 9975 loss: 2.3127782344818115\n",
      "step 9976 loss: 2.3383662700653076\n",
      "step 9977 loss: 2.5388245582580566\n",
      "step 9978 loss: 2.494527578353882\n",
      "step 9979 loss: 2.3597047328948975\n",
      "step 9980 loss: 2.5577876567840576\n",
      "step 9981 loss: 2.398709535598755\n",
      "step 9982 loss: 2.401496171951294\n",
      "step 9983 loss: 2.456613779067993\n",
      "step 9984 loss: 2.424605131149292\n",
      "step 9985 loss: 2.648049831390381\n",
      "step 9986 loss: 2.530766725540161\n",
      "step 9987 loss: 2.6104941368103027\n",
      "step 9988 loss: 2.504631757736206\n",
      "step 9989 loss: 2.455660581588745\n",
      "step 9990 loss: 2.5828373432159424\n",
      "step 9991 loss: 2.542987108230591\n",
      "step 9992 loss: 2.449852705001831\n",
      "step 9993 loss: 2.5931859016418457\n",
      "step 9994 loss: 2.6539061069488525\n",
      "step 9995 loss: 2.383460283279419\n",
      "step 9996 loss: 2.445333957672119\n",
      "step 9997 loss: 2.4149906635284424\n",
      "step 9998 loss: 2.351022958755493\n",
      "step 9999 loss: 2.5727508068084717\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"step {steps} loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenou\n"
     ]
    }
   ],
   "source": [
    "# after training, we have a much better result from the model\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), \n",
    "                                          dtype=torch.long), \n",
    "                                          max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test the bigram.py model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | loss 4.6207\n",
      "step 100 | loss 4.3283\n",
      "step 200 | loss 4.4074\n",
      "step 300 | loss 4.3164\n",
      "step 400 | loss 4.0631\n",
      "step 500 | loss 4.0428\n",
      "step 600 | loss 3.8508\n",
      "step 700 | loss 3.9037\n",
      "step 800 | loss 3.8130\n",
      "step 900 | loss 3.7051\n",
      "\n",
      "Generated text: \n",
      "VV3,,\n",
      "zMN'MEtzd&KvGS3$.F whiCESZZPNY?xc!&AtZxhk\n",
      "Twk rmaI;TzCVxre'iqUBRFbAGRtPIDtoUWht$JeaiXd\n",
      "DneiDSD\n"
     ]
    }
   ],
   "source": [
    "from biagram import BigramLanguageModel, TextDataset, train_model, generate_text\n",
    "\n",
    "dataset = TextDataset('input.txt')\n",
    "dataset.to_device('cuda')\n",
    "model = BigramLanguageModel(dataset.vocab_size)\n",
    "model.to_device('cuda')\n",
    "\n",
    "train_model(model, dataset, num_steps=1000)\n",
    "generated_text = generate_text(model, dataset)\n",
    "print(\"\\nGenerated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing the self attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for self attention, we want to get the current token and all past tokens (not the future tokens)\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        # take average of all previous tokens up to the current token\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0) # (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei/wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x  # (B, T, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "\n",
    "print(xbow[0])\n",
    "# first row is same\n",
    "# second row is average of first two tokens\n",
    "# third row is average of first three tokens\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c= tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# this method would be very inefficient for large contexts\n",
    "# lets use matrix multiplication to do this more efficiently\n",
    "torch.manual_seed(42)\n",
    "a= torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a, dim=1, keepdim=True)\n",
    "b= torch.randint(0,10,(3,2)).float()\n",
    "c= a @ b\n",
    "\n",
    "print(\"a=\",a)\n",
    "print(\"---\")\n",
    "print(\"b=\",b)\n",
    "print(\"---\")\n",
    "print(\"c=\",c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
